{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5119e53",
   "metadata": {},
   "source": [
    "## 1. 다국어 RAG 시스템 구축 실습\n",
    "\n",
    "- 실제 RAG 시스템은 다양한 언어의 문서를 다루거나, 여러 언어로 질문을 받는 상황에 놓일 수 있음.\n",
    "- 다국어 환경에서 RAG 시스템을 구축하는 방법에 대한 예시 확인\n",
    "\n",
    "**다국어 RAG의 주요 과제:**\n",
    "- **언어 불일치**: 질문과 문서의 언어가 다를 경우 검색 성능이 저하될 수 있음.\n",
    "- **임베딩 모델 선택**: 사용하는 임베딩 모델이 대상 언어들을 얼마나 잘 지원하는지가 중요하며, 교차 언어(cross-lingual) 성능이 좋은 모델이 필요할 수 있음.\n",
    "- **번역 품질 및 비용**: 자동 번역기를 사용할 경우 번역 품질, 지연 시간, 비용 등을 고려해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b183570",
   "metadata": {},
   "source": [
    "### 1-1 언어 교차(cross-lingual) 검색\n",
    "\n",
    "- 교차 언어 임베딩 모델은 서로 다른 언어의 텍스트라도 의미가 유사하면 벡터 공간에서 가깝게 위치하도록 학습됨. \n",
    "- 이러한 모델을 사용하면 예를 들어 한국어로 질문해도 영어 문서를 검색하거나, 그 반대의 경우도 가능하게 됨.\n",
    "\n",
    "**전략:**\n",
    "1.  한국어 문서와 영어 문서를 모두 준비.\n",
    "2.  교차 언어 성능이 우수한 임베딩 모델 (예: OpenAI `text-embedding-3-small`, HuggingFace `BAAI/bge-m3`, Ollama `bge-m3`)을 선택.\n",
    "3.  모든 문서를 선택한 임베딩 모델로 임베딩하여 하나의 벡터 저장소에 저장.\n",
    "4.  사용자 질문(한국어 또는 영어)을 동일한 임베딩 모델로 임베딩하여 벡터 저장소에서 유사 문서를 검색.\n",
    "\n",
    "**장점:**\n",
    "- 구현이 비교적 간단합니다. 단일 임베딩 모델과 단일 벡터 저장소만 관리하면 됨.\n",
    "\n",
    "**단점:**\n",
    "- 임베딩 모델의 교차 언어 성능에 크게 의존하며, 모델 성능이 부족하면 검색 정확도가 떨어질 수 있음\n",
    "- 동일 언어 내 검색(예: 한국어 질문 -> 한국어 문서)보다 성능이 낮을 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "def68998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4059b6",
   "metadata": {},
   "source": [
    "`(1) 다국어 문서 로드 및 전처리` 예시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4b4ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 텍스트 파일: ['./data\\\\Rivian_KR.txt', './data\\\\Tesla_KR.txt']\n",
      "영어 텍스트 파일: ['./data\\\\Rivian_EN.txt', './data\\\\Tesla_EN.txt']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os \n",
    "korean_txt_files = glob(os.path.join('./data', '*_KR.txt')) \n",
    "english_txt_files = glob(os.path.join('./data', '*_EN.txt'))\n",
    "\n",
    "print(\"한국어 텍스트 파일:\", korean_txt_files)\n",
    "print(\"영어 텍스트 파일:\", english_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e69a4d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 원본 Document 수: 2\n",
      "영어 원본 Document 수: 2\n",
      "\n",
      "한국어 데이터 샘플 (첫 번째 문서 메타데이터): {'source': './data\\\\Rivian_KR.txt'}\n",
      "한국어 데이터 샘플 (첫 번째 문서 내용 일부): 2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당한 투자를 통해 비약적인 성장을 거듭하며 미시간과 베이 지역에 연구 시설을 설립했습니다. 주요 공급업체와의 거리를 좁히기 위해 본사를 미시간주 리보니아로 이전했습니다.\n",
      "\n",
      "리비안의 \n",
      "\n",
      "영어 데이터 샘플 (첫 번째 문서 메타데이터): {'source': './data\\\\Rivian_EN.txt'}\n",
      "영어 데이터 샘플 (첫 번째 문서 내용 일부): Founded in 2009 by MIT PhD graduate RJ Scaringe, Rivian is an innovative American electric vehicle manufacturer. Initially focused on autonomous electric cars from 2011, Rivian's significant growth ph\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def load_text_files(txt_files):\n",
    "    data = []\n",
    "    for text_file in txt_files:\n",
    "        # TextLoader는 기본적으로 utf-8을 가정하나, 명시하는 것이 좋음\n",
    "        loader = TextLoader(text_file, encoding='utf-8') \n",
    "        data.extend(loader.load())\n",
    "    return data\n",
    "\n",
    "korean_data = load_text_files(korean_txt_files)\n",
    "english_data = load_text_files(english_txt_files)\n",
    "\n",
    "print(f\"한국어 원본 Document 수: {len(korean_data)}\")\n",
    "print(f\"영어 원본 Document 수: {len(english_data)}\")\n",
    "\n",
    "if korean_data:\n",
    "    print(\"\\n한국어 데이터 샘플 (첫 번째 문서 메타데이터):\", korean_data[0].metadata)\n",
    "    print(\"한국어 데이터 샘플 (첫 번째 문서 내용 일부):\", korean_data[0].page_content[:200])\n",
    "if english_data:\n",
    "    print(\"\\n영어 데이터 샘플 (첫 번째 문서 메타데이터):\", english_data[0].metadata)\n",
    "    print(\"영어 데이터 샘플 (첫 번째 문서 내용 일부):\", english_data[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9521a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 한국어 문서(청크) 수: 6\n",
      "분할된 영어 문서(청크) 수: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# 문장을 구분하여 분할 (마침표, 느낌표, 물음표 다음에 공백이 오는 경우 문장의 끝으로 판단)\n",
    "# 이 모델은 교차 언어 성능이 뛰어나므로, 다국어 문서 처리에 적합\n",
    "# why?? \"대규모 다국어 데이터셋으로 사전 학습\" 및 \"정교한 토크나이저\"\n",
    "text_splitter_multilingual = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"text-embedding-3-small\", # 이 모델의 토크나이저 사용\n",
    "    separator=r\"[.!?]\\s+\",      # 문장 끝 구두점 뒤 공백 기준 분할 (정규식)\n",
    "    chunk_size=200,              # 목표 청크 토큰 수 (CharacterTextSplitter는 이 값을 엄격히 따르지 않을 수 있음)\n",
    "    chunk_overlap=20,            # 청크 간 중복 토큰 수\n",
    "    is_separator_regex=True,     # separator를 정규식으로 해석\n",
    "    keep_separator=False,        # 구분자(공백) 제거\n",
    ")\n",
    "\n",
    "korean_docs_split = []\n",
    "english_docs_split = []\n",
    "\n",
    "if korean_data:\n",
    "    korean_docs_split = text_splitter_multilingual.split_documents(korean_data)\n",
    "if english_data:\n",
    "    english_docs_split = text_splitter_multilingual.split_documents(english_data)\n",
    "\n",
    "print(f\"분할된 한국어 문서(청크) 수: {len(korean_docs_split)}\")\n",
    "print(f\"분할된 영어 문서(청크) 수: {len(english_docs_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52633cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 한국어 분할 청크 샘플 ---\n",
      "\n",
      "[청크 1] (길이: 204자, 메타데이터: {'source': './data\\\\Rivian_KR.txt'})\n",
      "2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다[.!?]\\s+2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당한 투자를 통해 비약적인 성장을 거듭하며 미시간과 베이 지역에 연구 시설을 설립\n",
      "[...]\n",
      "\n",
      "[청크 2] (길이: 178자, 메타데이터: {'source': './data\\\\Rivian_KR.txt'})\n",
      "리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미드십 엔진 하이브리드 쿠페 스포츠카 R1(원래 이름은 아베라(Avera))이었습니다[.!?]\\s+이 차량은 모듈식 캡슐 구조와 쉽게 교체 가능한 차체 패널을 특징으로 하며, 2013년 말에서 201\n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "# 한국어 분할 청크 확인 (처음 2개)\n",
    "if korean_docs_split:\n",
    "    print(\"--- 한국어 분할 청크 샘플 ---\")\n",
    "    for i, doc in enumerate(korean_docs_split[:2]):\n",
    "        print(f\"\\n[청크 {i+1}] (길이: {len(doc.page_content)}자, 메타데이터: {doc.metadata})\")\n",
    "        print(doc.page_content[:150])\n",
    "        if len(doc.page_content) > 150: print(\"[...]\")\n",
    "else:\n",
    "    print(\"분할된 한국어 문서가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34634419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 영어 분할 청크 샘플 ---\n",
      "\n",
      "[청크 1] (길이: 757자, 메타데이터: {'source': './data\\\\Rivian_EN.txt'})\n",
      "Founded in 2009 by MIT PhD graduate RJ Scaringe, Rivian is an innovative American electric vehicle manufacturer[.!?]\\s+Initially focused on autonomous\n",
      "[...]\n",
      "\n",
      "[청크 2] (길이: 372자, 메타데이터: {'source': './data\\\\Rivian_EN.txt'})\n",
      "Rivian also considered various versions, including a diesel hybrid, a racing version named R1 GT for a Brazilian one-make series, a four-door sedan, a\n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "# 영어 분할 청크 확인 (처음 2개)\n",
    "if english_docs_split:\n",
    "    print(\"\\n--- 영어 분할 청크 샘플 ---\")\n",
    "    for i, doc in enumerate(english_docs_split[:2]):\n",
    "        print(f\"\\n[청크 {i+1}] (길이: {len(doc.page_content)}자, 메타데이터: {doc.metadata})\")\n",
    "        print(doc.page_content[:150])\n",
    "        if len(doc.page_content) > 150: print(\"[...]\")\n",
    "else:\n",
    "    print(\"분할된 영어 문서가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699cb7f",
   "metadata": {},
   "source": [
    "`(2) 문서 임베딩 및 벡터저장소에 저장 ` 예시\n",
    "1) 선택한 교차 언어 임베딩 모델들(OpenAI, HuggingFace, Ollama)을 사용하여 한국어와 영어 문서를 함께 임베딩하고, 각 모델별로 ChromaDB 벡터 저장소에 저장\n",
    "2) 이렇게 하면 각 임베딩 모델의 교차 언어 검색 성능을 비교 가능능\n",
    "\n",
    "- `collection_name`: 벡터 저장소 내에서 특정 문서 그룹을 식별하는 이름이며, 모델별로 다른 이름을 사용 가능.\n",
    "- `persist_directory`: 벡터 저장소 데이터를 디스크에 저장할 경로이며, 지정하면 저장소가 유지되어 재사용 가능.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f8383d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama 임베딩 모델 (bge-m3) 준비 완료.\n",
      "총 분할된 문서(청크) 수: 10\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# OpenAI 임베딩 모델 (교차 언어 지원 우수)\n",
    "embeddings_openai_small_cl = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Hugging Face 임베딩 모델 (BAAI/bge-m3, 교차 언어 지원 우수)\n",
    "embeddings_huggingface_bge_m3_cl = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\", \n",
    "    model_kwargs={'device': 'cpu'}, # CPU 또는 'cuda' \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Ollama 임베딩 모델 (bge-m3 또는 nomic-embed-text, 교차 언어 지원 가능성 확인 필요)\n",
    "# Ollama 서버 및 해당 모델이 준비되어 있어야 함\n",
    "embeddings_ollama_bge_m3_cl = None\n",
    "ollama_ready_cl = False\n",
    "try:\n",
    "    embeddings_ollama_bge_m3_cl = OllamaEmbeddings(model=\"bge-m3\") \n",
    "    # embeddings_ollama_nomic_cl = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    print(\"Ollama 임베딩 모델 (bge-m3) 준비 완료.\")\n",
    "    ollama_ready_cl = True\n",
    "except Exception as e:\n",
    "    print(f\"Ollama 연결 또는 모델 로드 실패 (교차언어용): {e}\")\n",
    "    print(\"Ollama (bge-m3) 교차언어 예제를 실행하려면 Ollama 서버를 실행하고 'bge-m3' 모델을 pull 해주세요.\")\n",
    "\n",
    "all_split_docs = korean_docs_split + english_docs_split\n",
    "print(f\"총 분할된 문서(청크) 수: {len(all_split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fea85ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 임베딩으로 벡터 저장소 구축 중...\n",
      "OpenAI 벡터 저장소 문서 수: 20\n",
      "\n",
      "Hugging Face (BAAI/bge-m3) 임베딩으로 벡터 저장소 구축 중...\n",
      "Hugging Face 벡터 저장소 문서 수: 20\n",
      "\n",
      "Ollama (bge-m3) 임베딩으로 벡터 저장소 구축 중...\n",
      "Ollama 벡터 저장소 문서 수: 10\n"
     ]
    }
   ],
   "source": [
    "# 다국어 벡터 저장소 구축\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "db_openai_cl = None\n",
    "db_huggingface_cl = None\n",
    "db_ollama_cl = None\n",
    "\n",
    "if all_split_docs: # 분할된 문서가 있을 경우에만 실행\n",
    "    print(\"OpenAI 임베딩으로 벡터 저장소 구축 중...\")\n",
    "    db_openai_cl = Chroma.from_documents(\n",
    "        documents=all_split_docs, \n",
    "        embedding=embeddings_openai_small_cl,\n",
    "        collection_name=\"db_openai_crosslingual_v2\", # 컬렉션 이름 변경 또는 기존 삭제 후 생성\n",
    "        persist_directory=\"./chroma_db_cl\", # 디렉토리 구분\n",
    "    )\n",
    "    print(f\"OpenAI 벡터 저장소 문서 수: {db_openai_cl._collection.count()}\")\n",
    "\n",
    "    print(\"\\nHugging Face (BAAI/bge-m3) 임베딩으로 벡터 저장소 구축 중...\")\n",
    "    db_huggingface_cl = Chroma.from_documents(\n",
    "        documents=all_split_docs, \n",
    "        embedding=embeddings_huggingface_bge_m3_cl,\n",
    "        collection_name=\"db_huggingface_crosslingual_v2\",\n",
    "        persist_directory=\"./chroma_db_cl\",\n",
    "    )\n",
    "    print(f\"Hugging Face 벡터 저장소 문서 수: {db_huggingface_cl._collection.count()}\")\n",
    "\n",
    "    if ollama_ready_cl and embeddings_ollama_bge_m3_cl:\n",
    "        print(\"\\nOllama (bge-m3) 임베딩으로 벡터 저장소 구축 중...\")\n",
    "        db_ollama_cl = Chroma.from_documents(\n",
    "            documents=all_split_docs, \n",
    "            embedding=embeddings_ollama_bge_m3_cl,\n",
    "            collection_name=\"db_ollama_crosslingual_v2\",\n",
    "            persist_directory=\"./chroma_db_cl\",\n",
    "        )\n",
    "        print(f\"Ollama 벡터 저장소 문서 수: {db_ollama_cl._collection.count()}\")\n",
    "    else:\n",
    "        print(\"\\nOllama가 준비되지 않아 Ollama 벡터 저장소 구축을 건너뜁니다.\")\n",
    "else:\n",
    "    print(\"분할된 문서가 없어 벡터 저장소 구축을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592085d",
   "metadata": {},
   "source": [
    "`(3) RAG 성능 비교 `  \n",
    "\n",
    "- 간단한 RAG 체인을 구성하여 각 임베딩 모델 기반의 벡터 저장소가 한국어 질문과 영어 질문에 대해 어떻게 응답하는지 비교\n",
    "\n",
    "**RAG 체인 구성 요소:**\n",
    "- **Retriever**: 벡터 저장소에서 유사 문서를 검색하며, `as_retriever()`로 변환하고, `search_kwargs={'k': 2}`로 상위 2개 문서를 가져오도록 설정.\n",
    "- **Prompt Template**: LLM에 전달할 프롬프트를 정의하며, 컨텍스트(검색된 문서)와 질문을 포함.\n",
    "- **LLM**: 질문과 컨텍스트를 바탕으로 최종 답변을 생성할 언어 모델 (예: `ChatOpenAI`)\n",
    "- **Output Parser**: LLM의 출력(주로 `AIMessage` 객체)에서 실제 텍스트 답변만 추출(`StrOutputParser`)\n",
    "- **RunnablePassthrough / format_docs**: LangChain Expression Language (LCEL)에서 데이터 흐름을 관리하고, 검색된 `Document` 객체 리스트를 LLM 프롬프트에 적합한 문자열 형태로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30396f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인 생성\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based ONLY on the following context.\n",
    "Do not use any external information or knowledge. \n",
    "If the answer is not in the context, answer \"잘 모르겠습니다.\".\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_cl = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 문서 포맷터 함수: Document 객체 리스트를 단일 문자열로 결합\n",
    "def format_docs_cl(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# LLM 모델 생성 (답변 생성용)\n",
    "llm_cl = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 체인 생성 함수 (벡터 저장소를 인자로 받음)\n",
    "def create_rag_chain_cl(vectorstore):\n",
    "    if not vectorstore:\n",
    "        # 벡터 저장소가 None이면, 실행 불가능한 더미 체인을 반환하거나 예외 처리\n",
    "        # 여기서는 간단히 None을 반환하여 호출하는 쪽에서 확인하도록 함\n",
    "        return None\n",
    "        \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 2}) # 상위 2개 문서 검색\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs_cl , \"question\": RunnablePassthrough()} # 검색 및 포맷팅\n",
    "        | prompt_template_cl  # 프롬프트 적용\n",
    "        | llm_cl              # LLM으로 답변 생성\n",
    "        | StrOutputParser()   # 출력 파싱 (텍스트만 추출)\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "346e5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI RAG 체인 생성 완료\n",
      "HuggingFace RAG 체인 생성 완료\n",
      "Ollama RAG 체인 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# 각 벡터 저장소에 대한 RAG 체인 생성\n",
    "rag_chain_openai_cl = create_rag_chain_cl(db_openai_cl)\n",
    "rag_chain_huggingface_cl = create_rag_chain_cl(db_huggingface_cl)\n",
    "rag_chain_ollama_cl = create_rag_chain_cl(db_ollama_cl)\n",
    "\n",
    "if rag_chain_openai_cl: print(\"OpenAI RAG 체인 생성 완료\")\n",
    "else: print(\"OpenAI RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_huggingface_cl: print(\"HuggingFace RAG 체인 생성 완료\")\n",
    "else: print(\"HuggingFace RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_ollama_cl: print(\"Ollama RAG 체인 생성 완료\")\n",
    "else: print(\"Ollama RAG 체인 생성 실패 (벡터 저장소 없음 또는 Ollama 준비 안됨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02e10620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 한국어 쿼리: '테슬라 창업자는 누구인가요?' ---\n",
      "OpenAI 응답 (KO): 잘 모르겠습니다.\n",
      "Hugging Face (bge-m3) 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "Ollama (bge-m3) 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko_cl = \"테슬라 창업자는 누구인가요?\" # 예시 문서에 관련 내용이 있어야 함\n",
    "print(f\"\\n--- 한국어 쿼리: '{query_ko_cl}' ---\")\n",
    "\n",
    "if rag_chain_openai_cl:\n",
    "    output_openai_ko = rag_chain_openai_cl.invoke(query_ko_cl)\n",
    "    print(f\"OpenAI 응답 (KO): {output_openai_ko}\")\n",
    "else:\n",
    "    print(\"OpenAI RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_huggingface_cl:\n",
    "    output_huggingface_ko = rag_chain_huggingface_cl.invoke(query_ko_cl)\n",
    "    print(f\"Hugging Face (bge-m3) 응답 (KO): {output_huggingface_ko}\")\n",
    "else:\n",
    "    print(\"HuggingFace RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_ollama_cl:\n",
    "    output_ollama_ko = rag_chain_ollama_cl.invoke(query_ko_cl)\n",
    "    print(f\"Ollama (bge-m3) 응답 (KO): {output_ollama_ko}\")\n",
    "else:\n",
    "    print(\"Ollama RAG 체인이 없어 실행 불가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f373ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 영어 쿼리: 'Who is the founder of Tesla?' ---\n",
      "OpenAI 응답 (EN): Tesla was founded by Martin Eberhard and Marc Tarpenning.\n",
      "Hugging Face (bge-m3) 응답 (EN): Tesla was founded in 2003 by Martin Eberhard and Marc Tarpenning.\n",
      "Ollama (bge-m3) 응답 (EN): 마틴 에버하드와 마크 타페닝입니다.\n",
      "\n",
      "성능 평가는 실제 문서 내용과 질문의 관련성에 따라 크게 달라집니다.\n",
      "교차 언어 검색은 임베딩 모델의 능력이 매우 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en_cl = \"Who is the founder of Tesla?\" # 예시 문서에 관련 내용이 있어야 함\n",
    "print(f\"\\n--- 영어 쿼리: '{query_en_cl}' ---\")\n",
    "\n",
    "if rag_chain_openai_cl:\n",
    "    output_openai_en = rag_chain_openai_cl.invoke(query_en_cl)\n",
    "    print(f\"OpenAI 응답 (EN): {output_openai_en}\")\n",
    "else:\n",
    "    print(\"OpenAI RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_huggingface_cl:\n",
    "    output_huggingface_en = rag_chain_huggingface_cl.invoke(query_en_cl)\n",
    "    print(f\"Hugging Face (bge-m3) 응답 (EN): {output_huggingface_en}\")\n",
    "else:\n",
    "    print(\"HuggingFace RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_ollama_cl:\n",
    "    output_ollama_en = rag_chain_ollama_cl.invoke(query_en_cl)\n",
    "    print(f\"Ollama (bge-m3) 응답 (EN): {output_ollama_en}\")\n",
    "else:\n",
    "    print(\"Ollama RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "print(\"\\n성능 평가는 실제 문서 내용과 질문의 관련성에 따라 크게 달라집니다.\")\n",
    "print(\"교차 언어 검색은 임베딩 모델의 능력이 매우 중요합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4a32b",
   "metadata": {},
   "source": [
    "### 1-2 언어 감지 및 자동번역 통합 \n",
    "\n",
    "- 이 전략은 주로 단일 언어(예: 한국어)로 구성된 문서 저장소를 가지고 있을 때, 다양한 언어의 사용자 질문을 처리하기 위해 사용\n",
    "- 또는 그 반대의 경우도 가능\n",
    "\n",
    "**전략:**\n",
    "1.  **언어 감지**: 사용자 질문의 언어를 감지 (예: `langdetect` 라이브러리 사용).\n",
    "2.  **질문 번역**: 감지된 질문 언어가 문서 저장소의 주 언어와 다르면, 질문을 문서 저장소의 언어로 번역(예: `deepl` API 사용).\n",
    "3.  **RAG 처리**: 번역된 질문을 사용하여 일반적인 RAG 체인을 통해 답변을 생성 (이때 답변은 문서 저장소의 언어로 생성됨).\n",
    "4.  **답변 번역**: 생성된 답변의 언어가 원래 질문의 언어와 다르면, 답변을 원래 질문의 언어로 다시 번역\n",
    "\n",
    "**장점:**\n",
    "- 단일 언어에 최적화된 임베딩 모델과 LLM을 활용하여 해당 언어에서의 검색 및 답변 생성 품질을 극대화할 수 있음.\n",
    "- 다양한 언어의 사용자 질문을 지원할 수 있음.\n",
    "\n",
    "**단점:**\n",
    "- **번역 품질 의존성**: 번역기의 성능에 따라 전체 시스템의 품질이 크게 좌우됨. 번역 오류는 잘못된 검색 결과나 부정확한 답변으로 이어질 수 있음.\n",
    "- **지연 시간 증가**: 질문과 답변 번역 과정에서 추가적인 API 호출로 인해 전체 응답 시간이 늘어남.\n",
    "- **번역 비용**: 상용 번역 API(예: DeepL, Google Translate) 사용 시 비용이 발생.\n",
    "- **언어 감지 오류**: 언어 감지가 정확하지 않으면 불필요하거나 잘못된 번역이 발생 가능성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928e845",
   "metadata": {},
   "source": [
    "`(1) 한국어 문서 벡터저장소 로드 (또는 생성)`  \n",
    "\n",
    "- 한국어 문서만으로 구성된 벡터 저장소가 이미 있다고 가정.\n",
    "- 만약 없다면, 한국어 문서(`korean_docs_split`)와 한국어에 강한 임베딩 모델(예: `OpenAIEmbeddings`, 또는 한국어 특화 HuggingFace 모델)을 사용하여 새로 생성할 수 있음.\n",
    "\n",
    "- `chroma_test`라는 이름의 기존 컬렉션을 로드하려고 시도.\n",
    "- 이 컬렉션은 이전 단계나 한국어 데이터로 이미 만들어졌다고 가정.\n",
    "- 만약 해당 컬렉션이 없다면, `Chroma.from_documents`를 사용하여 새로 만들어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85f3dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'db_openai_crosslingual_v2' 벡터 저장소 로드 완료. 문서 수: 20\n"
     ]
    }
   ],
   "source": [
    "# 한국어 문서로 저장되어 있는 벡터 저장소 로드 또는 생성\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_for_ko_store = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", \n",
    ")\n",
    "\n",
    "# 기존에 'chroma_test' 컬렉션이 한국어 데이터로 만들어져 있다고 가정.\n",
    "# 없다면, korean_docs_split을 사용해 새로 생성해야 함.\n",
    "# 예시: db_korean_only = Chroma.from_documents(documents=korean_docs_split, embedding=embeddings_for_ko_store, collection_name=\"korean_store_v1\", persist_directory=\"./chroma_db_ko_only\")\n",
    "\n",
    "COLLECTION_NAME_KO_ONLY = \"db_openai_crosslingual_v2\" # (한국어+영어 데이터 포함)\n",
    "PERSIST_DIR_KO_ONLY = \"./chroma_db_cl\"\n",
    "\n",
    "try:\n",
    "    # 여기서는 1-1에서 만든 db_openai_cl (한국어+영어 문서 포함)을 재사용\n",
    "    # 순수 한국어 저장소를 원한다면, 해당 저장소를 로드하거나 새로 만들어야 함.\n",
    "    vectorstore_ko_trans = Chroma(\n",
    "        embedding_function=embeddings_for_ko_store,\n",
    "        collection_name=COLLECTION_NAME_KO_ONLY, # 1-1에서 사용한 OpenAI 컬렉션 이름\n",
    "        persist_directory=PERSIST_DIR_KO_ONLY    # 1-1에서 사용한 디렉토리\n",
    "    )\n",
    "    print(f\"'{COLLECTION_NAME_KO_ONLY}' 벡터 저장소 로드 완료. 문서 수: {vectorstore_ko_trans._collection.count()}\")\n",
    "    # 이 저장소는 실제로는 한국어와 영어가 섞여있지만, 지금은 한국어 중심 저장소로 간주하고 진행\n",
    "    # 이상적으로는 순수 한국어 문서로 구성된 저장소를 사용하는 것이 이 시나리오에 더 적합\n",
    "except Exception as e:\n",
    "    print(f\"벡터 저장소 '{COLLECTION_NAME_KO_ONLY}' 로드 실패: {e}\")\n",
    "    print(\"이전 단계에서 해당 이름의 컬렉션이 생성되었는지, 경로가 올바른지 확인하세요.\")\n",
    "    vectorstore_ko_trans = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f1fc2",
   "metadata": {},
   "source": [
    "**언어 감지 및 번역 도구 설정**\n",
    "\n",
    "- `langdetect`: 텍스트의 언어를 감지합니다.\n",
    "- `deepl`: 고품질 번역을 제공하는 API 서비스입니다. 사용을 위해서는 API 키가 필요하며, `.env` 파일에 `DEEPL_API_KEY`로 저장되어 있어야 함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
