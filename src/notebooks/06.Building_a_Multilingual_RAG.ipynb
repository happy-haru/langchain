{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5119e53",
   "metadata": {},
   "source": [
    "## 1. 다국어 RAG 시스템 구축 실습\n",
    "\n",
    "- 실제 RAG 시스템은 다양한 언어의 문서를 다루거나, 여러 언어로 질문을 받는 상황에 놓일 수 있음.\n",
    "- 다국어 환경에서 RAG 시스템을 구축하는 방법에 대한 예시 확인\n",
    "\n",
    "**다국어 RAG의 주요 과제:**\n",
    "- **언어 불일치**: 질문과 문서의 언어가 다를 경우 검색 성능이 저하될 수 있음.\n",
    "- **임베딩 모델 선택**: 사용하는 임베딩 모델이 대상 언어들을 얼마나 잘 지원하는지가 중요하며, 교차 언어(cross-lingual) 성능이 좋은 모델이 필요할 수 있음.\n",
    "- **번역 품질 및 비용**: 자동 번역기를 사용할 경우 번역 품질, 지연 시간, 비용 등을 고려해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b183570",
   "metadata": {},
   "source": [
    "### 1-1 언어 교차(cross-lingual) 검색\n",
    "\n",
    "- 교차 언어 임베딩 모델은 서로 다른 언어의 텍스트라도 의미가 유사하면 벡터 공간에서 가깝게 위치하도록 학습됨. \n",
    "- 이러한 모델을 사용하면 한국어로 질문해도 영어 문서를 검색하거나, 그 반대의 경우도 가능하게 됨.\n",
    "\n",
    "**전략:**\n",
    "1.  한국어 문서와 영어 문서를 모두 준비.\n",
    "2.  교차 언어 성능이 우수한 임베딩 모델 (예: OpenAI `text-embedding-3-small`, HuggingFace `BAAI/bge-m3`, Ollama `bge-m3`)을 선택.\n",
    "3.  모든 문서를 선택한 임베딩 모델로 임베딩하여 하나의 벡터 저장소에 저장.\n",
    "4.  사용자 질문(한국어 또는 영어)을 동일한 임베딩 모델로 임베딩하여 벡터 저장소에서 유사 문서를 검색.\n",
    "\n",
    "**장점:**\n",
    "- 구현이 비교적 간단합니다. 단일 임베딩 모델과 단일 벡터 저장소만 관리하면 됨.\n",
    "\n",
    "**단점:**\n",
    "- 임베딩 모델의 교차 언어 성능에 크게 의존하며, 모델 성능이 부족하면 검색 정확도가 떨어질 수 있음\n",
    "- 동일 언어 내 검색(예: 한국어 질문 -> 한국어 문서)보다 성능이 낮을 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def68998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4059b6",
   "metadata": {},
   "source": [
    "`(1) 다국어 문서 로드 및 전처리` 예시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4b4ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 텍스트 파일: ['./data\\\\Rivian_KR.txt', './data\\\\Tesla_KR.txt']\n",
      "영어 텍스트 파일: ['./data\\\\Rivian_EN.txt', './data\\\\Tesla_EN.txt']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os \n",
    "korean_txt_files = glob(os.path.join('./data', '*_KR.txt')) \n",
    "english_txt_files = glob(os.path.join('./data', '*_EN.txt'))\n",
    "\n",
    "print(\"한국어 텍스트 파일:\", korean_txt_files)\n",
    "print(\"영어 텍스트 파일:\", english_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69a4d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 원본 Document 수: 2\n",
      "영어 원본 Document 수: 2\n",
      "\n",
      "한국어 데이터 샘플 (첫 번째 문서 메타데이터): {'source': './data\\\\Rivian_KR.txt'}\n",
      "한국어 데이터 샘플 (첫 번째 문서 내용 일부): 2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당한 투자를 통해 비약적인 성장을 거듭하며 미시간과 베이 지역에 연구 시설을 설립했습니다. 주요 공급업체와의 거리를 좁히기 위해 본사를 미시간주 리보니아로 이전했습니다.\n",
      "\n",
      "리비안의 \n",
      "\n",
      "영어 데이터 샘플 (첫 번째 문서 메타데이터): {'source': './data\\\\Rivian_EN.txt'}\n",
      "영어 데이터 샘플 (첫 번째 문서 내용 일부): Founded in 2009 by MIT PhD graduate RJ Scaringe, Rivian is an innovative American electric vehicle manufacturer. Initially focused on autonomous electric cars from 2011, Rivian's significant growth ph\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def load_text_files(txt_files):\n",
    "    data = []\n",
    "    for text_file in txt_files:\n",
    "        # TextLoader는 기본적으로 utf-8을 가정하나, 명시하는 것이 좋음\n",
    "        loader = TextLoader(text_file, encoding='utf-8') \n",
    "        data.extend(loader.load())\n",
    "    return data\n",
    "\n",
    "korean_data = load_text_files(korean_txt_files)\n",
    "english_data = load_text_files(english_txt_files)\n",
    "\n",
    "print(f\"한국어 원본 Document 수: {len(korean_data)}\")\n",
    "print(f\"영어 원본 Document 수: {len(english_data)}\")\n",
    "\n",
    "if korean_data:\n",
    "    print(\"\\n한국어 데이터 샘플 (첫 번째 문서 메타데이터):\", korean_data[0].metadata)\n",
    "    print(\"한국어 데이터 샘플 (첫 번째 문서 내용 일부):\", korean_data[0].page_content[:200])\n",
    "if english_data:\n",
    "    print(\"\\n영어 데이터 샘플 (첫 번째 문서 메타데이터):\", english_data[0].metadata)\n",
    "    print(\"영어 데이터 샘플 (첫 번째 문서 내용 일부):\", english_data[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9521a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 한국어 문서(청크) 수: 6\n",
      "분할된 영어 문서(청크) 수: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# 문장을 구분하여 분할 (마침표, 느낌표, 물음표 다음에 공백이 오는 경우 문장의 끝으로 판단)\n",
    "# 이 모델은 교차 언어 성능이 뛰어나므로, 다국어 문서 처리에 적합\n",
    "# why?? \"대규모 다국어 데이터셋으로 사전 학습\" 및 \"정교한 토크나이저\"\n",
    "text_splitter_multilingual = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"text-embedding-3-small\", # 이 모델의 토크나이저 사용\n",
    "    separator=r\"[.!?]\\s+\",      # 문장 끝 구두점 뒤 공백 기준 분할 (정규식)\n",
    "    chunk_size=200,              # 목표 청크 토큰 수 (CharacterTextSplitter는 이 값을 엄격히 따르지 않을 수 있음)\n",
    "    chunk_overlap=20,            # 청크 간 중복 토큰 수\n",
    "    is_separator_regex=True,     # separator를 정규식으로 해석\n",
    "    keep_separator=False,        # 구분자(공백) 제거\n",
    ")\n",
    "\n",
    "korean_docs_split = []\n",
    "english_docs_split = []\n",
    "\n",
    "if korean_data:\n",
    "    korean_docs_split = text_splitter_multilingual.split_documents(korean_data)\n",
    "if english_data:\n",
    "    english_docs_split = text_splitter_multilingual.split_documents(english_data)\n",
    "\n",
    "print(f\"분할된 한국어 문서(청크) 수: {len(korean_docs_split)}\")\n",
    "print(f\"분할된 영어 문서(청크) 수: {len(english_docs_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52633cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 한국어 분할 청크 샘플 ---\n",
      "\n",
      "[청크 1] (길이: 204자, 메타데이터: {'source': './data\\\\Rivian_KR.txt'})\n",
      "2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다[.!?]\\s+2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당한 투자를 통해 비약적인 성장을 거듭하며 미시간과 베이 지역에 연구 시설을 설립\n",
      "[...]\n",
      "\n",
      "[청크 2] (길이: 178자, 메타데이터: {'source': './data\\\\Rivian_KR.txt'})\n",
      "리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미드십 엔진 하이브리드 쿠페 스포츠카 R1(원래 이름은 아베라(Avera))이었습니다[.!?]\\s+이 차량은 모듈식 캡슐 구조와 쉽게 교체 가능한 차체 패널을 특징으로 하며, 2013년 말에서 201\n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "# 한국어 분할 청크 확인 (처음 2개)\n",
    "if korean_docs_split:\n",
    "    print(\"--- 한국어 분할 청크 샘플 ---\")\n",
    "    for i, doc in enumerate(korean_docs_split[:2]):\n",
    "        print(f\"\\n[청크 {i+1}] (길이: {len(doc.page_content)}자, 메타데이터: {doc.metadata})\")\n",
    "        print(doc.page_content[:150])\n",
    "        if len(doc.page_content) > 150: print(\"[...]\")\n",
    "else:\n",
    "    print(\"분할된 한국어 문서가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34634419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 영어 분할 청크 샘플 ---\n",
      "\n",
      "[청크 1] (길이: 757자, 메타데이터: {'source': './data\\\\Rivian_EN.txt'})\n",
      "Founded in 2009 by MIT PhD graduate RJ Scaringe, Rivian is an innovative American electric vehicle manufacturer[.!?]\\s+Initially focused on autonomous\n",
      "[...]\n",
      "\n",
      "[청크 2] (길이: 372자, 메타데이터: {'source': './data\\\\Rivian_EN.txt'})\n",
      "Rivian also considered various versions, including a diesel hybrid, a racing version named R1 GT for a Brazilian one-make series, a four-door sedan, a\n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "# 영어 분할 청크 확인 (처음 2개)\n",
    "if english_docs_split:\n",
    "    print(\"\\n--- 영어 분할 청크 샘플 ---\")\n",
    "    for i, doc in enumerate(english_docs_split[:2]):\n",
    "        print(f\"\\n[청크 {i+1}] (길이: {len(doc.page_content)}자, 메타데이터: {doc.metadata})\")\n",
    "        print(doc.page_content[:150])\n",
    "        if len(doc.page_content) > 150: print(\"[...]\")\n",
    "else:\n",
    "    print(\"분할된 영어 문서가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699cb7f",
   "metadata": {},
   "source": [
    "`(2) 문서 임베딩 및 벡터저장소에 저장 ` 예시\n",
    "1) 선택한 교차 언어 임베딩 모델들(OpenAI, HuggingFace, Ollama)을 사용하여 한국어와 영어 문서를 함께 임베딩하고, 각 모델별로 ChromaDB 벡터 저장소에 저장\n",
    "2) 이렇게 하면 각 임베딩 모델의 교차 언어 검색 성능을 비교 가능능\n",
    "\n",
    "- `collection_name`: 벡터 저장소 내에서 특정 문서 그룹을 식별하는 이름이며, 모델별로 다른 이름을 사용 가능.\n",
    "- `persist_directory`: 벡터 저장소 데이터를 디스크에 저장할 경로이며, 지정하면 저장소가 유지되어 재사용 가능.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8383d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama 임베딩 모델 (bge-m3) 준비 완료.\n",
      "총 분할된 문서(청크) 수: 10\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# OpenAI 임베딩 모델 (교차 언어 지원 우수)\n",
    "embeddings_openai_small_cl = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Hugging Face 임베딩 모델 (BAAI/bge-m3, 교차 언어 지원 우수)\n",
    "embeddings_huggingface_bge_m3_cl = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\", \n",
    "    model_kwargs={'device': 'cpu'}, # CPU 또는 'cuda' \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Ollama 임베딩 모델 (bge-m3 또는 nomic-embed-text, 교차 언어 지원 가능성 확인 필요)\n",
    "# Ollama 서버 및 해당 모델이 준비되어 있어야 함\n",
    "embeddings_ollama_bge_m3_cl = None\n",
    "ollama_ready_cl = False\n",
    "try:\n",
    "    embeddings_ollama_bge_m3_cl = OllamaEmbeddings(model=\"bge-m3\") \n",
    "    # embeddings_ollama_nomic_cl = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    print(\"Ollama 임베딩 모델 (bge-m3) 준비 완료.\")\n",
    "    ollama_ready_cl = True\n",
    "except Exception as e:\n",
    "    print(f\"Ollama 연결 또는 모델 로드 실패 (교차언어용): {e}\")\n",
    "    print(\"Ollama (bge-m3) 교차언어 예제를 실행하려면 Ollama 서버를 실행하고 'bge-m3' 모델을 pull 해주세요.\")\n",
    "\n",
    "all_split_docs = korean_docs_split + english_docs_split\n",
    "print(f\"총 분할된 문서(청크) 수: {len(all_split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fea85ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 임베딩으로 벡터 저장소 구축 중...\n",
      "OpenAI 벡터 저장소 문서 수: 10\n",
      "\n",
      "Hugging Face (BAAI/bge-m3) 임베딩으로 벡터 저장소 구축 중...\n",
      "Hugging Face 벡터 저장소 문서 수: 10\n",
      "\n",
      "Ollama (bge-m3) 임베딩으로 벡터 저장소 구축 중...\n",
      "Ollama 벡터 저장소 문서 수: 10\n"
     ]
    }
   ],
   "source": [
    "# 다국어 벡터 저장소 구축\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "db_openai_cl = None\n",
    "db_huggingface_cl = None\n",
    "db_ollama_cl = None\n",
    "\n",
    "if all_split_docs: # 분할된 문서가 있을 경우에만 실행\n",
    "    print(\"OpenAI 임베딩으로 벡터 저장소 구축 중...\")\n",
    "    db_openai_cl = Chroma.from_documents(\n",
    "        documents=all_split_docs, \n",
    "        embedding=embeddings_openai_small_cl,\n",
    "        collection_name=\"db_openai_crosslingual_v2\", # 컬렉션 이름 변경 또는 기존 삭제 후 생성\n",
    "        persist_directory=\"./chroma_db_cl\", # 디렉토리 구분\n",
    "    )\n",
    "    print(f\"OpenAI 벡터 저장소 문서 수: {db_openai_cl._collection.count()}\")\n",
    "\n",
    "    print(\"\\nHugging Face (BAAI/bge-m3) 임베딩으로 벡터 저장소 구축 중...\")\n",
    "    db_huggingface_cl = Chroma.from_documents(\n",
    "        documents=all_split_docs, \n",
    "        embedding=embeddings_huggingface_bge_m3_cl,\n",
    "        collection_name=\"db_huggingface_crosslingual_v2\",\n",
    "        persist_directory=\"./chroma_db_cl\",\n",
    "    )\n",
    "    print(f\"Hugging Face 벡터 저장소 문서 수: {db_huggingface_cl._collection.count()}\")\n",
    "\n",
    "    if ollama_ready_cl and embeddings_ollama_bge_m3_cl:\n",
    "        print(\"\\nOllama (bge-m3) 임베딩으로 벡터 저장소 구축 중...\")\n",
    "        db_ollama_cl = Chroma.from_documents(\n",
    "            documents=all_split_docs, \n",
    "            embedding=embeddings_ollama_bge_m3_cl,\n",
    "            collection_name=\"db_ollama_crosslingual_v2\",\n",
    "            persist_directory=\"./chroma_db_cl\",\n",
    "        )\n",
    "        print(f\"Ollama 벡터 저장소 문서 수: {db_ollama_cl._collection.count()}\")\n",
    "    else:\n",
    "        print(\"\\nOllama가 준비되지 않아 Ollama 벡터 저장소 구축을 건너뜁니다.\")\n",
    "else:\n",
    "    print(\"분할된 문서가 없어 벡터 저장소 구축을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592085d",
   "metadata": {},
   "source": [
    "`(3) RAG 성능 비교 `  \n",
    "\n",
    "- 간단한 RAG 체인을 구성하여 각 임베딩 모델 기반의 벡터 저장소가 한국어 질문과 영어 질문에 대해 어떻게 응답하는지 비교\n",
    "\n",
    "**RAG 체인 구성 요소:**\n",
    "- **Retriever**: 벡터 저장소에서 유사 문서를 검색하며, `as_retriever()`로 변환하고, `search_kwargs={'k': 2}`로 상위 2개 문서를 가져오도록 설정.\n",
    "- **Prompt Template**: LLM에 전달할 프롬프트를 정의하며, 컨텍스트(검색된 문서)와 질문을 포함.\n",
    "- **LLM**: 질문과 컨텍스트를 바탕으로 최종 답변을 생성할 언어 모델 (예: `ChatOpenAI`)\n",
    "- **Output Parser**: LLM의 출력(주로 `AIMessage` 객체)에서 실제 텍스트 답변만 추출(`StrOutputParser`)\n",
    "- **RunnablePassthrough / format_docs**: LangChain Expression Language (LCEL)에서 데이터 흐름을 관리하고, 검색된 `Document` 객체 리스트를 LLM 프롬프트에 적합한 문자열 형태로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30396f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인 생성\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based ONLY on the following context.\n",
    "Do not use any external information or knowledge. \n",
    "If the answer is not in the context, answer \"잘 모르겠습니다.\".\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_cl = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 문서 포맷터 함수: Document 객체 리스트를 단일 문자열로 결합\n",
    "def format_docs_cl(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# LLM 모델 생성 (답변 생성용)\n",
    "llm_cl = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 체인 생성 함수 (벡터 저장소를 인자로 받음)\n",
    "def create_rag_chain_cl(vectorstore):\n",
    "    if not vectorstore:\n",
    "        # 벡터 저장소가 None이면, 실행 불가능한 더미 체인을 반환하거나 예외 처리\n",
    "        # 여기서는 간단히 None을 반환하여 호출하는 쪽에서 확인하도록 함\n",
    "        return None\n",
    "        \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 2}) # 상위 2개 문서 검색\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs_cl , \"question\": RunnablePassthrough()} # 검색 및 포맷팅\n",
    "        | prompt_template_cl  # 프롬프트 적용\n",
    "        | llm_cl              # LLM으로 답변 생성\n",
    "        | StrOutputParser()   # 출력 파싱 (텍스트만 추출)\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "346e5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI RAG 체인 생성 완료\n",
      "HuggingFace RAG 체인 생성 완료\n",
      "Ollama RAG 체인 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# 각 벡터 저장소에 대한 RAG 체인 생성\n",
    "rag_chain_openai_cl = create_rag_chain_cl(db_openai_cl)\n",
    "rag_chain_huggingface_cl = create_rag_chain_cl(db_huggingface_cl)\n",
    "rag_chain_ollama_cl = create_rag_chain_cl(db_ollama_cl)\n",
    "\n",
    "if rag_chain_openai_cl: print(\"OpenAI RAG 체인 생성 완료\")\n",
    "else: print(\"OpenAI RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_huggingface_cl: print(\"HuggingFace RAG 체인 생성 완료\")\n",
    "else: print(\"HuggingFace RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_ollama_cl: print(\"Ollama RAG 체인 생성 완료\")\n",
    "else: print(\"Ollama RAG 체인 생성 실패 (벡터 저장소 없음 또는 Ollama 준비 안됨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e10620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 한국어 쿼리: '테슬라 창업자는 누구인가요?' ---\n",
      "OpenAI 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "Hugging Face (bge-m3) 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "Ollama (bge-m3) 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko_cl = \"테슬라 창업자는 누구인가요?\" # 예시 문서에 관련 내용이 있어야 함\n",
    "print(f\"\\n--- 한국어 쿼리: '{query_ko_cl}' ---\")\n",
    "\n",
    "if rag_chain_openai_cl:\n",
    "    output_openai_ko = rag_chain_openai_cl.invoke(query_ko_cl)\n",
    "    print(f\"OpenAI 응답 (KO): {output_openai_ko}\")\n",
    "else:\n",
    "    print(\"OpenAI RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_huggingface_cl:\n",
    "    output_huggingface_ko = rag_chain_huggingface_cl.invoke(query_ko_cl)\n",
    "    print(f\"Hugging Face (bge-m3) 응답 (KO): {output_huggingface_ko}\")\n",
    "else:\n",
    "    print(\"HuggingFace RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_ollama_cl:\n",
    "    output_ollama_ko = rag_chain_ollama_cl.invoke(query_ko_cl)\n",
    "    print(f\"Ollama (bge-m3) 응답 (KO): {output_ollama_ko}\")\n",
    "else:\n",
    "    print(\"Ollama RAG 체인이 없어 실행 불가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f373ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 영어 쿼리: 'Who is the founder of Tesla?' ---\n",
      "OpenAI 응답 (EN): 마틴 에버하드와 마크 타페닝입니다.\n",
      "Hugging Face (bge-m3) 응답 (EN): 마틴 에버하드와 마크 타페닝입니다.\n",
      "Ollama (bge-m3) 응답 (EN): 마틴 에버하드와 마크 타페닝입니다.\n"
     ]
    }
   ],
   "source": [
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en_cl = \"Who is the founder of Tesla?\" # 예시 문서에 관련 내용이 있어야 함\n",
    "print(f\"\\n--- 영어 쿼리: '{query_en_cl}' ---\")\n",
    "\n",
    "if rag_chain_openai_cl:\n",
    "    output_openai_en = rag_chain_openai_cl.invoke(query_en_cl)\n",
    "    print(f\"OpenAI 응답 (EN): {output_openai_en}\")\n",
    "else:\n",
    "    print(\"OpenAI RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_huggingface_cl:\n",
    "    output_huggingface_en = rag_chain_huggingface_cl.invoke(query_en_cl)\n",
    "    print(f\"Hugging Face (bge-m3) 응답 (EN): {output_huggingface_en}\")\n",
    "else:\n",
    "    print(\"HuggingFace RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_ollama_cl:\n",
    "    output_ollama_en = rag_chain_ollama_cl.invoke(query_en_cl)\n",
    "    print(f\"Ollama (bge-m3) 응답 (EN): {output_ollama_en}\")\n",
    "else:\n",
    "    print(\"Ollama RAG 체인이 없어 실행 불가\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4a32b",
   "metadata": {},
   "source": [
    "### 1-2 언어 감지 및 자동번역 통합 \n",
    "\n",
    "- 이 전략은 주로 단일 언어(예: 한국어)로 구성된 문서 저장소를 가지고 있을 때, 다양한 언어의 사용자 질문을 처리하기 위해 사용\n",
    "- 또는 그 반대의 경우도 가능\n",
    "\n",
    "**전략:**\n",
    "1.  **언어 감지**: 사용자 질문의 언어를 감지 (예: `langdetect` 라이브러리 사용).\n",
    "2.  **질문 번역**: 감지된 질문 언어가 문서 저장소의 주 언어와 다르면, 질문을 문서 저장소의 언어로 번역(예: `deepl` API 사용).\n",
    "3.  **RAG 처리**: 번역된 질문을 사용하여 일반적인 RAG 체인을 통해 답변을 생성 (이때 답변은 문서 저장소의 언어로 생성됨).\n",
    "4.  **답변 번역**: 생성된 답변의 언어가 원래 질문의 언어와 다르면, 답변을 원래 질문의 언어로 다시 번역\n",
    "\n",
    "**장점:**\n",
    "- 단일 언어에 최적화된 임베딩 모델과 LLM을 활용하여 해당 언어에서의 검색 및 답변 생성 품질을 극대화할 수 있음.\n",
    "- 다양한 언어의 사용자 질문을 지원할 수 있음.\n",
    "\n",
    "**단점:**\n",
    "- **번역 품질 의존성**: 번역기의 성능에 따라 전체 시스템의 품질이 크게 좌우됨. 번역 오류는 잘못된 검색 결과나 부정확한 답변으로 이어질 수 있음.\n",
    "- **지연 시간 증가**: 질문과 답변 번역 과정에서 추가적인 API 호출로 인해 전체 응답 시간이 늘어남.\n",
    "- **번역 비용**: 상용 번역 API(예: DeepL, Google Translate) 사용 시 비용이 발생.\n",
    "- **언어 감지 오류**: 언어 감지가 정확하지 않으면 불필요하거나 잘못된 번역이 발생 가능성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928e845",
   "metadata": {},
   "source": [
    "`(1) 한국어 문서 벡터저장소 로드 (또는 생성)`  \n",
    "\n",
    "- 한국어 문서만으로 구성된 벡터 저장소가 이미 있다고 가정.\n",
    "- 만약 없다면, 한국어 문서(`korean_docs_split`)와 한국어에 강한 임베딩 모델(예: `OpenAIEmbeddings`, 또는 한국어 특화 HuggingFace 모델)을 사용하여 새로 생성할 수 있음.\n",
    "\n",
    "- `chroma_test`라는 이름의 기존 컬렉션을 로드하려고 시도.\n",
    "- 이 컬렉션은 이전 단계나 한국어 데이터로 이미 만들어졌다고 가정.\n",
    "- 만약 해당 컬렉션이 없다면, `Chroma.from_documents`를 사용하여 새로 만들어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f3dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'db_openai_crosslingual_v2' 벡터 저장소 로드 완료. 문서 수: 10\n"
     ]
    }
   ],
   "source": [
    "# 한국어 문서로 저장되어 있는 벡터 저장소 로드 또는 생성\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_for_ko_store = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", \n",
    ")\n",
    "\n",
    "# 기존에 'chroma_test' 컬렉션이 한국어 데이터로 만들어져 있다고 가정.\n",
    "# 없다면, korean_docs_split을 사용해 새로 생성해야 함.\n",
    "# 예시: db_korean_only = Chroma.from_documents(documents=korean_docs_split, embedding=embeddings_for_ko_store, collection_name=\"korean_store_v1\", persist_directory=\"./chroma_db_ko_only\")\n",
    "\n",
    "COLLECTION_NAME_KO_ONLY = \"db_openai_crosslingual_v2\" # (한국어+영어 데이터 포함)\n",
    "PERSIST_DIR_KO_ONLY = \"./chroma_db_cl\"\n",
    "\n",
    "try:\n",
    "    # 여기서는 1-1에서 만든 db_openai_cl (한국어+영어 문서 포함)을 재사용\n",
    "    # 순수 한국어 저장소를 원한다면, 해당 저장소를 로드하거나 새로 만들어야 함.\n",
    "    vectorstore_ko_trans = Chroma(\n",
    "        embedding_function=embeddings_for_ko_store,\n",
    "        collection_name=COLLECTION_NAME_KO_ONLY, # 1-1에서 사용한 OpenAI 컬렉션 이름\n",
    "        persist_directory=PERSIST_DIR_KO_ONLY    # 1-1에서 사용한 디렉토리\n",
    "    )\n",
    "    print(f\"'{COLLECTION_NAME_KO_ONLY}' 벡터 저장소 로드 완료. 문서 수: {vectorstore_ko_trans._collection.count()}\")\n",
    "    # 이 저장소는 실제로는 한국어와 영어가 섞여있지만, 지금은 한국어 중심 저장소로 간주하고 진행\n",
    "    # 이상적으로는 순수 한국어 문서로 구성된 저장소를 사용하는 것이 이 시나리오에 더 적합\n",
    "except Exception as e:\n",
    "    print(f\"벡터 저장소 '{COLLECTION_NAME_KO_ONLY}' 로드 실패: {e}\")\n",
    "    print(\"이전 단계에서 해당 이름의 컬렉션이 생성되었는지, 경로가 올바른지 확인하세요.\")\n",
    "    vectorstore_ko_trans = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f1fc2",
   "metadata": {},
   "source": [
    "**언어 감지 및 번역 도구 설정**\n",
    "\n",
    "- `langdetect`: 텍스트의 언어를 감지\n",
    "- `deepl`: 고품질 번역을 제공하는 API 서비스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a269aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepL 번역기 초기화 완료.\n",
      "번역 필요: 감지된 언어 'en' -> 목표 언어 'KO'\n",
      "원본 (EN): Hello. How are you today?\n",
      "감지된 언어: en\n",
      "번역된 텍스트 (KO): 안녕하세요. 오늘은 어떠세요?\n",
      "----\n",
      "번역 필요: 감지된 언어 'ko' -> 목표 언어 'EN'\n",
      "원본 (KO): 안녕하세요. 오늘 기분이 어떠신가요?\n",
      "감지된 언어: ko\n",
      "번역된 텍스트 (EN-US): hello. How are you feeling today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import deepl\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# 언어 감지 결과의 일관성을 위해 시드 설정 (선택 사항)\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "deepl_api_key = os.getenv('DEEPL_API_KEY')\n",
    "translator = None\n",
    "if deepl_api_key:\n",
    "    translator = deepl.Translator(deepl_api_key)\n",
    "    print(\"DeepL 번역기 초기화 완료.\")\n",
    "else:\n",
    "    print(\"DEEPL_API_KEY가 설정되지 않았습니다. 번역 기능을 사용하려면 API 키를 설정하세요.\")\n",
    "\n",
    "def detect_and_translate(text: str, target_lang_deepl: str = 'KO', target_lang_detect: str = 'ko'):\n",
    "    if not translator:\n",
    "        print(\"번역기가 설정되지 않아 원본 텍스트를 반환합니다.\")\n",
    "        # 번역기 없어도 언어 감지는 시도\n",
    "        try:\n",
    "            detected_lang = detect(text)\n",
    "            return text, detected_lang\n",
    "        except Exception as e:\n",
    "            print(f\"언어 감지 실패: {e}. 원본 텍스트 및 'unknown' 반환.\")\n",
    "            return text, \"unknown\"\n",
    "\n",
    "    try:\n",
    "        detected_lang = detect(text) # langdetect는 'ko', 'en' 등 소문자 코드를 반환\n",
    "    except Exception as e:\n",
    "        print(f\"언어 감지 실패: {e}. 원본 텍스트 사용.\")\n",
    "        return text, \"unknown\"\n",
    "\n",
    "    # target_lang_deepl ('EN', 'KO', 'EN-US' 등)을 langdetect 스타일 ('en', 'ko')로 변환하여 비교하기 위함\n",
    "    simple_target_deepl_code = ''\n",
    "    temp_target_deepl_upper = target_lang_deepl.upper()\n",
    "    if temp_target_deepl_upper.startswith('EN'): # 'EN', 'EN-US', 'EN-GB' 등\n",
    "        simple_target_deepl_code = 'en'\n",
    "    elif temp_target_deepl_upper == 'KO':\n",
    "        simple_target_deepl_code = 'ko'\n",
    "    # 다른 언어에 대한 규칙 추가 가능\n",
    "    else:\n",
    "        # 일반적인 경우 (예: 'FR', 'JA') DeepL 코드는 2자리이므로 앞 두 글자를 소문자로 사용\n",
    "        if len(target_lang_deepl) >= 2:\n",
    "            simple_target_deepl_code = target_lang_deepl[:2].lower()\n",
    "        else: # 매우 드문 경우\n",
    "            simple_target_deepl_code = target_lang_deepl.lower()\n",
    "\n",
    "    # 감지된 언어(소문자)와 실제 목표 언어(단순화된 소문자 코드)를 비교\n",
    "    if detected_lang.lower() != simple_target_deepl_code:\n",
    "        print(f\"번역 필요: 감지된 언어 '{detected_lang}' -> 목표 언어 '{target_lang_deepl}'\")\n",
    "        try:\n",
    "            # DeepL은 'EN-US', 'EN-GB' 등을 지원. 단순히 'EN'으로 하면 기본 'EN-US'로 될 수 있음.\n",
    "            # langdetect가 'en'을 반환하면, DeepL의 'EN-US'로 매핑하는 것이 안전할 수 있음.\n",
    "            target_lang_deepl_actual = target_lang_deepl # 기본값 설정\n",
    "            if target_lang_deepl.upper() == 'EN': # 만약 영어로 번역해야 한다면\n",
    "                target_lang_deepl_actual = 'EN-US' # 미국 영어로 지정 (DeepL 기본 동작과 일치시키거나 명시적 지정)\n",
    "            # 다른 target_lang_deepl 값들은 그대로 사용 (예: 'KO', 'FR', 'DE')\n",
    "            # (위의 if문에서 target_lang_deepl_actual = target_lang_deepl 로 이미 설정됨)\n",
    "\n",
    "            result = translator.translate_text(text, target_lang=target_lang_deepl_actual)\n",
    "            return str(result), detected_lang\n",
    "        except Exception as e:\n",
    "            print(f\"번역 실패: {e}. 원본 텍스트 사용.\")\n",
    "            return text, detected_lang\n",
    "    else: # 감지된 언어와 (단순화된) 목표 언어가 같으면 번역 안 함\n",
    "        print(f\"번역 불필요: 감지된 언어 '{detected_lang}' (단순화: '{detected_lang.lower()}') == 목표 언어 '{target_lang_deepl}' (단순화: '{simple_target_deepl_code}'). 원본 텍스트 반환.\")\n",
    "        return text, detected_lang\n",
    "\n",
    "# 문서 번역 테스트\n",
    "if translator:\n",
    "    sample_text_en = \"Hello. How are you today?\"\n",
    "    translated_text, detected_lang_sample = detect_and_translate(sample_text_en, target_lang_deepl='KO') # target_lang_detect 기본값 'ko' 사용\n",
    "    print(f\"원본 (EN): {sample_text_en}\")\n",
    "    print(f\"감지된 언어: {detected_lang_sample}\")\n",
    "    print(f\"번역된 텍스트 (KO): {translated_text}\")\n",
    "    print(\"----\")\n",
    "\n",
    "    sample_text_ko = \"안녕하세요. 오늘 기분이 어떠신가요?\"\n",
    "    translated_text_en, detected_lang_sample_ko = detect_and_translate(sample_text_ko, target_lang_deepl='EN') # target_lang_detect 기본값 'ko' 사용\n",
    "    print(f\"원본 (KO): {sample_text_ko}\")\n",
    "    print(f\"감지된 언어: {detected_lang_sample_ko}\")\n",
    "    print(f\"번역된 텍스트 (EN-US): {translated_text_en}\")\n",
    "else:\n",
    "    print(\"DeepL 번역기가 없어 번역 테스트를 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73f06d",
   "metadata": {},
   "source": [
    "`(2) RAG 체인 성능 평가 `  \n",
    "\n",
    "- `detect_and_translate` 함수를 통합하여, 질문과 답변을 필요에 따라 번역하는 RAG 체인을 생성.\n",
    "- 이 체인은 내부적으로 한국어 컨텍스트를 기반으로 답변을 생성하고, 원래 질문 언어에 맞춰 최종 답변을 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f65af2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 통합 RAG 체인 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "lang_rag_chain_translate = None\n",
    "if vectorstore_ko_trans: # 한국어(중심) 벡터 저장소가 로드되었는지 확인\n",
    "    retriever_ko_trans = vectorstore_ko_trans.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "    # RAG 체인 구성 (5-1의 프롬프트, LLM, 포맷터 재사용)\n",
    "    lang_rag_chain_translate = (\n",
    "        {\"context\": retriever_ko_trans | format_docs_cl , \"question\": RunnablePassthrough()}\n",
    "        | prompt_template_cl # 1-1에서 정의한 프롬프트 템플릿\n",
    "        | llm_cl             # 1-1에서 정의한 LLM\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"번역 통합 RAG 체인 생성 완료.\")\n",
    "else:\n",
    "    print(\"한국어(중심) 벡터 저장소가 없어 번역 통합 RAG 체인을 생성할 수 없습니다.\")\n",
    "\n",
    "# 번역 통합 체인 실행 함수\n",
    "def run_lang_rag_chain_with_translation(query: str):\n",
    "    if not lang_rag_chain_translate:\n",
    "        return \"번역 통합 RAG 체인이 준비되지 않았습니다.\"\n",
    "    if not translator: # DeepL 번역기가 준비되지 않았으면\n",
    "        print(\"DeepL 번역기가 없어 번역 없이 원본 질문으로 RAG를 실행합니다.\")\n",
    "        # 이 경우, 질문과 문서의 언어가 일치하지 않으면 성능이 저하 가능성 \n",
    "        return lang_rag_chain_translate.invoke(query)\n",
    "\n",
    "    # 1. 질문 언어 감지 및 한국어(문서 저장소 언어)로 번역\n",
    "    translated_query, original_lang_code_lc = detect_and_translate(query, target_lang_deepl='KO', target_lang_detect='ko')\n",
    "    print(f\"원본 질문 ({original_lang_code_lc}): {query}\")\n",
    "    if original_lang_code_lc.lower() != 'ko':\n",
    "        print(f\"번역된 질문 (KO): {translated_query}\")\n",
    "    \n",
    "    # 2. 번역된 (한국어) 질문으로 RAG 체인 실행 -> 답변은 한국어로 생성됨\n",
    "    output_ko = lang_rag_chain_translate.invoke(translated_query)\n",
    "    print(f\"RAG 답변 (KO): {output_ko}\")\n",
    "    \n",
    "    # 3. 생성된 한국어 답변을 원래 질문 언어로 다시 번역\n",
    "    if original_lang_code_lc.lower() != 'ko':\n",
    "        # DeepL target_lang은 대문자 (예: 'EN-US', 'JA')\n",
    "        # langdetect는 소문자 (예: 'en', 'ja')\n",
    "        # DeepL에서 지원하는 형태로 변환 필요\n",
    "        target_lang_deepl_for_answer = original_lang_code_lc.upper()\n",
    "        if target_lang_deepl_for_answer == 'EN': # langdetect가 'en'이면\n",
    "            target_lang_deepl_for_answer = 'EN-US' # DeepL용 'EN-US'로\n",
    "        # 다른 언어들도 필요시 매핑 규칙 추가 (예: 'ZH-CN' -> 'ZH')\n",
    "\n",
    "        final_output, _ = detect_and_translate(output_ko, target_lang_deepl=target_lang_deepl_for_answer, target_lang_detect='ko')\n",
    "        print(f\"최종 번역된 답변 ({target_lang_deepl_for_answer}): {final_output}\")\n",
    "        return final_output\n",
    "    else:\n",
    "        return output_ko # 원래 질문이 한국어였으면 번역 없이 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5586123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 실행: 한국어 질문 '테슬라 창업자는 누구인가요?' ---\n",
      "번역 불필요: 감지된 언어 'ko' (단순화: 'ko') == 목표 언어 'KO' (단순화: 'ko'). 원본 텍스트 반환.\n",
      "원본 질문 (ko): 테슬라 창업자는 누구인가요?\n",
      "RAG 답변 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "최종 결과 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "\n",
      "--- 실행: 영어 질문 'Who is the founder of Tesla?' ---\n",
      "번역 필요: 감지된 언어 'en' -> 목표 언어 'KO'\n",
      "원본 질문 (en): Who is the founder of Tesla?\n",
      "번역된 질문 (KO): 테슬라의 창립자는 누구인가요?\n",
      "RAG 답변 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "번역 필요: 감지된 언어 'ko' -> 목표 언어 'EN-US'\n",
      "최종 번역된 답변 (EN-US): Martin Everhard and Mark Tappening.\n",
      "최종 결과 (EN): Martin Everhard and Mark Tappening.\n",
      "\n",
      "--- 실행: 일본어 질문 'テスラの創業者は誰ですか？' ---\n",
      "번역 필요: 감지된 언어 'ja' -> 목표 언어 'KO'\n",
      "원본 질문 (ja): テスラの創業者は誰ですか？\n",
      "번역된 질문 (KO): 테슬라의 창업자는 누구인가요?\n",
      "RAG 답변 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "번역 필요: 감지된 언어 'ko' -> 목표 언어 'JA'\n",
      "최종 번역된 답변 (JA): マーティン・エバーハードとマーク・タフェニングです。\n",
      "최종 결과 (JA): マーティン・エバーハードとマーク・タフェニングです。\n"
     ]
    }
   ],
   "source": [
    "if lang_rag_chain_translate and translator:\n",
    "    # 한국어 쿼리에 대한 성능 평가 (번역 불필요)\n",
    "    query_ko_trans = \"테슬라 창업자는 누구인가요?\"\n",
    "    print(f\"\\n--- 실행: 한국어 질문 '{query_ko_trans}' ---\")\n",
    "    output_ko_final = run_lang_rag_chain_with_translation(query_ko_trans)\n",
    "    print(f\"최종 결과 (KO): {output_ko_final}\")\n",
    "\n",
    "    # 영어 쿼리에 대한 성능 평가 (질문 KO로 번역 -> 답변 KO 생성 -> 답변 EN으로 번역)\n",
    "    query_en_trans = \"Who is the founder of Tesla?\"\n",
    "    print(f\"\\n--- 실행: 영어 질문 '{query_en_trans}' ---\")\n",
    "    output_en_final = run_lang_rag_chain_with_translation(query_en_trans)\n",
    "    print(f\"최종 결과 (EN): {output_en_final}\")\n",
    "\n",
    "    # 다른 언어 테스트 (예: 일본어, DeepL 무료 버전은 제한적일 수 있음)\n",
    "    query_ja_trans = \"テスラの創業者は誰ですか？\"\n",
    "    print(f\"\\n--- 실행: 일본어 질문 '{query_ja_trans}' ---\")\n",
    "    output_ja_final = run_lang_rag_chain_with_translation(query_ja_trans)\n",
    "    print(f\"최종 결과 (JA): {output_ja_final}\")\n",
    "else:\n",
    "    print(\"\\n번역 통합 RAG 체인 또는 DeepL 번역기가 준비되지 않아 실행을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708249ce",
   "metadata": {},
   "source": [
    "### 1-3 언어 감지 및 벡터저장소 라우팅\n",
    "\n",
    "- 이 전략은 각 언어별로 최적화된 문서 저장소와 임베딩 모델을 구축하고, 사용자 질문의 언어를 감지하여 해당 언어의 저장소로 요청을 라우팅(routing)하는 방식.\n",
    "\n",
    "**전략:**\n",
    "1.  **언어별 벡터 저장소 구축**: 한국어 문서는 한국어에 강한 임베딩 모델로, 영어 문서는 영어에 강한 (또는 좋은 다국어) 임베딩 모델로 각각 별도의 벡터 저장소를 만듬.\n",
    "2.  **언어 감지**: 사용자 질문의 언어를 감지\n",
    "3.  **라우팅**: 감지된 언어에 해당하는 벡터 저장소 및 관련 RAG 체인으로 질문을 전달\n",
    "4.  **RAG 처리**: 선택된 RAG 체인에서 답변을 생성\n",
    "\n",
    "**장점:**\n",
    "- 각 언어에 최적화된 임베딩과 RAG 파이프라인을 사용할 수 있어, 해당 언어 내에서의 검색 및 답변 품질이 높을 수 있음.\n",
    "- 번역 과정이 없어 지연 시간과 번역 비용이 발생하지 않음/ (단, 교차 언어 질의는 직접 지원하지 않음).\n",
    "\n",
    "**단점:**\n",
    "- **여러 벡터 저장소 관리**: 지원하는 언어 수만큼 벡터 저장소와 RAG 체인을 관리해야 하므로 복잡성이 증가.\n",
    "- **교차 언어 검색 미지원**: 기본적으로는 질문 언어와 동일한 언어의 문서만 검색(예: 영어 질문으로는 영어 문서만 검색). 교차 언어 검색을 지원하려면 각 저장소에서 사용하는 임베딩 모델 자체가 교차 언어 성능이 뛰어나거나, 추가적인 번역 로직이 필요.\n",
    "- **언어 감지 정확도 중요**: 언어 감지가 잘못되면 엉뚱한 저장소로 라우팅되어 성능이 저하"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e7670",
   "metadata": {},
   "source": [
    "`(1) 언어별 벡터저장소 생성 `  \n",
    "\n",
    "- 한국어 문서(`korean_docs_split`)와 영어 문서(`english_docs_split`)를 각각 다른 임베딩 모델(또는 동일한 고성능 다국어 모델)을 사용하여 별도의 ChromaDB 컬렉션에 저장\n",
    "\n",
    "- 여기서는 예시로 한국어 문서는 HuggingFace의 `BAAI/bge-m3` (다국어 모델이지만 한국어 데이터에 대한 성능도 준수)를 사용하고, 영어 문서는 Ollama의 `bge-m3` (동일 모델을 Ollama 통해 사용)를 사용한다고 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec094ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문서용 벡터 저장소 생성 중 (HuggingFace bge-m3 사용)...\n",
      "한국어 벡터 저장소 문서 수: 6\n",
      "\n",
      "영어 문서용 벡터 저장소 생성 중 (Ollama bge-m3 사용)...\n",
      "영어 벡터 저장소 문서 수: 4\n"
     ]
    }
   ],
   "source": [
    "# 한국어 문서, 영어 문서를 각각 다른 벡터 저장소에 저장\n",
    "db_korean_route = None\n",
    "db_english_route = None\n",
    "\n",
    "# 한국어 문서용 벡터 저장소 (HuggingFace bge-m3 사용)\n",
    "if korean_docs_split:\n",
    "    print(\"한국어 문서용 벡터 저장소 생성 중 (HuggingFace bge-m3 사용)...\")\n",
    "    db_korean_route = Chroma.from_documents(\n",
    "        documents=korean_docs_split, \n",
    "        embedding=embeddings_huggingface_bge_m3_cl, # 5-1에서 정의한 HuggingFace bge-m3 임베딩 재사용\n",
    "        collection_name=\"db_korean_routed_v2\",\n",
    "        persist_directory=\"./chroma_db_routed\", # 라우팅용 별도 디렉토리\n",
    "    )\n",
    "    print(f\"한국어 벡터 저장소 문서 수: {db_korean_route._collection.count()}\")\n",
    "else:\n",
    "    print(\"분할된 한국어 문서가 없어 한국어 벡터 저장소 생성을 건너뜁니다.\")\n",
    "\n",
    "# 영어 문서용 벡터 저장소 (Ollama bge-m3 사용)\n",
    "if english_docs_split and ollama_ready_cl and embeddings_ollama_bge_m3_cl:\n",
    "    print(\"\\n영어 문서용 벡터 저장소 생성 중 (Ollama bge-m3 사용)...\")\n",
    "    db_english_route = Chroma.from_documents(\n",
    "        documents=english_docs_split, \n",
    "        embedding=embeddings_ollama_bge_m3_cl, # 5-1에서 정의한 Ollama bge-m3 임베딩 재사용\n",
    "        collection_name=\"db_english_routed_v2\",\n",
    "        persist_directory=\"./chroma_db_routed\",\n",
    "    )\n",
    "    print(f\"영어 벡터 저장소 문서 수: {db_english_route._collection.count()}\")\n",
    "elif not english_docs_split:\n",
    "    print(\"\\n분할된 영어 문서가 없어 영어 벡터 저장소 생성을 건너뜁니다.\")\n",
    "else:\n",
    "    print(\"\\nOllama가 준비되지 않아 영어 벡터 저장소(Ollama bge-m3) 생성을 건너뜁니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586c90a",
   "metadata": {},
   "source": [
    "`(2) RAG 체인 성능 평가 `  \n",
    "\n",
    "언어 감지 결과를 바탕으로 적절한 RAG 체인(한국어용 또는 영어용)을 선택하여 실행하는 함수를 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ba3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라우팅용 한국어 RAG 체인 생성 완료\n",
      "라우팅용 영어 RAG 체인 생성 완료\n",
      "\n",
      "--- 실행: 라우팅 한국어 질문 '테슬라 창업자는 누구인가요?' ---\n",
      "감지된 질문 언어: ko\n",
      "한국어 RAG 체인으로 라우팅합니다.\n",
      "결과 (KO 라우팅): 마틴 에버하드와 마크 타페닝입니다.\n",
      "\n",
      "--- 실행: 라우팅 영어 질문 'Who is the founder of Tesla?' ---\n",
      "감지된 질문 언어: en\n",
      "영어 RAG 체인으로 라우팅합니다.\n",
      "결과 (EN 라우팅): Tesla was founded by Martin Eberhard and Marc Tarpenning.\n",
      "\n",
      "--- 실행: 라우팅 일본어 질문 'テスラの創業者は誰ですか？' ---\n",
      "감지된 질문 언어: ja\n",
      "결과 (JA 라우팅): 지원하지 않는 언어(ja)이거나 해당 언어의 RAG 체인이 없습니다. 한국어 또는 영어로 질문해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 각 언어별 RAG 체인 생성 (create_rag_chain_cl 함수 재사용)\n",
    "rag_chain_korean_routed = create_rag_chain_cl(db_korean_route)\n",
    "rag_chain_english_routed = create_rag_chain_cl(db_english_route)\n",
    "\n",
    "if rag_chain_korean_routed: print(\"라우팅용 한국어 RAG 체인 생성 완료\")\n",
    "else: print(\"라우팅용 한국어 RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_english_routed: print(\"라우팅용 영어 RAG 체인 생성 완료\")\n",
    "else: print(\"라우팅용 영어 RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "def run_route_rag_chain(query: str):\n",
    "    try:\n",
    "        detected_query_lang = detect(query) # 질문 언어 감지\n",
    "        print(f\"감지된 질문 언어: {detected_query_lang}\")\n",
    "    except Exception as e:\n",
    "        print(f\"질문 언어 감지 실패: {e}. 기본 체인(한국어) 시도 또는 에러 반환.\")\n",
    "        # detected_query_lang = 'unknown' # 또는 기본 언어 설정\n",
    "        return f\"질문 언어 감지 실패: {e}\"\n",
    "    \n",
    "    if detected_query_lang == 'ko' and rag_chain_korean_routed:\n",
    "        print(\"한국어 RAG 체인으로 라우팅합니다.\")\n",
    "        return rag_chain_korean_routed.invoke(query)\n",
    "    elif detected_query_lang == 'en' and rag_chain_english_routed:\n",
    "        print(\"영어 RAG 체인으로 라우팅합니다.\")\n",
    "        return rag_chain_english_routed.invoke(query)\n",
    "    else:\n",
    "        # 지원하지 않는 언어 또는 해당 언어 체인이 없는 경우\n",
    "        if detected_query_lang == 'ko' and not rag_chain_korean_routed:\n",
    "            return \"한국어 RAG 체인이 준비되지 않았습니다.\"\n",
    "        if detected_query_lang == 'en' and not rag_chain_english_routed:\n",
    "            return \"영어 RAG 체인이 준비되지 않았습니다.\"\n",
    "        return f\"지원하지 않는 언어({detected_query_lang})이거나 해당 언어의 RAG 체인이 없습니다. 한국어 또는 영어로 질문해주세요.\"\n",
    "    \n",
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko_route = \"테슬라 창업자는 누구인가요?\"\n",
    "print(f\"\\n--- 실행: 라우팅 한국어 질문 '{query_ko_route}' ---\")\n",
    "output_ko_route = run_route_rag_chain(query_ko_route)\n",
    "print(f\"결과 (KO 라우팅): {output_ko_route}\")\n",
    "\n",
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en_route = \"Who is the founder of Tesla?\"\n",
    "print(f\"\\n--- 실행: 라우팅 영어 질문 '{query_en_route}' ---\")\n",
    "output_en_route = run_route_rag_chain(query_en_route)\n",
    "print(f\"결과 (EN 라우팅): {output_en_route}\")\n",
    "\n",
    "# 지원하지 않는 언어 테스트 (예: 일본어)\n",
    "query_ja_route = \"テスラの創業者は誰ですか？\"\n",
    "print(f\"\\n--- 실행: 라우팅 일본어 질문 '{query_ja_route}' ---\")\n",
    "output_ja_route = run_route_rag_chain(query_ja_route)\n",
    "print(f\"결과 (JA 라우팅): {output_ja_route}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
