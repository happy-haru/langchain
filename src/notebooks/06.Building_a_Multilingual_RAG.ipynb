{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5119e53",
   "metadata": {},
   "source": [
    "## 1. 다국어 RAG 시스템 구축 실습\n",
    "\n",
    "- 실제 RAG 시스템은 다양한 언어의 문서를 다루거나, 여러 언어로 질문을 받는 상황에 놓일 수 있음.\n",
    "- 다국어 환경에서 RAG 시스템을 구축하는 방법에 대한 예시 확인\n",
    "\n",
    "**다국어 RAG의 주요 과제:**\n",
    "- **언어 불일치**: 질문과 문서의 언어가 다를 경우 검색 성능이 저하될 수 있음.\n",
    "- **임베딩 모델 선택**: 사용하는 임베딩 모델이 대상 언어들을 얼마나 잘 지원하는지가 중요하며, 교차 언어(cross-lingual) 성능이 좋은 모델이 필요할 수 있음.\n",
    "- **번역 품질 및 비용**: 자동 번역기를 사용할 경우 번역 품질, 지연 시간, 비용 등을 고려해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b183570",
   "metadata": {},
   "source": [
    "### 1-1 언어 교차(cross-lingual) 검색\n",
    "\n",
    "- 교차 언어 임베딩 모델은 서로 다른 언어의 텍스트라도 의미가 유사하면 벡터 공간에서 가깝게 위치하도록 학습됨. \n",
    "- 이러한 모델을 사용하면 한국어로 질문해도 영어 문서를 검색하거나, 그 반대의 경우도 가능하게 됨.\n",
    "\n",
    "**전략:**\n",
    "1.  한국어 문서와 영어 문서를 모두 준비.\n",
    "2.  교차 언어 성능이 우수한 임베딩 모델 (예: OpenAI `text-embedding-3-small`, HuggingFace `BAAI/bge-m3`, Ollama `bge-m3`)을 선택.\n",
    "3.  모든 문서를 선택한 임베딩 모델 하나로 임베딩해서 하나의 벡터 저장소에 벡터 저장소에 저장.\n",
    "4.  사용자 질문(한국어 또는 영어)을 동일한 임베딩 모델로 임베딩하여 벡터 저장소에서 유사 문서를 검색.\n",
    "\n",
    "**장점:**\n",
    "- **구현 심플**: 임베딩 모델 하나, 벡터 저장소 하나! 관리 포인트가 적어서 상대적으로 만들기 쉬움.\n",
    "- **유지보수 용이**: 시스템 구조가 단순해서 유지보수도 편함.\n",
    "\n",
    "\n",
    "**단점:**\n",
    "- **임베딩 모델 의존도 UP**: 모델의 교차 언어 성능이 곧 전체 시스템의 성능임. 모델이 별로면 검색 정확도도 뚝... 📉\n",
    "- **단일 언어 검색보다 약할 수도**: 특정 언어에만 특화된 모델보다는 여러 언어를 다루다 보니, 한국어 질문 -> 한국어 문서 검색 같은 단일 언어 검색 성능이 살짝 떨어질 수 있음.\n",
    "- **모든 언어 쌍에 완벽하진 않음**: 모델이 특정 언어 쌍(예: 영어-스페인어)에는 강해도, 다른 언어 쌍(예: 한국어-스와힐리어)에는 약할 수 있음. 데이터셋의 한계 때문임.\n",
    "\n",
    "**꿀팁 & 노하우:** 💡\n",
    "- **모델 선택 신중하게**: `mTEB (Multilingual Text Embedding Benchmark)` 같은 벤치마크 사이트에서 교차 언어 검색(Cross-Lingual Retrieval) 점수가 높은 모델을 찾아보는 게 좋음. (`BAAI/bge-m3`가 이런 벤치마크에서 좋은 성능을 보여주는 대표적인 모델임!)\n",
    "- **데이터 중요성**: 학습 데이터에 포함된 언어 및 데이터 양에 따라 성능이 달라지니, 내가 사용할 주요 언어들이 모델 학습 시 잘 다뤄졌는지 확인해보는 것도 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def68998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4059b6",
   "metadata": {},
   "source": [
    "`(1) 다국어 문서 로드 및 전처리` 예시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4b4ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 텍스트 파일: ['../data\\\\Rivian_KR.txt', '../data\\\\Tesla_KR.txt']\n",
      "영어 텍스트 파일: ['../data\\\\Rivian_EN.txt', '../data\\\\Tesla_EN.txt']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os \n",
    "korean_txt_files = glob(os.path.join('../data', '*_KR.txt')) \n",
    "english_txt_files = glob(os.path.join('../data', '*_EN.txt'))\n",
    "\n",
    "print(\"한국어 텍스트 파일:\", korean_txt_files)\n",
    "print(\"영어 텍스트 파일:\", english_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69a4d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 원본 Document 수: 2\n",
      "영어 원본 Document 수: 2\n",
      "\n",
      "한국어 데이터 샘플 (첫 번째 문서 메타데이터): {'source': '../data\\\\Rivian_KR.txt'}\n",
      "한국어 데이터 샘플 (첫 번째 문서 내용 일부): 리비안 오토모티브(Rivian Automotive, Inc.)는 미국의 전기 자동차 제조업체이자 자동차 기술 회사임.\n",
      "2009년에 로버트 \"RJ\" 스캐린지(Robert \"RJ\" Scaringe)에 의해 설립되었음. 본사는 캘리포니아주 어바인에 위치해 있음.\n",
      "리비안의 주력 제품은 R1T 전기 픽업트럭과 R1S 전기 SUV임. 이들 차량은 \"스케이트보드\" 플랫\n",
      "\n",
      "영어 데이터 샘플 (첫 번째 문서 메타데이터): {'source': '../data\\\\Rivian_EN.txt'}\n",
      "영어 데이터 샘플 (첫 번째 문서 내용 일부): Rivian Automotive, Inc. is an American electric vehicle manufacturer and automotive technology company.\n",
      "Founded in 2009 by Robert \"RJ\" Scaringe, it is headquartered in Irvine, California.\n",
      "Rivian's mai\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def load_text_files(txt_files):\n",
    "    data = []\n",
    "    for text_file in txt_files:\n",
    "        # TextLoader는 기본적으로 utf-8을 가정하나, 명시하는 것이 좋음\n",
    "        loader = TextLoader(text_file, encoding='utf-8') \n",
    "        data.extend(loader.load())\n",
    "    return data\n",
    "\n",
    "korean_data = load_text_files(korean_txt_files)\n",
    "english_data = load_text_files(english_txt_files)\n",
    "\n",
    "print(f\"한국어 원본 Document 수: {len(korean_data)}\")\n",
    "print(f\"영어 원본 Document 수: {len(english_data)}\")\n",
    "\n",
    "if korean_data:\n",
    "    print(\"\\n한국어 데이터 샘플 (첫 번째 문서 메타데이터):\", korean_data[0].metadata)\n",
    "    print(\"한국어 데이터 샘플 (첫 번째 문서 내용 일부):\", korean_data[0].page_content[:200])\n",
    "if english_data:\n",
    "    print(\"\\n영어 데이터 샘플 (첫 번째 문서 메타데이터):\", english_data[0].metadata)\n",
    "    print(\"영어 데이터 샘플 (첫 번째 문서 내용 일부):\", english_data[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9521a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문서를 잘게 쪼갠 청크 수: 5\n",
      "영어 문서를 잘게 쪼갠 청크 수: 2\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문서를 의미 있는 단위(청크)로 쪼개는 작업임.\n",
    "# CharacterTextSplitter.from_tiktoken_encoder를 쓰면 특정 모델의 토크나이저를 기준으로 글자 수를 계산해서 자름.\n",
    "# `text-embedding-3-small` 모델은 다국어 처리에 강점이 있어서 이 모델의 토크나이저를 쓰는 건 좋은 선택!\n",
    "# 왜냐하면, (1) 대규모 다국어 데이터셋으로 학습했고, (2) 다양한 언어의 미묘한 차이까지 고려한 정교한 토크나이저를 사용하기 때문임.\n",
    "text_splitter_multilingual = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"text-embedding-3-small\", # 이 모델의 토크나이저를 사용해서 청크 크기를 계산함\n",
    "    separator=r\"[.!?]\\s+\",      # 문장 끝맺음(마침표, 느낌표, 물음표) 뒤에 공백이 오는 걸 기준으로 문장을 나눔 (정규식)\n",
    "    chunk_size=200,              # 목표 청크 크기 (토큰 수). CharacterTextSplitter는 이 값을 정확히 지키진 않을 수 있음.\n",
    "    chunk_overlap=20,            # 청크끼리 얼마나 겹치게 할 건지 (토큰 수). 문맥 유지를 위해 중요!\n",
    "    is_separator_regex=True,     # separator가 정규 표현식이라고 알려줌.\n",
    "    keep_separator=True,        # 구분자(여기선 문장 끝맺음 뒤 공백)는 청크에 포함 안 시킴.\n",
    ")\n",
    "\n",
    "korean_docs_split = []\n",
    "english_docs_split = []\n",
    "\n",
    "if korean_data:\n",
    "    korean_docs_split = text_splitter_multilingual.split_documents(korean_data)\n",
    "if english_data:\n",
    "    english_docs_split = text_splitter_multilingual.split_documents(english_data)\n",
    "\n",
    "print(f\"한국어 문서를 잘게 쪼갠 청크 수: {len(korean_docs_split)}\")\n",
    "print(f\"영어 문서를 잘게 쪼갠 청크 수: {len(english_docs_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52633cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 한국어 분할 청크 샘플 ---\n",
      "\n",
      "[청크 1] (길이: 180자, 메타데이터: {'source': '../data\\\\Rivian_KR.txt'})\n",
      "리비안 오토모티브(Rivian Automotive, Inc.)는 미국의 전기 자동차 제조업체이자 자동차 기술 회사임.\n",
      "2009년에 로버트 \"RJ\" 스캐린지(Robert \"RJ\" Scaringe)에 의해 설립되었음. 본사는 캘리포니아주 어바인에 위치해 있음.\n",
      "리비안의 주\n",
      "[...]\n",
      "\n",
      "[청크 2] (길이: 170자, 메타데이터: {'source': '../data\\\\Rivian_KR.txt'})\n",
      ". 이들 차량은 \"스케이트보드\" 플랫폼을 기반으로 하며, 오프로드 성능과 장거리 주행 능력을 강조함.\n",
      "리비안은 아마존(Amazon)과 포드(Ford) 등 주요 기업으로부터 투자를 유치했으며, 아마존에는 전기 배송 밴을 공급하는 계약을 체결하기도 했음.\n",
      "2021년 말에 \n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "# 한국어 분할 청크 확인 (처음 2개)\n",
    "if korean_docs_split:\n",
    "    print(\"--- 한국어 분할 청크 샘플 ---\")\n",
    "    for i, doc in enumerate(korean_docs_split[:2]):\n",
    "        print(f\"\\n[청크 {i+1}] (길이: {len(doc.page_content)}자, 메타데이터: {doc.metadata})\")\n",
    "        print(doc.page_content[:150])\n",
    "        if len(doc.page_content) > 150: print(\"[...]\")\n",
    "else:\n",
    "    print(\"분할된 한국어 문서가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34634419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 영어 분할 청크 샘플 ---\n",
      "\n",
      "[청크 1] (길이: 625자, 메타데이터: {'source': '../data\\\\Rivian_EN.txt'})\n",
      "Rivian Automotive, Inc. is an American electric vehicle manufacturer and automotive technology company.\n",
      "Founded in 2009 by Robert \"RJ\" Scaringe, it is\n",
      "[...]\n",
      "\n",
      "[청크 2] (길이: 741자, 메타데이터: {'source': '../data\\\\Tesla_EN.txt'})\n",
      "Tesla, Inc. is an American electric vehicle and clean energy company.\n",
      "It was co-founded in 2003 by Martin Eberhard and Marc Tarpenning. Elon Musk was \n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "# 영어 분할 청크 확인 (처음 2개)\n",
    "if english_docs_split:\n",
    "    print(\"\\n--- 영어 분할 청크 샘플 ---\")\n",
    "    for i, doc in enumerate(english_docs_split[:2]):\n",
    "        print(f\"\\n[청크 {i+1}] (길이: {len(doc.page_content)}자, 메타데이터: {doc.metadata})\")\n",
    "        print(doc.page_content[:150])\n",
    "        if len(doc.page_content) > 150: print(\"[...]\")\n",
    "else:\n",
    "    print(\"분할된 영어 문서가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699cb7f",
   "metadata": {},
   "source": [
    "**(2) 문서 임베딩 및 벡터 저장소에 저장**\n",
    "\n",
    "1) 선택한 교차 언어 임베딩 모델들(OpenAI, HuggingFace, Ollama)을 사용하여 한국어와 영어 문서를 함께 임베딩하고, 각 모델별로 ChromaDB 벡터 저장소에 저장\n",
    "2) 이렇게 하면 각 임베딩 모델의 교차 언어 검색 성능을 비교 가능\n",
    "\n",
    "- `collection_name`: 벡터 저장소 내에서 특정 문서 그룹을 식별하는 이름이며, 모델별로 다른 이름을 사용 가능.\n",
    "- `persist_directory`: 벡터 저장소 데이터를 디스크에 저장할 경로이며, 지정하면 저장소가 유지되어 재사용 가능.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8383d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama 임베딩 모델 (bge-m3) 준비 완료.\n",
      "총 분할된 문서(청크) 수: 7\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# 1 `text-embedding-3-small`은 교차 언어 지원이 꽤 괜찮은 모델임.\n",
    "embeddings_openai_small_cl = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 2. Hugging Face 임베딩 모델: `BAAI/bge-m3`는 mTEB 벤치마크에서도 상위권을 차지하는 강력한 다국어 및 교차 언어 모델임.\n",
    "# `normalize_embeddings: True`는 임베딩 벡터를 정규화해서 유사도 계산 시 코사인 유사도 성능을 높여줌.\n",
    "embeddings_huggingface_bge_m3_cl = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\", \n",
    "    model_kwargs={'device': 'cpu'},# GPU 없으면 'cpu', 있으면 'cuda'로 설정. HuggingFace 모델은 GPU 쓰면 훨씬 빠름!\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Ollama 임베딩 모델 (bge-m3 또는 nomic-embed-text, 교차 언어 지원 가능)\n",
    "# Ollama 서버 및 해당 모델이 준비되어 있어야 함\n",
    "embeddings_ollama_bge_m3_cl = None\n",
    "ollama_ready_cl = False\n",
    "try:\n",
    "    embeddings_ollama_bge_m3_cl = OllamaEmbeddings(model=\"bge-m3\") \n",
    "    # embeddings_ollama_nomic_cl = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    print(\"Ollama 임베딩 모델 (bge-m3) 준비 완료.\")\n",
    "    ollama_ready_cl = True\n",
    "except Exception as e:\n",
    "    print(f\"Ollama 연결 또는 모델 로드 실패 (교차언어용): {e}\")\n",
    "    print(\"Ollama (bge-m3) 교차언어 예제를 실행하려면 Ollama 서버를 실행하고 'bge-m3' 모델을 pull 해주세요.\")\n",
    "\n",
    "all_split_docs = korean_docs_split + english_docs_split\n",
    "print(f\"총 분할된 문서(청크) 수: {len(all_split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fea85ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 임베딩으로 벡터 저장소 구축 중...\n",
      "OpenAI 벡터 저장소 문서 수: 14\n",
      "\n",
      "Hugging Face (BAAI/bge-m3) 임베딩으로 벡터 저장소 구축 중...\n",
      "Hugging Face 벡터 저장소 문서 수: 14\n",
      "\n",
      "Ollama (bge-m3) 임베딩으로 벡터 저장소 구축 중...\n",
      "Ollama 벡터 저장소 문서 수: 14\n"
     ]
    }
   ],
   "source": [
    "# 다국어 벡터 저장소 구축\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "db_openai_cl = None\n",
    "db_huggingface_cl = None\n",
    "db_ollama_cl = None\n",
    "\n",
    "if all_split_docs: # 분할된 문서가 있을 경우에만 실행\n",
    "    print(\"OpenAI 임베딩으로 벡터 저장소 구축 중...\")\n",
    "    db_openai_cl = Chroma.from_documents(\n",
    "        documents=all_split_docs, \n",
    "        embedding=embeddings_openai_small_cl,\n",
    "        collection_name=\"db_openai_crosslingual_v2\", # 컬렉션 이름 변경 또는 기존 삭제 후 생성\n",
    "        persist_directory=\"./chroma_db_cl\", # 디렉토리 구분\n",
    "    )\n",
    "    print(f\"OpenAI 벡터 저장소 문서 수: {db_openai_cl._collection.count()}\")\n",
    "\n",
    "    print(\"\\nHugging Face (BAAI/bge-m3) 임베딩으로 벡터 저장소 구축 중...\")\n",
    "    db_huggingface_cl = Chroma.from_documents(\n",
    "        documents=all_split_docs, \n",
    "        embedding=embeddings_huggingface_bge_m3_cl,\n",
    "        collection_name=\"db_huggingface_crosslingual_v2\",\n",
    "        persist_directory=\"./chroma_db_cl\",\n",
    "    )\n",
    "    print(f\"Hugging Face 벡터 저장소 문서 수: {db_huggingface_cl._collection.count()}\")\n",
    "\n",
    "    if ollama_ready_cl and embeddings_ollama_bge_m3_cl:\n",
    "        print(\"\\nOllama (bge-m3) 임베딩으로 벡터 저장소 구축 중...\")\n",
    "        db_ollama_cl = Chroma.from_documents(\n",
    "            documents=all_split_docs, \n",
    "            embedding=embeddings_ollama_bge_m3_cl,\n",
    "            collection_name=\"db_ollama_crosslingual_v2\",\n",
    "            persist_directory=\"./chroma_db_cl\",\n",
    "        )\n",
    "        print(f\"Ollama 벡터 저장소 문서 수: {db_ollama_cl._collection.count()}\")\n",
    "    else:\n",
    "        print(\"\\nOllama가 준비되지 않아 Ollama 벡터 저장소 구축을 건너뜁니다.\")\n",
    "else:\n",
    "    print(\"분할된 문서가 없어 벡터 저장소 구축을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592085d",
   "metadata": {},
   "source": [
    "`(3) RAG 성능 비교 `  \n",
    "\n",
    "- 간단한 RAG 체인을 구성하여 각 임베딩 모델 기반의 벡터 저장소가 한국어 질문과 영어 질문에 대해 어떻게 응답하는지 비교\n",
    "\n",
    "**RAG 체인 구성 요소:**\n",
    "- **Retriever**: 벡터 저장소에서 유사 문서를 검색하며, `as_retriever()`로 변환하고, `search_kwargs={'k': 3}`로 상위 2개 문서를 가져오도록 설정.\n",
    "- **Prompt Template**: LLM에 전달할 프롬프트를 정의하며, 컨텍스트(검색된 문서)와 질문을 포함.\n",
    "- **LLM**: 질문과 컨텍스트를 바탕으로 최종 답변을 생성할 언어 모델 (예: `ChatOpenAI`)\n",
    "- **Output Parser**: LLM의 출력(주로 `AIMessage` 객체)에서 실제 텍스트 답변만 추출(`StrOutputParser`)\n",
    "- **RunnablePassthrough / format_docs**: LangChain Expression Language (LCEL)에서 데이터 흐름을 관리하고, 검색된 `Document` 객체 리스트를 LLM 프롬프트에 적합한 문자열 형태로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb64aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인 생성\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based ONLY on the following context.\n",
    "Do not use any external information or knowledge. \n",
    "If the answer is not in the context, answer \"잘 모르겠습니다.\".\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_cl = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d30396f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 포맷터 함수: Document 객체 리스트를 단일 문자열로 결합\n",
    "def format_docs_cl(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# LLM 모델 생성 (답변 생성용)\n",
    "llm_cl = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 체인 생성 함수 (벡터 저장소를 인자로 받음)\n",
    "def create_rag_chain_cl(vectorstore):\n",
    "    if not vectorstore:\n",
    "        # 벡터 저장소가 None이면, 실행 불가능한 더미 체인을 반환하거나 예외 처리\n",
    "        # 여기서는 간단히 None을 반환하여 호출하는 쪽에서 확인하도록 함\n",
    "        return None\n",
    "        \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 3}) # 상위 3개 문서 검색\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs_cl , \"question\": RunnablePassthrough()} # 검색 및 포맷팅\n",
    "        | prompt_template_cl  # 프롬프트 적용\n",
    "        | llm_cl              # LLM으로 답변 생성\n",
    "        | StrOutputParser()   # 출력 파싱 (텍스트만 추출)\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346e5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI RAG 체인 생성 완료\n",
      "HuggingFace RAG 체인 생성 완료\n",
      "Ollama RAG 체인 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# 각 벡터 저장소에 대한 RAG 체인 생성\n",
    "rag_chain_openai_cl = create_rag_chain_cl(db_openai_cl)\n",
    "rag_chain_huggingface_cl = create_rag_chain_cl(db_huggingface_cl)\n",
    "rag_chain_ollama_cl = create_rag_chain_cl(db_ollama_cl)\n",
    "\n",
    "if rag_chain_openai_cl: print(\"OpenAI RAG 체인 생성 완료\")\n",
    "else: print(\"OpenAI RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_huggingface_cl: print(\"HuggingFace RAG 체인 생성 완료\")\n",
    "else: print(\"HuggingFace RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_ollama_cl: print(\"Ollama RAG 체인 생성 완료\")\n",
    "else: print(\"Ollama RAG 체인 생성 실패 (벡터 저장소 없음 또는 Ollama 준비 안됨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02e10620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 한국어 쿼리: '테슬라 창업자는 누구인가요?' ---\n",
      "OpenAI 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "Hugging Face (bge-m3) 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "Ollama (bge-m3) 응답 (KO): 마틴 에버하드와 마크 타페닝입니다.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko_cl = \"테슬라 창업자는 누구인가요?\" # 예시 문서에 관련 내용이 있어야 함\n",
    "print(f\"\\n--- 한국어 쿼리: '{query_ko_cl}' ---\")\n",
    "\n",
    "if rag_chain_openai_cl:\n",
    "    output_openai_ko = rag_chain_openai_cl.invoke(query_ko_cl)\n",
    "    print(f\"OpenAI 응답 (KO): {output_openai_ko}\")\n",
    "else:\n",
    "    print(\"OpenAI RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_huggingface_cl:\n",
    "    output_huggingface_ko = rag_chain_huggingface_cl.invoke(query_ko_cl)\n",
    "    print(f\"Hugging Face (bge-m3) 응답 (KO): {output_huggingface_ko}\")\n",
    "else:\n",
    "    print(\"HuggingFace RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_ollama_cl:\n",
    "    output_ollama_ko = rag_chain_ollama_cl.invoke(query_ko_cl)\n",
    "    print(f\"Ollama (bge-m3) 응답 (KO): {output_ollama_ko}\")\n",
    "else:\n",
    "    print(\"Ollama RAG 체인이 없어 실행 불가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f373ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 영어 쿼리: 'Who is the founder of Tesla?' ---\n",
      "OpenAI 응답 (EN): 테슬라는 마틴 에버하드와 마크 타페닝에 의해 공동 창립되었습니다.\n",
      "Hugging Face (bge-m3) 응답 (EN): 테슬라는 2003년에 마틴 에버하드와 마크 타페닝에 의해 공동 창립되었습니다.\n",
      "Ollama (bge-m3) 응답 (EN): 테슬라는 마틴 에버하드와 마크 타페닝에 의해 공동 창립되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en_cl = \"Who is the founder of Tesla?\" # 예시 문서에 관련 내용이 있어야 함\n",
    "print(f\"\\n--- 영어 쿼리: '{query_en_cl}' ---\")\n",
    "\n",
    "if rag_chain_openai_cl:\n",
    "    output_openai_en = rag_chain_openai_cl.invoke(query_en_cl)\n",
    "    print(f\"OpenAI 응답 (EN): {output_openai_en}\")\n",
    "else:\n",
    "    print(\"OpenAI RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_huggingface_cl:\n",
    "    output_huggingface_en = rag_chain_huggingface_cl.invoke(query_en_cl)\n",
    "    print(f\"Hugging Face (bge-m3) 응답 (EN): {output_huggingface_en}\")\n",
    "else:\n",
    "    print(\"HuggingFace RAG 체인이 없어 실행 불가\")\n",
    "\n",
    "if rag_chain_ollama_cl:\n",
    "    output_ollama_en = rag_chain_ollama_cl.invoke(query_en_cl)\n",
    "    print(f\"Ollama (bge-m3) 응답 (EN): {output_ollama_en}\")\n",
    "else:\n",
    "    print(\"Ollama RAG 체인이 없어 실행 불가\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4a32b",
   "metadata": {},
   "source": [
    "### 1-2 언어 감지 및 자동번역 통합 \n",
    "\n",
    "- 이 전략은 주로 단일 언어(예: 한국어)로 된 문서 저장소를 가지고 있을 때, 외국어로 들어오는 질문을 처리하기 위해 사용됨.\n",
    "반대로, 영어 문서 저장소에 한국어 질문을 받고 싶을 때도 쓸 수 있음!\n",
    "핵심은 **질문과 답변을 필요에 따라 번역**하는 거임\n",
    "\n",
    "**전략:**\n",
    "1.  **언어 감지**: 사용자 질문의 언어를 감지 (예: `langdetect` 라이브러리 사용).\n",
    "2.  **질문 번역**: 감지된 질문 언어가 문서 저장소의 주 언어와 다르면, 질문을 문서 저장소의 언어로 번역(예: `deepl` API 사용).\n",
    "3.  **RAG 처리**: 번역된 질문을 사용하여 일반적인 RAG 체인을 통해 답변을 생성 (이때 답변은 문서 저장소의 언어로 생성됨).\n",
    "4.  **답변 번역**: 생성된 답변의 언어가 원래 질문의 언어와 다르면, 답변을 원래 질문의 언어로 다시 번역\n",
    "\n",
    "**장점:**\n",
    "- 단일 언어에 최적화된 임베딩 모델과 LLM을 활용하여 해당 언어에서의 검색 및 답변 생성 품질을 극대화할 수 있음.\n",
    "- 다양한 언어의 사용자 질문을 지원할 수 있음.\n",
    "\n",
    "**단점:**\n",
    "- **번역 품질 의존성**: 번역기의 성능에 따라 전체 시스템의 품질이 크게 좌우됨. 번역 오류는 잘못된 검색 결과나 부정확한 답변으로 이어질 수 있음.\n",
    "- **지연 시간 증가**: 질문과 답변 번역 과정에서 추가적인 API 호출로 인해 전체 응답 시간이 늘어남.\n",
    "- **번역 비용**: 상용 번역 API(예: DeepL, Google Translate) 사용 시 비용이 발생.\n",
    "- **언어 감지 오류**: 언어 감지가 정확하지 않으면 불필요하거나 잘못된 번역이 발생 가능성.\n",
    "\n",
    "\n",
    "\n",
    "**꿀팁 & 노하우:** 💡\n",
    "- **번역기 선택**: DeepL은 번역 품질이 좋다고 알려져 있지만 유료임. Google Translate API, Papago API 등 다른 옵션도 있고, 심지어 자체 번역 모델(오픈소스 모델 fine-tuning 등)을 구축할 수도 있음 (전문가 영역!).\n",
    "- **언어 감지 라이브러리**: `langdetect`는 사용하기 쉽지만 가끔 짧은 텍스트에서 틀릴 수 있음. `fastText`의 언어 식별 모델은 더 정확하지만, 모델 파일이 크고 설정이 좀 더 필요함.\n",
    "- **번역 캐싱**: 똑같은 문장을 여러 번 번역하지 않도록, 이미 번역한 결과는 저장해뒀다가 재사용(캐싱)하면 비용과 시간을 아낄 수 있음.\n",
    "- **타겟 언어 명확화**: \"문서 저장소의 주 언어\"를 명확히 정하고, 모든 질문을 그 언어로 통일시키는 것이 관리하기 편함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928e845",
   "metadata": {},
   "source": [
    "`(1) 한국어 문서 벡터저장소 로드 (또는 생성)`  \n",
    "\n",
    "- 한국어 문서만으로 구성된 벡터 저장소가 이미 있다고 가정.\n",
    "- 만약 없다면, 한국어 문서(`korean_docs_split`)와 한국어에 강한 임베딩 모델(예: `OpenAIEmbeddings`, 또는 한국어 특화 HuggingFace 모델)을 사용하여 새로 생성할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f3dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문서로 저장되어 있는 벡터 저장소 로드 또는 생성\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_for_ko_store = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", \n",
    ")\n",
    "\n",
    "# 기존에 'chroma_test' 컬렉션이 한국어 데이터로 만들어져 있다고 가정.\n",
    "# 없다면, korean_docs_split을 사용해 새로 생성해야 함.\n",
    "# 예시\n",
    "db_korean_only = Chroma.from_documents(documents=korean_docs_split, embedding=embeddings_for_ko_store, \n",
    "                                       collection_name=\"korean_store_v1\", persist_directory=\"./chroma_db_ko_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f54d6012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'db_openai_crosslingual_v2' 벡터 저장소 로드 완료. 문서 수: 14\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME_KO_ONLY = \"db_openai_crosslingual_v2\" # (한국어+영어 데이터 포함)\n",
    "PERSIST_DIR_KO_ONLY = \"./chroma_db_cl\"\n",
    "\n",
    "try:\n",
    "    # 여기서는 1-1에서 만든 db_openai_cl (한국어+영어 문서 포함)을 재사용\n",
    "    # 순수 한국어 저장소를 원한다면, 해당 저장소를 로드하거나 새로 만들어야 함.\n",
    "    vectorstore_ko_trans = Chroma(\n",
    "        embedding_function=embeddings_for_ko_store,\n",
    "        collection_name=COLLECTION_NAME_KO_ONLY, # 1-1에서 사용한 OpenAI 컬렉션 이름\n",
    "        persist_directory=PERSIST_DIR_KO_ONLY    # 1-1에서 사용한 디렉토리\n",
    "    )\n",
    "    print(f\"'{COLLECTION_NAME_KO_ONLY}' 벡터 저장소 로드 완료. 문서 수: {vectorstore_ko_trans._collection.count()}\")\n",
    "    # 이 저장소는 실제로는 한국어와 영어가 섞여있지만, 지금은 한국어 중심 저장소로 간주하고 진행\n",
    "    # 이상적으로는 순수 한국어 문서로 구성된 저장소를 사용하는 것이 이 시나리오에 더 적합\n",
    "except Exception as e:\n",
    "    print(f\"벡터 저장소 '{COLLECTION_NAME_KO_ONLY}' 로드 실패: {e}\")\n",
    "    print(\"이전 단계에서 해당 이름의 컬렉션이 생성되었는지, 경로가 올바른지 확인하세요.\")\n",
    "    vectorstore_ko_trans = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f1fc2",
   "metadata": {},
   "source": [
    "**언어 감지 및 번역 도구 설정**\n",
    "\n",
    "- `langdetect`: 텍스트의 언어를 감지\n",
    "- `deepl`: 고품질 번역을 제공하는 API 서비스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39aabf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepL 번역기 초기화 완료.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import deepl\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# 언어 감지 결과의 일관성을 위해 시드 설정 (선택 사항)\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "deepl_api_key = os.getenv('DEEPL_API_KEY')\n",
    "translator = None\n",
    "if deepl_api_key:\n",
    "    translator = deepl.Translator(deepl_api_key)\n",
    "    print(\"DeepL 번역기 초기화 완료.\")\n",
    "else:\n",
    "    print(\"DEEPL_API_KEY가 설정되지 않았습니다. 번역 기능을 사용하려면 API 키를 설정하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf115373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_translate(text: str, target_lang_deepl: str = 'KO', target_lang_detect: str = 'ko'):\n",
    "    if not translator:\n",
    "        print(\"번역기가 설정되지 않아 원본 텍스트를 반환합니다.\")\n",
    "        # 번역기 없어도 언어 감지는 시도\n",
    "        try:\n",
    "            detected_lang = detect(text)\n",
    "            return text, detected_lang\n",
    "        except Exception as e:\n",
    "            print(f\"언어 감지 실패: {e}. 원본 텍스트 및 'unknown' 반환.\")\n",
    "            return text, \"unknown\"\n",
    "\n",
    "    try:\n",
    "        detected_lang = detect(text) # langdetect는 'ko', 'en' 등 소문자 코드를 반환\n",
    "    except Exception as e:\n",
    "        print(f\"언어 감지 실패: {e}. 원본 텍스트 사용.\")\n",
    "        return text, \"unknown\"\n",
    "\n",
    "    # target_lang_deepl ('EN', 'KO', 'EN-US' 등)을 langdetect 스타일 ('en', 'ko')로 변환하여 비교하기 위함\n",
    "    simple_target_deepl_code = ''\n",
    "    temp_target_deepl_upper = target_lang_deepl.upper()\n",
    "    if temp_target_deepl_upper.startswith('EN'): # 'EN', 'EN-US', 'EN-GB' 등\n",
    "        simple_target_deepl_code = 'en'\n",
    "    elif temp_target_deepl_upper == 'KO':\n",
    "        simple_target_deepl_code = 'ko'\n",
    "    # 다른 언어에 대한 규칙 추가 가능\n",
    "    else:\n",
    "        # 일반적인 경우 (예: 'FR', 'JA') DeepL 코드는 2자리이므로 앞 두 글자를 소문자로 사용\n",
    "        if len(target_lang_deepl) >= 2:\n",
    "            simple_target_deepl_code = target_lang_deepl[:2].lower()\n",
    "        else: # 매우 드문 경우\n",
    "            simple_target_deepl_code = target_lang_deepl.lower()\n",
    "\n",
    "    # 감지된 언어(소문자)와 실제 목표 언어(단순화된 소문자 코드)를 비교\n",
    "    if detected_lang.lower() != simple_target_deepl_code:\n",
    "        print(f\"번역 필요: 감지된 언어 '{detected_lang}' -> 목표 언어 '{target_lang_deepl}'\")\n",
    "        try:\n",
    "            # DeepL은 'EN-US', 'EN-GB' 등을 지원. 단순히 'EN'으로 하면 기본 'EN-US'로 될 수 있음.\n",
    "            # langdetect가 'en'을 반환하면, DeepL의 'EN-US'로 매핑하는 것이 안전할 수 있음.\n",
    "            target_lang_deepl_actual = target_lang_deepl # 기본값 설정\n",
    "            if target_lang_deepl.upper() == 'EN': # 만약 영어로 번역해야 한다면\n",
    "                target_lang_deepl_actual = 'EN-US' # 미국 영어로 지정 (DeepL 기본 동작과 일치시키거나 명시적 지정)\n",
    "            # 다른 target_lang_deepl 값들은 그대로 사용 (예: 'KO', 'FR', 'DE')\n",
    "            # (위의 if문에서 target_lang_deepl_actual = target_lang_deepl 로 이미 설정됨)\n",
    "\n",
    "            result = translator.translate_text(text, target_lang=target_lang_deepl_actual)\n",
    "            return str(result), detected_lang\n",
    "        except Exception as e:\n",
    "            print(f\"번역 실패: {e}. 원본 텍스트 사용.\")\n",
    "            return text, detected_lang\n",
    "    else: # 감지된 언어와 (단순화된) 목표 언어가 같으면 번역 안 함\n",
    "        print(f\"번역 불필요: 감지된 언어 '{detected_lang}' (단순화: '{detected_lang.lower()}') == 목표 언어 '{target_lang_deepl}' (단순화: '{simple_target_deepl_code}'). 원본 텍스트 반환.\")\n",
    "        return text, detected_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a269aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 필요: 감지된 언어 'en' -> 목표 언어 'KO'\n",
      "원본 (EN): Hello. How are you today?\n",
      "감지된 언어: en\n",
      "번역된 텍스트 (KO): 안녕하세요. 오늘은 어떠세요?\n",
      "----\n",
      "번역 필요: 감지된 언어 'ko' -> 목표 언어 'EN'\n",
      "원본 (KO): 안녕하세요. 오늘 기분이 어떠신가요?\n",
      "감지된 언어: ko\n",
      "번역된 텍스트 (EN-US): hello. How are you feeling today?\n"
     ]
    }
   ],
   "source": [
    "# 문서 번역 테스트\n",
    "if translator:\n",
    "    sample_text_en = \"Hello. How are you today?\"\n",
    "    translated_text, detected_lang_sample = detect_and_translate(sample_text_en, target_lang_deepl='KO') # target_lang_detect 기본값 'ko' 사용\n",
    "    print(f\"원본 (EN): {sample_text_en}\")\n",
    "    print(f\"감지된 언어: {detected_lang_sample}\")\n",
    "    print(f\"번역된 텍스트 (KO): {translated_text}\")\n",
    "    print(\"----\")\n",
    "\n",
    "    sample_text_ko = \"안녕하세요. 오늘 기분이 어떠신가요?\"\n",
    "    translated_text_en, detected_lang_sample_ko = detect_and_translate(sample_text_ko, target_lang_deepl='EN') # target_lang_detect 기본값 'ko' 사용\n",
    "    print(f\"원본 (KO): {sample_text_ko}\")\n",
    "    print(f\"감지된 언어: {detected_lang_sample_ko}\")\n",
    "    print(f\"번역된 텍스트 (EN-US): {translated_text_en}\")\n",
    "else:\n",
    "    print(\"DeepL 번역기가 없어 번역 테스트를 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73f06d",
   "metadata": {},
   "source": [
    "**(2) 번역 통합 RAG 체인 성능 평가**\n",
    "\n",
    "이제 `detect_and_translate` 함수를 RAG 체인 앞뒤에 붙여서, 번역 통합 RAG 시스템을 만들어볼 거임.\n",
    "이 체인은 다음과 같이 작동함:\n",
    "1. 외국어 질문이 들어오면 -> 한국어(우리 문서 저장소 언어)로 번역\n",
    "2. 번역된 한국어 질문으로 -> 한국어 컨텍스트 기반 답변 생성 (답변도 한국어)\n",
    "3. 생성된 한국어 답변을 -> 원래 질문 언어로 다시 번역해서 사용자에게 전달!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e64a23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 기능을 통합할 RAG 체인 (한국어 기반) 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "lang_rag_chain_translate = None\n",
    "if vectorstore_ko_trans: # 한국어(중심) 벡터 저장소가 제대로 로드됐는지 먼저 확인\n",
    "    retriever_ko_trans = vectorstore_ko_trans.as_retriever(search_kwargs={'k': 2}) # 여기서도 문서 2개 가져옴\n",
    "\n",
    "    # RAG 체인 자체는 1-1에서 만든 것과 거의 동일함 (프롬프트, LLM, 포맷터 재사용)\n",
    "    # 달라지는 건 이 체인을 어떻게 호출하고 결과를 처리하는지임!\n",
    "    lang_rag_chain_translate = (\n",
    "        {\"context\": retriever_ko_trans | format_docs_cl , \"question\": RunnablePassthrough()}\n",
    "        | prompt_template_cl # 1-1에서 정의한 프롬프트 (컨텍스트 기반 답변, 모르면 모른다고 하기)\n",
    "        | llm_cl             # 1-1에서 정의한 LLM (gpt-4o-mini)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"번역 기능을 통합할 RAG 체인 (한국어 기반) 준비 완료!\")\n",
    "else:\n",
    "    print(\"한국어(중심) 벡터 저장소가 없어서 번역 통합 RAG 체인을 만들 수가 없어요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f65af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역을 포함해서 전체 RAG 과정을 실행하는 함수\n",
    "def run_lang_rag_chain_with_translation(query: str, rag_chain_to_use, \n",
    "                                        document_language_deepl='KO', \n",
    "                                        document_language_detect='ko'):\n",
    "    if not rag_chain_to_use:\n",
    "        return \"RAG 체인이 준비되지 않았습니다. 먼저 체인을 만들어주세요.\"\n",
    "\n",
    "    print(f\"\\n[처리 시작] 원본 질문: '{query}'\")\n",
    "\n",
    "    # 1. 질문 언어 감지 및 문서 저장소 언어(여기선 한국어)로 번역\n",
    "    #    DeepL 번역기가 없으면, 번역 없이 원본 질문과 감지된 언어 코드만 받음.\n",
    "    translated_query, original_lang_code_lc = detect_and_translate(query, \n",
    "                                                                   target_lang_deepl=document_language_deepl, \n",
    "                                                                   target_lang_detect=document_language_detect)\n",
    "    print(f\"   질문 언어 감지 결과: '{original_lang_code_lc}'\")\n",
    "    if original_lang_code_lc.lower() != document_language_detect.lower() and query != translated_query:\n",
    "        print(f\"   문서 저장소 언어({document_language_deepl})로 번역된 질문: {translated_query}\")\n",
    "    \n",
    "    # 2. 번역된 (또는 원본) 질문으로 RAG 체인 실행 -> 답변은 문서 저장소 언어로 생성됨\n",
    "    print(f\"   RAG 시스템에 '{translated_query}' 전달하여 답변 생성 중...\")\n",
    "    output_in_doc_lang = rag_chain_to_use.invoke(translated_query)\n",
    "    print(f\"   RAG 시스템 답변 (문서 저장소 언어 - {document_language_deepl}): {output_in_doc_lang}\")\n",
    "    \n",
    "    # 3. 생성된 답변을 원래 질문 언어로 다시 번역 (필요한 경우, DeepL 번역기 있을 때만)\n",
    "    if original_lang_code_lc.lower() != document_language_detect.lower() and query != translated_query: # 원본 질문이 번역되었었다면\n",
    "        # DeepL이 지원하는 target_lang 코드로 변환 필요.\n",
    "        # langdetect는 'en', 'ja' 등을 반환하지만, DeepL은 'EN-US', 'JA' 등을 기대함.\n",
    "        target_lang_deepl_for_answer = original_lang_code_lc.upper() # 일단 대문자로\n",
    "        if target_lang_deepl_for_answer == 'EN': # langdetect가 'en'이면 보통 'EN-US'나 'EN-GB'를 써야 함\n",
    "            target_lang_deepl_for_answer = 'EN-US' # 예시로 미국 영어 사용\n",
    "        # 다른 언어(예: 'JA', 'FR')들은 보통 2자리 코드가 그대로 쓰임. 필요시 규칙 추가.\n",
    "\n",
    "        print(f\"   답변을 원본 질문 언어('{target_lang_deepl_for_answer}')로 다시 번역 시도...\")\n",
    "        final_output, _ = detect_and_translate(output_in_doc_lang, \n",
    "                                               target_lang_deepl=target_lang_deepl_for_answer, \n",
    "                                               target_lang_detect=target_lang_deepl_for_answer.lower()[:2]) # 답변 번역 시엔 target_lang_detect를 DeepL 코드 기준으로 맞춰줌\n",
    "        if output_in_doc_lang != final_output:\n",
    "             print(f\"   최종 번역된 답변 ({target_lang_deepl_for_answer}): {final_output}\")\n",
    "        else:\n",
    "             print(f\"   답변 번역 시도했으나, (아마도 번역기 부재로) 원본 답변과 동일합니다.\")\n",
    "        return final_output\n",
    "    else: # 원래 질문이 문서 저장소 언어였으면 번역 없이 바로 반환\n",
    "        print(f\"   원본 질문이 이미 문서 저장소 언어와 같으므로, 답변 번역 없이 반환합니다.\")\n",
    "        return output_in_doc_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5586123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[처리 시작] 원본 질문: '테슬라 창업자는 누구인가요?'\n",
      "번역 불필요: 감지된 언어 'ko' (단순화: 'ko') == 목표 언어 'KO' (단순화: 'ko'). 원본 텍스트 반환.\n",
      "   질문 언어 감지 결과: 'ko'\n",
      "   RAG 시스템에 '테슬라 창업자는 누구인가요?' 전달하여 답변 생성 중...\n",
      "   RAG 시스템 답변 (문서 저장소 언어 - KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "   원본 질문이 이미 문서 저장소 언어와 같으므로, 답변 번역 없이 반환합니다.\n",
      "최종 답변 (KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "\n",
      "[처리 시작] 원본 질문: 'Who is the founder of Tesla?'\n",
      "번역 필요: 감지된 언어 'en' -> 목표 언어 'KO'\n",
      "   질문 언어 감지 결과: 'en'\n",
      "   문서 저장소 언어(KO)로 번역된 질문: 테슬라의 창립자는 누구인가요?\n",
      "   RAG 시스템에 '테슬라의 창립자는 누구인가요?' 전달하여 답변 생성 중...\n",
      "   RAG 시스템 답변 (문서 저장소 언어 - KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "   답변을 원본 질문 언어('EN-US')로 다시 번역 시도...\n",
      "번역 필요: 감지된 언어 'ko' -> 목표 언어 'EN-US'\n",
      "   최종 번역된 답변 (EN-US): Martin Everhard and Mark Tappening.\n",
      "최종 답변 (EN-US): Martin Everhard and Mark Tappening.\n",
      "\n",
      "[처리 시작] 원본 질문: 'テスラの創業者は誰ですか？'\n",
      "번역 필요: 감지된 언어 'ja' -> 목표 언어 'KO'\n",
      "   질문 언어 감지 결과: 'ja'\n",
      "   문서 저장소 언어(KO)로 번역된 질문: 테슬라의 창업자는 누구인가요?\n",
      "   RAG 시스템에 '테슬라의 창업자는 누구인가요?' 전달하여 답변 생성 중...\n",
      "   RAG 시스템 답변 (문서 저장소 언어 - KO): 마틴 에버하드와 마크 타페닝입니다.\n",
      "   답변을 원본 질문 언어('JA')로 다시 번역 시도...\n",
      "번역 필요: 감지된 언어 'ko' -> 목표 언어 'JA'\n",
      "   최종 번역된 답변 (JA): マーティン・エバーハードとマーク・タフェニングです。\n",
      "최종 답변 (JA): マーティン・エバーハードとマーク・タフェニングです。\n"
     ]
    }
   ],
   "source": [
    "if lang_rag_chain_translate: # 체인이 준비되었을 때만 테스트!\n",
    "    # 시나리오 1: 한국어 질문 (번역 거의 불필요)\n",
    "    query_ko_trans = \"테슬라 창업자는 누구인가요?\"\n",
    "    # print(f\"\\n--- [번역 통합 RAG] 한국어 질문 테스트: '{query_ko_trans}' ---\")\n",
    "    output_ko_final = run_lang_rag_chain_with_translation(query_ko_trans, lang_rag_chain_translate, \n",
    "                                                          document_language_deepl='KO', document_language_detect='ko')\n",
    "    print(f\"최종 답변 (KO): {output_ko_final}\")\n",
    "\n",
    "    # 시나리오 2: 영어 질문 (질문: EN->KO 번역, 답변: KO->EN-US 번역 필요)\n",
    "    query_en_trans = \"Who is the founder of Tesla?\"\n",
    "    # print(f\"\\n--- [번역 통합 RAG] 영어 질문 테스트: '{query_en_trans}' ---\")\n",
    "    output_en_final = run_lang_rag_chain_with_translation(query_en_trans, lang_rag_chain_translate, \n",
    "                                                          document_language_deepl='KO', document_language_detect='ko')\n",
    "    print(f\"최종 답변 (EN-US): {output_en_final}\")\n",
    "\n",
    "    # 시나리오 3: 일본어 질문 (DeepL 무료 버전은 지원 언어 제한적일 수 있음. API 키 플랜 확인!)\n",
    "    # (질문: JA->KO 번역, 답변: KO->JA 번역 필요)\n",
    "    query_ja_trans = \"テスラの創業者は誰ですか？\"\n",
    "    # print(f\"\\n--- [번역 통합 RAG] 일본어 질문 테스트: '{query_ja_trans}' ---\")\n",
    "    output_ja_final = run_lang_rag_chain_with_translation(query_ja_trans, lang_rag_chain_translate, \n",
    "                                                          document_language_deepl='KO', document_language_detect='ko')\n",
    "    print(f\"최종 답변 (JA): {output_ja_final}\")\n",
    "else:\n",
    "    print(\"\\n번역 통합 RAG 체인이 준비되지 않아서 실행을 건너뜁니다. 😢\")\n",
    "    if not translator:\n",
    "        print(\"   특히 DeepL 번역기가 준비되지 않은 것 같네요. API 키를 확인해주세요!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708249ce",
   "metadata": {},
   "source": [
    "### 1-3 언어 감지 및 벡터저장소 라우팅\n",
    "\n",
    "- 이 전략은 각 언어별로 최적화된 RAG 시스템(문서 저장소, 임베딩 모델 등)을 따로 구축하고, 사용자 질문의 언어를 감지해서 알맞은 시스템으로 \"길안내(라우팅)\" 해주는 방식임.\n",
    "\n",
    "\n",
    "**전략:**\n",
    "1.  **언어별 벡터 저장소 구축**: 한국어 문서는 한국어 특화 임베딩 모델로, 영어 문서는 영어 특화 (또는 좋은 다국어) 임베딩 모델로 각각 별도의 벡터 저장소를 만듦. (필요하면 언어별로 LLM도 다르게 설정 가능!)\n",
    "2.  **언어 감지**: 사용자 질문의 언어를 감지\n",
    "3.  **라우팅**: 감지된 언어에 해당하는 벡터 저장소 및 관련 RAG 체인으로 질문을 전달\n",
    "4.  **RAG 처리**: 선택된 RAG 체인에서 답변을 생성\n",
    "\n",
    "**장점:**\n",
    "- 각 언어에 최적화된 임베딩과 RAG 파이프라인을 사용할 수 있어, 해당 언어 내에서의 검색 및 답변 품질이 높을 수 있음.\n",
    "- 번역 과정이 없어 지연 시간과 번역 비용이 발생하지 않음/ (단, 교차 언어 질의는 직접 지원하지 않음).\n",
    "\n",
    "**단점:**\n",
    "- **여러 벡터 저장소 관리**: 지원하는 언어 수만큼 벡터 저장소와 RAG 체인을 관리해야 하므로 복잡성이 증가.\n",
    "- **교차 언어 검색 미지원**: 기본적으로는 질문 언어와 동일한 언어의 문서만 검색(예: 영어 질문으로는 영어 문서만 검색). 교차 언어 검색을 지원하려면 각 저장소에서 사용하는 임베딩 모델 자체가 교차 언어 성능이 뛰어나거나, 추가적인 번역 로직이 필요.\n",
    "- **언어 감지 정확도 중요**: 언어 감지가 잘못되면 엉뚱한 저장소로 라우팅되어 성능이 저하\n",
    "\n",
    "\n",
    "**꿀팁 & 노하우:** 💡\n",
    "- **라우팅 로직 설계**: 단순히 언어 코드(`ko`, `en`)로 분기하는 것 외에도, 특정 키워드나 질문 패턴에 따라 다른 RAG 체인으로 보낼 수도 있음. (LangChain의 `RouterChain` 같은 걸 활용 가능)\n",
    "- **Fallback 전략**: 언어 감지가 애매하거나 지원하지 않는 언어의 질문이 들어왔을 때, 어떻게 처리할지 미리 정해두는 게 좋음. (예: 기본 언어(영어) RAG로 보내거나, \"지원하지 않는 언어입니다\" 안내)\n",
    "- **점진적 확장**: 처음부터 모든 언어를 다 지원하려고 하기보다, 주요 타겟 언어 2~3개부터 시작해서 점차 늘려나가는 방식이 현실적임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e7670",
   "metadata": {},
   "source": [
    "**(1) 언어별 벡터 저장소 생성**\n",
    "\n",
    "이제 진짜 언어별로 독립된 벡터 저장소를 만들어 볼 거임.\n",
    "- 한국어 문서(`korean_docs_split`)는 한국어 처리에 유리한 임베딩 모델로!\n",
    "- 영어 문서(`english_docs_split`)는 영어 처리에 유리하거나 좋은 다국어 임베딩 모델로!\n",
    "각각 별도의 ChromaDB 컬렉션에 저장할 거임.\n",
    "\n",
    "**임베딩 모델 선택 예시:**\n",
    "- **한국어 문서용**: 1-1에서 사용했던 HuggingFace의 `BAAI/bge-m3`를 그대로 써보겠음. (이 모델은 다국어지만 한국어 성능도 좋음. 만약 더 한국어에 특화된 모델이 있다면 그걸 써도 좋음! 예: `ko-sroberta-multitask`)\n",
    "- **영어 문서용**: 1-1에서 사용했던 Ollama의 `bge-m3`를 써보겠음. (만약 Ollama가 안된다면 OpenAI 모델 등으로 대체 가능)\n",
    "\n",
    "**중요**: 각 언어에 \"최적\"인 임베딩을 찾는 것은 실험이 필요할 수 있음! 여기서는 사용 가능한 모델로 구성함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec094ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문서용 벡터 저장소(HuggingFace bge-m3 사용) 생성 시작...\n",
      "   한국어 라우팅용 벡터 저장소에 저장된 문서(청크) 수: 10\n",
      "\n",
      "영어 문서용 임베딩으로 Ollama bge-m3를 사용합니다.\n",
      "영어 문서용 벡터 저장소 생성 시작...\n",
      "   영어 라우팅용 벡터 저장소에 저장된 문서(청크) 수: 4\n"
     ]
    }
   ],
   "source": [
    "# 한국어 문서와 영어 문서를 각각 다른 벡터 저장소(컬렉션)에 저장할 거임.\n",
    "db_korean_route = None\n",
    "db_english_route = None\n",
    "\n",
    "# === 한국어 문서용 벡터 저장소 ===\n",
    "# 여기서는 1-1에서 정의한 HuggingFace bge-m3 임베딩(embeddings_huggingface_bge_m3_cl)을 재사용.\n",
    "if korean_docs_split and embeddings_huggingface_bge_m3_cl:\n",
    "    print(\"한국어 문서용 벡터 저장소(HuggingFace bge-m3 사용) 생성 시작...\")\n",
    "    db_korean_route = Chroma.from_documents(\n",
    "        documents=korean_docs_split, \n",
    "        embedding=embeddings_huggingface_bge_m3_cl, \n",
    "        collection_name=\"db_korean_for_routing_v3\", # 라우팅용 한국어 컬렉션 이름\n",
    "        persist_directory=\"./chroma_db_routed\", # 라우팅 전용 저장 폴더\n",
    "    )\n",
    "    print(f\"   한국어 라우팅용 벡터 저장소에 저장된 문서(청크) 수: {db_korean_route._collection.count()}\")\n",
    "else:\n",
    "    print(\"분할된 한국어 문서가 없거나 HuggingFace 임베딩 모델이 준비 안돼서 한국어 라우팅용 벡터 저장소는 스킵함.\")\n",
    "\n",
    "# === 영어 문서용 벡터 저장소 ===\n",
    "# 여기서는 1-1에서 정의한 Ollama bge-m3 임베딩(embeddings_ollama_bge_m3_cl)을 재사용.\n",
    "# Ollama가 안되면 OpenAI 임베딩 (embeddings_openai_small_cl) 등으로 대체 가능.\n",
    "embedding_for_english_route = None\n",
    "if ollama_ready_cl and embeddings_ollama_bge_m3_cl:\n",
    "    embedding_for_english_route = embeddings_ollama_bge_m3_cl\n",
    "    print(\"\\n영어 문서용 임베딩으로 Ollama bge-m3를 사용합니다.\")\n",
    "elif embeddings_openai_small_cl: # Ollama 없으면 OpenAI 모델로 대체 시도\n",
    "    embedding_for_english_route = embeddings_openai_small_cl\n",
    "    print(\"\\nOllama bge-m3를 사용할 수 없어, 영어 문서용 임베딩으로 OpenAI text-embedding-3-small을 대신 사용합니다.\")\n",
    "else:\n",
    "    print(\"\\n영어 문서용 임베딩 모델을 찾을 수 없습니다.\")\n",
    "\n",
    "if english_docs_split and embedding_for_english_route:\n",
    "    print(\"영어 문서용 벡터 저장소 생성 시작...\")\n",
    "    db_english_route = Chroma.from_documents(\n",
    "        documents=english_docs_split, \n",
    "        embedding=embedding_for_english_route, \n",
    "        collection_name=\"db_english_for_routing_v3\", # 라우팅용 영어 컬렉션 이름\n",
    "        persist_directory=\"./chroma_db_routed\", # 라우팅 전용 저장 폴더 (위와 동일 폴더, 다른 컬렉션)\n",
    "    )\n",
    "    print(f\"   영어 라우팅용 벡터 저장소에 저장된 문서(청크) 수: {db_english_route._collection.count()}\")\n",
    "elif not english_docs_split:\n",
    "    print(\"\\n분할된 영어 문서가 없어서 영어 라우팅용 벡터 저장소는 스킵함.\")\n",
    "else:\n",
    "    print(\"\\n영어 문서용 임베딩 모델이 준비 안돼서 영어 라우팅용 벡터 저장소는 스킵함.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586c90a",
   "metadata": {},
   "source": [
    "**(2) 라우팅 RAG 체인 성능 평가**\n",
    "\n",
    "- 이제 언어 감지 결과를 보고 \"이 질문은 한국어 RAG 팀으로!\", \"저 질문은 영어 RAG 팀으로!\" 하고 똑똑하게 나눠주는 함수를 만들어서 테스트해는 것임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ba3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라우팅용 한국어 RAG 체인 생성 완료\n",
      "라우팅용 영어 RAG 체인 생성 완료\n",
      "\n",
      "--- 실행: 라우팅 한국어 질문 '테슬라 창업자는 누구인가요?' ---\n",
      "감지된 질문 언어: ko\n",
      "한국어 RAG 체인으로 라우팅합니다.\n",
      "결과 (KO 라우팅): 마틴 에버하드와 마크 타페닝입니다.\n",
      "\n",
      "--- 실행: 라우팅 영어 질문 'Who is the founder of Tesla?' ---\n",
      "감지된 질문 언어: en\n",
      "영어 RAG 체인으로 라우팅합니다.\n",
      "결과 (EN 라우팅): Tesla was co-founded in 2003 by Martin Eberhard and Marc Tarpenning.\n",
      "\n",
      "--- 실행: 라우팅 일본어 질문 'テスラの創業者は誰ですか？' ---\n",
      "감지된 질문 언어: ja\n",
      "결과 (JA 라우팅): 지원하지 않는 언어(ja)이거나 해당 언어의 RAG 체인이 없습니다. 한국어 또는 영어로 질문해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 각 언어별 RAG 체인 생성 (create_rag_chain_cl 함수 재사용)\n",
    "rag_chain_korean_routed = create_rag_chain_cl(db_korean_route)\n",
    "rag_chain_english_routed = create_rag_chain_cl(db_english_route)\n",
    "\n",
    "if rag_chain_korean_routed: print(\"라우팅용 한국어 RAG 체인 생성 완료\")\n",
    "else: print(\"라우팅용 한국어 RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "if rag_chain_english_routed: print(\"라우팅용 영어 RAG 체인 생성 완료\")\n",
    "else: print(\"라우팅용 영어 RAG 체인 생성 실패 (벡터 저장소 없음)\")\n",
    "\n",
    "def run_route_rag_chain(query: str):\n",
    "    try:\n",
    "        detected_query_lang = detect(query) # 질문 언어 감지\n",
    "        print(f\"감지된 질문 언어: {detected_query_lang}\")\n",
    "    except Exception as e:\n",
    "        print(f\"질문 언어 감지 실패: {e}. 기본 체인(한국어) 시도 또는 에러 반환.\")\n",
    "        # detected_query_lang = 'unknown' # 또는 기본 언어 설정\n",
    "        return f\"질문 언어 감지 실패: {e}\"\n",
    "    \n",
    "    if detected_query_lang == 'ko' and rag_chain_korean_routed:\n",
    "        print(\"한국어 RAG 체인으로 라우팅합니다.\")\n",
    "        return rag_chain_korean_routed.invoke(query)\n",
    "    elif detected_query_lang == 'en' and rag_chain_english_routed:\n",
    "        print(\"영어 RAG 체인으로 라우팅합니다.\")\n",
    "        return rag_chain_english_routed.invoke(query)\n",
    "    else:\n",
    "        # 지원하지 않는 언어 또는 해당 언어 체인이 없는 경우\n",
    "        if detected_query_lang == 'ko' and not rag_chain_korean_routed:\n",
    "            return \"한국어 RAG 체인이 준비되지 않았습니다.\"\n",
    "        if detected_query_lang == 'en' and not rag_chain_english_routed:\n",
    "            return \"영어 RAG 체인이 준비되지 않았습니다.\"\n",
    "        return f\"지원하지 않는 언어({detected_query_lang})이거나 해당 언어의 RAG 체인이 없습니다. 한국어 또는 영어로 질문해주세요.\"\n",
    "    \n",
    "# 한국어 쿼리에 대한 성능 평가\n",
    "query_ko_route = \"테슬라 창업자는 누구인가요?\"\n",
    "print(f\"\\n--- 실행: 라우팅 한국어 질문 '{query_ko_route}' ---\")\n",
    "output_ko_route = run_route_rag_chain(query_ko_route)\n",
    "print(f\"결과 (KO 라우팅): {output_ko_route}\")\n",
    "\n",
    "# 영어 쿼리에 대한 성능 평가\n",
    "query_en_route = \"Who is the founder of Tesla?\"\n",
    "print(f\"\\n--- 실행: 라우팅 영어 질문 '{query_en_route}' ---\")\n",
    "output_en_route = run_route_rag_chain(query_en_route)\n",
    "print(f\"결과 (EN 라우팅): {output_en_route}\")\n",
    "\n",
    "# 지원하지 않는 언어 테스트 (예: 일본어)\n",
    "query_ja_route = \"テスラの創業者は誰ですか？\"\n",
    "print(f\"\\n--- 실행: 라우팅 일본어 질문 '{query_ja_route}' ---\")\n",
    "output_ja_route = run_route_rag_chain(query_ja_route)\n",
    "print(f\"결과 (JA 라우팅): {output_ja_route}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
