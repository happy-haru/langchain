{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a88caf",
   "metadata": {},
   "source": [
    "## 1. 검색 성능 평가\n",
    "\n",
    "- 구축한 검색 시스템(Retriever)이 얼마나 잘 동작하는지 정량적으로 평가하는 것은 매우 중요함. \n",
    "- 이를 위해 테스트 데이터셋을 만들고, 정보 검색(IR) 분야의 평가지표를 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1089c06",
   "metadata": {},
   "source": [
    "### 1.1 테스트 데이터 준비\n",
    "\n",
    "- 좋은 평가를 위해서는 양질의 테스트 데이터셋이 필요함. \n",
    "- 여기서는 기존 문서들을 기반으로 질문-답변(QA) 쌍을 합성하고, 이를 검토/수정하여 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d71465",
   "metadata": {},
   "source": [
    "`(1) 평가용 문서 데이터 정제`\n",
    "\n",
    "- `korean_docs`를 평가용으로 좀 더 가공함.\n",
    "  - 각 문서에 고유 `doc_id` 메타데이터 추가.\n",
    "  - 문장 구분 기호를 줄바꿈 문자로 변경 (가독성 및 LLM 처리 용이성).\n",
    "  - 문서 내용 끝에 해당 문서가 어떤 기업에 대한 정보인지 명시 (LLM이 QA 생성 시 참고하도록).\n",
    "- JSONL (JSON Lines) 형식으로 저장하여 재사용 용이하게 함.\n",
    "- **장점:** 평가 데이터의 일관성 및 재현성 확보, LLM이 QA 생성 시 컨텍스트 명확화.\n",
    "- **단점:** 수동 전처리 과정 필요, 데이터 형식 변환에 따른 약간의 오버헤드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc5a92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eee247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from glob import glob\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b1d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_files(txt_files):\n",
    "    data = []\n",
    "    for text_file in txt_files:\n",
    "        print(f\"로딩 중: {text_file}\")\n",
    "        loader = TextLoader(text_file, encoding='utf-8') \n",
    "        data.extend(loader.load())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ad8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로딩 중: data\\Rivian_KR.txt\n",
      "로딩 중: data\\Tesla_KR.txt\n",
      "\n",
      "로드된 전체 문서 수: 2\n",
      "첫 번째 문서 내용 일부: 2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당...\n"
     ]
    }
   ],
   "source": [
    "korean_txt_files = glob(os.path.join('data', '*_KR.txt'))\n",
    "if not korean_txt_files:\n",
    "    print(\"'data' 폴더에 '*_KR.txt' 파일이 없습니다. 예시 데이터를 생성하거나 경로를 확인하세요.\")\n",
    "    korean_data = [] # 빈 리스트로 초기화\n",
    "else:\n",
    "    korean_data = load_text_files(korean_txt_files)\n",
    "\n",
    "print(f\"\\n로드된 전체 문서 수: {len(korean_data)}\")\n",
    "if korean_data:\n",
    "    print(f\"첫 번째 문서 내용 일부: {korean_data[0].page_content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165bc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "# 문장 구분자(정규식)를 사용하여 텍스트 분할기 생성\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,          # 토큰 수 계산 기준\n",
    "    separator=r\"[.!?]\\s+\",     # 문장 구분자: 마침표, 느낌표, 물음표 뒤 공백\n",
    "    chunk_size=100,             # 청크 최대 토큰 수 (토크나이저 기준)\n",
    "    chunk_overlap=20,            # 청크 간 중복 토큰 수\n",
    "    is_separator_regex=True,    # separator가 정규식임을 명시\n",
    "    keep_separator=True,        # 구분자 유지 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092f4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "분할된 한국어 문서 수: 8\n",
      "첫 번째 분할 문서 내용: 2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당한 투자를 통해 비약적인 성장을 거듭하며 미시간과 베이 지역에 연구 시설을 설립했습니다. 주요 공급업체와의 거리를 좁히기 위해 본사를 미시간주 리보니아로 이전했습니다\n",
      "첫 번째 분할 문서 메타데이터: {'source': 'data\\\\Rivian_KR.txt'}\n"
     ]
    }
   ],
   "source": [
    "korean_docs = []\n",
    "if korean_data: # 로드된 데이터가 있을 경우에만 분할 수행\n",
    "    korean_docs = text_splitter.split_documents(korean_data)\n",
    "\n",
    "print(f\"\\n분할된 한국어 문서 수: {len(korean_docs)}\")\n",
    "if korean_docs:\n",
    "    print(f\"첫 번째 분할 문서 내용: {korean_docs[0].page_content}\")\n",
    "    print(f\"첫 번째 분할 문서 메타데이터: {korean_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577a0156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가용으로 정제된 문서 수: 8\n",
      "\n",
      "첫 번째 정제 문서 예시:\n",
      "2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당한 투자를 통해 비약적인 성장을 거듭하며 미시간과 베이 지역에 연구 시설을 설립했습니다. 주요 공급업체와의 거리를 좁히기 위해 본사를 미시간주 리보니아로 이전했습니다\n",
      "\n",
      "(참고: 이 문서는 'Rivian'에 대한 정보를 담고 있습니다.)\n",
      "------------------------------\n",
      "{'source': 'data\\\\Rivian_KR.txt', 'doc_id': '0'}\n",
      "==============================\n",
      "\n",
      "평가용 문서가 './data/final_docs_for_eval.jsonl' 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_29428\\3445766471.py:5: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  new_doc = doc.copy()\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "final_docs_for_eval = []\n",
    "if korean_docs: \n",
    "    for i, doc in enumerate(korean_docs):\n",
    "        new_doc = doc.copy() \n",
    "        new_doc.metadata['doc_id'] = f'{i}'\n",
    "        source_filename = os.path.basename(new_doc.metadata.get('source', ''))\n",
    "        corp_name = source_filename.split('_')[0] if '_' in source_filename else source_filename.replace('.txt', '')\n",
    "        new_doc.page_content = f\"{new_doc.page_content}\\n\\n(참고: 이 문서는 '{corp_name}'에 대한 정보를 담고 있습니다.)\"\n",
    "        final_docs_for_eval.append(new_doc)\n",
    "\n",
    "    print(f\"평가용으로 정제된 문서 수: {len(final_docs_for_eval)}\")\n",
    "    if final_docs_for_eval:\n",
    "        print(\"\\n첫 번째 정제 문서 예시:\")\n",
    "        print(final_docs_for_eval[0].page_content)\n",
    "        print(\"-\" * 30)\n",
    "        print(final_docs_for_eval[0].metadata)\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "    with open('./data/final_docs_for_eval.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for doc_item in final_docs_for_eval:\n",
    "            f.write(json.dumps({'page_content': doc_item.page_content, 'metadata': doc_item.metadata}) + '\\n')\n",
    "    print(\"\\n평가용 문서가 './data/final_docs_for_eval.jsonl' 파일로 저장되었습니다.\")\n",
    "\n",
    "else:\n",
    "    print(\"korean_docs가 없어 평가용 문서를 정제할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78e254",
   "metadata": {},
   "source": [
    "`(2) Question-Answer (QA) 합성`\n",
    "\n",
    "- 정제된 문서(`final_docs_for_eval`)의 각 청크(문서)를 컨텍스트로 하여, LLM(여기서는 OpenAI GPT 모델)을 사용해 질문과 답변 쌍을 생성함.\n",
    "- `PydanticOutputParser`를 사용하여 LLM의 출력을 구조화된 객체(QAPair, QASet)로 파싱함.\n",
    "- 프롬프트 엔지니어링을 통해 사실 기반의, 검색 엔진 사용자 스타일 질문을 생성하도록 유도함.\n",
    "- **장점:** 대량의 QA 평가 데이터셋을 자동으로 생성 가능.\n",
    "- **단점:**\n",
    "  - LLM API 사용 비용 발생 (OpenAI API 키 필요).\n",
    "  - 생성된 QA의 품질이 완벽하지 않을 수 있어 검토 및 수정 과정이 필요함.\n",
    "  - 프롬프트에 민감하게 반응하므로, 원하는 품질의 QA를 얻기 위해 프롬프트 튜닝이 필요할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9ec3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA 생성 테스트용 컨텍스트:\n",
      "2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율주행 전기차에 집중했던 리비안은 2015년 상당한 투자를 통해 비약적인 성장을 거듭하며 미시간과 베이 지역에 연구 시설을 설립했습니다. 주요 공급업체와의 거리를 좁히기 위해 본사를 미시간주 리보니아로 이전했습니다\n",
      "\n",
      "(참고: 이 문서는 'Rivian'에 대한 정보를 담고 있습니다.)\n",
      "\n",
      "생성된 QA 쌍 예시:\n",
      "  질문: 리비안의 설립자는 누구인가요?\n",
      "  답변: 리비안은 2009년 MIT 박사 과정생 RJ 스캐린지에 의해 설립되었습니다.\n",
      "실제 QA 생성은 주석 처리됨 (API 비용 발생 방지). 필요시 주석 해제 후 실행.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "# QA 쌍을 위한 Pydantic 모델 정의\n",
    "class QAPair(BaseModel):\n",
    "    question: str = Field(description=\"생성된 질문 (한국어로 작성)\")\n",
    "    answer: str = Field(description=\"질문에 대한 답변 (한국어로 작성, 질문의 핵심 내용을 반영)\")\n",
    "\n",
    "class QASet(BaseModel):\n",
    "    qa_pairs: List[QAPair] = Field(description=\"질문-답변 쌍(QAPair)의 리스트\")\n",
    "\n",
    "# QA 생성 프롬프트 템플릿\n",
    "QA_GENERATION_TEMPLATE_KOREAN = \"\"\"\n",
    "당신은 주어진 컨텍스트를 기반으로 사실에 입각한 질문-답변 쌍을 생성하는 AI입니다.\n",
    "컨텍스트에서 특정하고 간결한 사실 정보로 답변할 수 있는 질문 {num_questions_per_chunk}개를 만드세요.\n",
    "질문은 사용자가 검색 엔진에 질문할 법한 스타일로 작성해주세요.\n",
    "질문에 \"제시된 구절에 따르면\" 또는 \"컨텍스트에 따르면\" 같은 문구는 포함하지 마세요.\n",
    "답변에는 명확하고 완전한 정보를 제공하기 위해 질문의 핵심 내용이 포함되도록 하세요.\n",
    "\n",
    "---------------------------------------------------------\n",
    "출력은 다음 형식으로 제공해주세요:\n",
    "{format_instructions}\n",
    "---------------------------------------------------------\n",
    "이제 컨텍스트를 제공합니다:\n",
    "컨텍스트: {context}\n",
    "\"\"\"\n",
    "\n",
    "qa_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "pydantic_parser_qa = PydanticOutputParser(pydantic_object=QASet)\n",
    "\n",
    "qa_generation_prompt = ChatPromptTemplate.from_template(\n",
    "    template=QA_GENERATION_TEMPLATE_KOREAN,\n",
    "    partial_variables={\"format_instructions\": pydantic_parser_qa.get_format_instructions()}\n",
    ")\n",
    "\n",
    "qa_generation_chain = qa_generation_prompt | qa_llm | pydantic_parser_qa\n",
    "\n",
    "# QA 생성 테스트 (첫 번째 정제 문서 사용)\n",
    "if final_docs_for_eval:\n",
    "    test_context_for_qa = final_docs_for_eval[0].page_content\n",
    "    print(f\"QA 생성 테스트용 컨텍스트:\\n{test_context_for_qa}\\n\")\n",
    "    \n",
    "    qa_set_example = qa_generation_chain.invoke({\n",
    "        \"context\": test_context_for_qa,\n",
    "        \"num_questions_per_chunk\": 1 # 테스트로 1개만 생성\n",
    "    })\n",
    "    print(\"생성된 QA 쌍 예시:\")\n",
    "    for qa_pair in qa_set_example.qa_pairs:\n",
    "        print(f\"  질문: {qa_pair.question}\")\n",
    "        print(f\"  답변: {qa_pair.answer}\")\n",
    "    print(\"실제 QA 생성은 주석 처리됨 (API 비용 발생 방지). 필요시 주석 해제 후 실행.\")\n",
    "else:\n",
    "    print(\"정제된 평가용 문서가 없어 QA 생성 테스트를 스킵합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08953e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8개의 문서에 대해 QA 생성을 시작합니다...\n",
      "1/8번째 문서 처리 중... 성공\n",
      "2/8번째 문서 처리 중... 성공\n",
      "3/8번째 문서 처리 중... 성공\n",
      "4/8번째 문서 처리 중... 성공\n",
      "5/8번째 문서 처리 중... 성공\n",
      "6/8번째 문서 처리 중... 성공\n",
      "7/8번째 문서 처리 중... 성공\n",
      "8/8번째 문서 처리 중... 성공\n",
      "\n",
      "총 16개의 QA 쌍이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 전체 평가용 문서에 대해 QA 생성\n",
    "NUM_QUESTIONS_PER_CHUNK = 2 # 각 문서 청크당 생성할 QA 개수\n",
    "generated_qa_outputs = []\n",
    "\n",
    "if final_docs_for_eval :\n",
    "    print(f\"{len(final_docs_for_eval)}개의 문서에 대해 QA 생성을 시작합니다...\")\n",
    "    for i, doc_item in enumerate(final_docs_for_eval):\n",
    "        print(f\"{i+1}/{len(final_docs_for_eval)}번째 문서 처리 중...\", end=' ')\n",
    "        try:\n",
    "            qa_set_generated = qa_generation_chain.invoke({\n",
    "                \"context\": doc_item.page_content,\n",
    "                \"num_questions_per_chunk\": NUM_QUESTIONS_PER_CHUNK\n",
    "            })\n",
    "            for qa_pair in qa_set_generated.qa_pairs:\n",
    "                generated_qa_outputs.append({\n",
    "                    'context': [doc_item.page_content], # 정답 컨텍스트 (리스트 형태)\n",
    "                    'source': [doc_item.metadata.get('source', '')], # 출처 (리스트 형태)\n",
    "                    'doc_id': [doc_item.metadata.get('doc_id', '')], # 문서 ID (리스트 형태)\n",
    "                    'question': qa_pair.question,\n",
    "                    'answer': qa_pair.answer\n",
    "                })\n",
    "            print(\"성공\")\n",
    "        except Exception as e:\n",
    "            print(f\"실패: {e}\")\n",
    "            continue # 오류 발생 시 다음 문서로\n",
    "    \n",
    "    df_generated_qa_test = pd.DataFrame(generated_qa_outputs)\n",
    "    print(f\"\\n총 {df_generated_qa_test.shape[0]}개의 QA 쌍이 생성되었습니다.\")\n",
    "    df_generated_qa_test.to_excel(\"./data/generated_qa_test.xlsx\", index=False)\n",
    "else:\n",
    "    print(\"QA 합성은 실제 실행되지 않았습니다. (final_docs_for_eval이 없거나, 실행 플래그가 False임)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8bcaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신...</td>\n",
       "      <td>[data\\Rivian_KR.txt]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>리비안은 언제 설립되었나요?</td>\n",
       "      <td>리비안은 2009년에 설립되었습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신...</td>\n",
       "      <td>[data\\Rivian_KR.txt]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>리비안의 본사는 어디에 위치하고 있나요?</td>\n",
       "      <td>리비안의 본사는 미시간주 리보니아에 위치하고 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[.\\n\\n리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미...</td>\n",
       "      <td>[data\\Rivian_KR.txt]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>리비안의 초기 프로젝트는 어떤 차량이었나요?</td>\n",
       "      <td>리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미드십 엔진 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[.\\n\\n리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미...</td>\n",
       "      <td>[data\\Rivian_KR.txt]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>R1 차량의 특징은 무엇인가요?</td>\n",
       "      <td>R1 차량은 모듈식 캡슐 구조와 쉽게 교체 가능한 차체 패널을 특징으로 합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[. 리비안은 디젤 하이브리드, 브라질 원메이크 시리즈를 위한 R1 GT라는 이름의...</td>\n",
       "      <td>[data\\Rivian_KR.txt]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>리비안의 R1 GT는 어떤 종류의 차량인가요?</td>\n",
       "      <td>R1 GT는 리비안의 레이싱 버전으로, 디젤 하이브리드 차량입니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context                source  \\\n",
       "0  [2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신...  [data\\Rivian_KR.txt]   \n",
       "1  [2009년 MIT 박사 과정생 RJ 스캐린지가 설립한 리비안(Rivian)은 혁신...  [data\\Rivian_KR.txt]   \n",
       "2  [.\\n\\n리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미...  [data\\Rivian_KR.txt]   \n",
       "3  [.\\n\\n리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미...  [data\\Rivian_KR.txt]   \n",
       "4  [. 리비안은 디젤 하이브리드, 브라질 원메이크 시리즈를 위한 R1 GT라는 이름의...  [data\\Rivian_KR.txt]   \n",
       "\n",
       "  doc_id                   question  \\\n",
       "0    [0]            리비안은 언제 설립되었나요?   \n",
       "1    [0]     리비안의 본사는 어디에 위치하고 있나요?   \n",
       "2    [1]   리비안의 초기 프로젝트는 어떤 차량이었나요?   \n",
       "3    [1]          R1 차량의 특징은 무엇인가요?   \n",
       "4    [2]  리비안의 R1 GT는 어떤 종류의 차량인가요?   \n",
       "\n",
       "                                              answer  \n",
       "0                               리비안은 2009년에 설립되었습니다.  \n",
       "1                     리비안의 본사는 미시간주 리보니아에 위치하고 있습니다.  \n",
       "2  리비안의 초기 프로젝트는 피터 스티븐스가 디자인한 2+2 시트 배열의 미드십 엔진 ...  \n",
       "3       R1 차량은 모듈식 캡슐 구조와 쉽게 교체 가능한 차체 패널을 특징으로 합니다.  \n",
       "4              R1 GT는 리비안의 레이싱 버전으로, 디젤 하이브리드 차량입니다.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generated_qa_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e79bc",
   "metadata": {},
   "source": [
    "`(3) 테스트 데이터 검토 및 수정`\n",
    "\n",
    "- LLM이 생성한 QA 데이터는 완벽하지 않을 수 있으므로, 사람이 직접 검토하고 수정하는 과정이 중요함.\n",
    "- **장점:** 평가 데이터의 품질을 크게 향상시켜 평가 결과의 신뢰도를 높임.\n",
    "- **단점:** 시간과 노력이 많이 소요되는 수동 작업임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815d28a",
   "metadata": {},
   "source": [
    "### 1.2 Information Retrieval (IR) 평가지표\n",
    "\n",
    "- 검색 시스템이 얼마나 관련 있는 문서를 잘 찾아내는지, 그리고 그 순서는 적절한지를 평가하는 지표\n",
    "- 주요 지표: Hit Rate, MRR, mAP@k, NDCG@k 등."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98828a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 정답 문서 (actual_docs_sample) 및 예측 문서 (predicted_docs_sample) 준비 완료.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# 각 쿼리에 대한 정답 문서 (실제 관련 문서)\n",
    "actual_docs_sample = [\n",
    "    # Query 1에 대한 정답\n",
    "    [Document(metadata={'id': 'doc1'}, page_content='내용1')],\n",
    "    # Query 2에 대한 정답\n",
    "    [Document(metadata={'id': 'doc2'}, page_content='내용2'), Document(metadata={'id': 'doc5'}, page_content='내용5')]\n",
    "]\n",
    "\n",
    "# 각 쿼리에 대한 검색기가 예측한(찾아온) 문서\n",
    "predicted_docs_sample = [\n",
    "    # Query 1에 대한 예측\n",
    "    [Document(metadata={'id': 'doc1'}, page_content='내용1'), Document(metadata={'id': 'doc3'}, page_content='내용3')],\n",
    "    # Query 2에 대한 예측\n",
    "    [Document(metadata={'id': 'doc4'}, page_content='내용4'), Document(metadata={'id': 'doc1'}, page_content='내용1'), \n",
    "     Document(metadata={'id': 'doc5'}, page_content='내용5'), Document(metadata={'id': 'doc2'}, page_content='내용2')]\n",
    "]\n",
    "\n",
    "print(\"샘플 정답 문서 (actual_docs_sample) 및 예측 문서 (predicted_docs_sample) 준비 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76225948",
   "metadata": {},
   "source": [
    "**평가 지표 설명 (개념)**\n",
    "\n",
    "`(1) Hit Rate@k`\n",
    "- 정의: 각 쿼리에 대해 검색된 상위 `k`개의 문서 중에 실제 정답 문서가 **하나라도 포함**되어 있으면 1 (Hit), 아니면 0으로 계산. 전체 쿼리에 대한 평균을 냄.\n",
    "- 특징: 간단하고 직관적이지만, 정답 문서의 순위나 개수는 고려하지 않음.\n",
    "- 예시 (k=2, Query 1): 예측 [`doc1`, `doc3`], 정답 [`doc1`]. `doc1`이 포함되어 Hit (1).\n",
    "- 예시 (k=2, Query 2): 예측 [`doc4`, `doc1`], 정답 [`doc2`, `doc5`]. `doc2` 또는 `doc5` 미포함 (정답 문서 ID 기준). Not Hit (0). (샘플 데이터에서는 `doc5`가 3번째, `doc2`가 4번째에 있음)\n",
    "- Hit Rate@2 = (1 + 0) / 2 = 0.5 (만약 Query 2 예측에 `doc5`나 `doc2`가 상위 2개 안에 있었다면 1)\n",
    "\n",
    "`(2) MRR@k (Mean Reciprocal Rank)`\n",
    "- 정의: 각 쿼리에 대해 **첫 번째 정답 문서**가 검색된 순위(rank)의 역수(1/rank)를 계산. 이 값들의 평균.\n",
    "- 특징: 사용자가 원하는 결과를 얼마나 빨리 찾을 수 있는지(첫 정답의 순위)를 평가. 상위 `k`개 내에서만 고려.\n",
    "- 예시 (k=4, Query 1): 예측 [`doc1`, `doc3`], 정답 [`doc1`]. `doc1`이 1등. RR = 1/1 = 1.\n",
    "- 예시 (k=4, Query 2): 예측 [`doc4`, `doc1`, `doc5`, `doc2`], 정답 [`doc2`, `doc5`]. 첫 정답 `doc5`가 3등. RR = 1/3.\n",
    "- MRR@4 = (1 + 1/3) / 2 ≈ 0.667.\n",
    "\n",
    "`(3) mAP@k (Mean Average Precision at k)`\n",
    "- 정의: 각 쿼리에 대한 AP@k(Average Precision at k)를 구하고, 이들의 평균을 냄. AP@k는 상위 `k`개 결과 내에서 각 정답 문서가 나올 때마다의 Precision(정밀도) 값들의 평균.\n",
    "- 특징: 검색 결과의 순서와 정확도를 모두 고려. 여러 정답이 있는 경우 유용.\n",
    "- Precision@i: 상위 i개 결과 중 정답 문서의 비율.\n",
    "- AP@k 계산: 각 정답 문서 위치 `j` (j <= k)에서의 Precision@j 값을 모두 더한 후, 총 정답 문서 수 (또는 k개 내에 있는 정답 문서 수)로 나눔.\n",
    "- 예시 (k=4, Query 1): 예측 [`doc1`(정답), `doc3`], 정답 [`doc1`]. \n",
    "  - P@1 (`doc1`): 1/1 = 1. AP@4_Q1 = (1) / 1 = 1.\n",
    "- 예시 (k=4, Query 2): 예측 [`doc4`, `doc1`, `doc5`(정답), `doc2`(정답)], 정답 [`doc2`, `doc5`].\n",
    "  - P@1 (`doc4`): 0/1=0.\n",
    "  - P@2 (`doc1`): 0/2=0.\n",
    "  - P@3 (`doc5`): 1/3. (첫 정답)\n",
    "  - P@4 (`doc2`): 2/4. (두 번째 정답)\n",
    "  - AP@4_Q2 = (1/3 + 2/4) / 2 = (0.333 + 0.5) / 2 = 0.4165.\n",
    "- mAP@4 = (1 + 0.4165) / 2 ≈ 0.708.\n",
    "\n",
    "`(4) NDCG@k (Normalized Discounted Cumulative Gain at k)`\n",
    "- 정의: 검색 결과의 순서와 문서의 관련성 등급을 모두 고려하여 평가. 이상적인 검색 결과(IDCG) 대비 현재 검색 결과(DCG)의 비율.\n",
    "- 특징: 관련성이 높은 문서가 상위에 있을수록 높은 점수. 가장 정교한 지표 중 하나.\n",
    "- DCG@k = Σ ( (2^relevance_i - 1) / log2(i+1) )  (i는 순위 1부터 k까지, relevance_i는 i번째 문서의 관련도 점수)\n",
    "- IDCG@k: 이상적인 순서일 때의 DCG@k (정답 문서들을 관련도 순으로 정렬했을 때).\n",
    "- NDCG@k = DCG@k / IDCG@k.\n",
    "- (이진 관련성 가정: 정답이면 1, 아니면 0)\n",
    "- 예시 (k=4, Query 1): 예측 [`doc1`(rel=1), `doc3`(rel=0)], 정답 [`doc1`].\n",
    "  - DCG@4_Q1 = (2^1-1)/log2(1+1) + (2^0-1)/log2(2+1) = 1/1 + 0 = 1.\n",
    "  - IDCG@4_Q1 = (2^1-1)/log2(1+1) = 1.\n",
    "  - NDCG@4_Q1 = 1/1 = 1.\n",
    "- 예시 (k=4, Query 2): 예측 [`doc4`(0), `doc1`(0), `doc5`(1), `doc2`(1)], 정답 [`doc2`, `doc5`]. (이상적 순서는 `doc2`(1), `doc5`(1) 또는 그 반대)\n",
    "  - DCG@4_Q2 = 0/log2(2) + 0/log2(3) + 1/log2(4) + 1/log2(5) = 0 + 0 + 1/2 + 1/2.32 = 0.5 + 0.43 = 0.93.\n",
    "  - IDCG@4_Q2 = 1/log2(2) + 1/log2(3) = 1 + 1/1.58 = 1 + 0.63 = 1.63.\n",
    "  - NDCG@4_Q2 = 0.93 / 1.63 ≈ 0.57.\n",
    "- 평균 NDCG@4 = (1 + 0.57) / 2 = 0.785."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
