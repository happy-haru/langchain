{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9605ae4",
   "metadata": {},
   "source": [
    "## 1. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€: ì™œ í•´ì•¼ í•˜ëŠ”ê°€?\n",
    "\n",
    "- **'ê°'ì´ ì•„ë‹Œ 'ë°ì´í„°'ë¡œ ë§í•˜ê¸°:** êµ¬ì¶•í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œ(Retriever)ì´ ì–¼ë§ˆë‚˜ ì˜ ë™ì‘í•˜ëŠ”ì§€ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•¨. 'ì™ ì§€ ì˜ ë˜ëŠ” ê²ƒ ê°™ë‹¤'ëŠ” ëŠë‚Œì„ ë„˜ì–´, ì‹œìŠ¤í…œì˜ ê°•ì ê³¼ ì•½ì ì„ ê°ê´€ì ì¸ ìˆ«ìë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŒ.\n",
    "- **ê°œì„  ë°©í–¥ ì„¤ì •:** í‰ê°€ ì§€í‘œë¥¼ í†µí•´ ì–´ë–¤ ì¢…ë¥˜ì˜ ì§ˆë¬¸ì— ì·¨ì•½í•œì§€, ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•´ì•¼ í• ì§€ ë“± ê°œì„  ë°©í–¥ì„ ëª…í™•íˆ ì„¤ì •í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "### í‰ê°€ì˜ ì¥ë‹¨ì \n",
    "- **ì¥ì :**\n",
    "  - **ê°ê´€ì„± í™•ë³´:** ì—¬ëŸ¬ Retrieverë‚˜ ì„¤ì • ë³€ê²½ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ê³µì •í•˜ê²Œ ë¹„êµ ê°€ëŠ¥í•¨.\n",
    "  - **ë³‘ëª© í˜„ìƒ ì§„ë‹¨:** RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ì´ Retrieverì¸ì§€, LLMì˜ ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì¸ì§€ êµ¬ë¶„í•˜ëŠ” ë° ë„ì›€ì´ ë¨.\n",
    "  - **ì ì§„ì  ê°œì„ :** í‰ê°€-ê°œì„  ì‚¬ì´í´ì„ ë°˜ë³µí•˜ë©° ì‹œìŠ¤í…œì„ ì²´ê³„ì ìœ¼ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìŒ.\n",
    "- **ë‹¨ì :**\n",
    "  - **ì´ˆê¸° ë¹„ìš©:** ì–‘ì§ˆì˜ í‰ê°€ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ëŠ” ë° ì‹œê°„ê³¼ ë…¸ë ¥ì´ ì†Œìš”ë¨.\n",
    "  - **ì§€í‘œì˜ í•œê³„:** íŠ¹ì • ì§€í‘œê°€ ë†’ë‹¤ê³  í•´ì„œ ì‚¬ìš©ì ë§Œì¡±ë„ê°€ í•­ìƒ ë¹„ë¡€í•˜ëŠ” ê²ƒì€ ì•„ë‹˜. ì •ì„±ì  í‰ê°€ë¥¼ ë³‘í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e9f63d",
   "metadata": {},
   "source": [
    "### 1.1 í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "- ì¢‹ì€ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” ì–‘ì§ˆì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ í•„ìš”í•¨. \n",
    "- ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸-ë‹µë³€(QA) ìŒì„ í•©ì„±í•˜ê³ , ì´ë¥¼ ê²€í† /ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7605f7b",
   "metadata": {},
   "source": [
    "## 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„: GIGO(Garbage In, Garbage Out)\n",
    "\n",
    "- ì¢‹ì€ í‰ê°€ëŠ” ì¢‹ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì‹œì‘ë¨. ì“°ë ˆê¸° ê°™ì€ ë°ì´í„°ë¡œ í‰ê°€í•˜ë©´ ì“°ë ˆê¸° ê°™ì€ ê²°ë¡ ë§Œ ë‚˜ì˜´.\n",
    "- ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ **(1) ë°ì´í„° ì •ì œ â†’ (2) QA ìŒ í•©ì„± â†’ (3) QA ê²€ìˆ˜**ì˜ 3ë‹¨ê³„ë¡œ ê³ í’ˆì§ˆ í‰ê°€ì…‹ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì • ì œì‹œ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d71465",
   "metadata": {},
   "source": [
    "### 2.1. í‰ê°€ìš© ë¬¸ì„œ ë°ì´í„° ì •ì œ\n",
    "\n",
    "- `korean_docs`ë¥¼ í‰ê°€ìš©ìœ¼ë¡œ ì¢€ ë” ì„¸ë°€í•˜ê²Œ ê°€ê³µí•˜ëŠ” ë‹¨ê³„ì„.\n",
    "  - **ê³ ìœ  ID ë¶€ì—¬:** ê° ë¬¸ì„œ ì¡°ê°(ì²­í¬)ì— ê³ ìœ  `doc_id` ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ë‚˜ì¤‘ì— ì •ë‹µì„ ì •í™•íˆ ì‹ë³„í•  ìˆ˜ ìˆê²Œ í•¨.\n",
    "  - **ì»¨í…ìŠ¤íŠ¸ ëª…ì‹œ:** ë¬¸ì„œ ë‚´ìš© ëì— \"ì´ ë¬¸ì„œëŠ” 'Tesla'ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤.\"ì™€ ê°™ì´ ì¶œì²˜ë¥¼ ëª…ì‹œí•¨. ì´ëŠ” LLMì´ QAë¥¼ ìƒì„±í•  ë•Œ ì–´ë–¤ ê¸°ì—…ì— ëŒ€í•œ ë‚´ìš©ì¸ì§€ ëª…í™•íˆ ì¸ì§€í•˜ì—¬ ë” ì¢‹ì€ í’ˆì§ˆì˜ QAë¥¼ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì¥ì¹˜ì„.\n",
    "- JSONL (JSON Lines) í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì¬ì‚¬ìš©ì„±ì„ ë†’ì„.\n",
    "\n",
    "\n",
    "#### ì¥ì ê³¼ ë‹¨ì \n",
    "- **ì¥ì :** \n",
    "  - **í‰ê°€ì˜ ì‹ ë¢°ì„± í–¥ìƒ:** `doc_id`ë¡œ ì •ë‹µ ë¬¸ì„œë¥¼ ëª…í™•íˆ ì¶”ì í•  ìˆ˜ ìˆì–´ í‰ê°€ì˜ ì •í™•ë„ê°€ ì˜¬ë¼ê°.\n",
    "  - **QA í’ˆì§ˆ í–¥ìƒ:** LLMì— ëª…ì‹œì ì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì—¬, ë” ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ê³  ë§¥ë½ì— ë§ëŠ” QA ìŒì„ ìƒì„±í•˜ê²Œ í•¨.\n",
    "  - **ì¬í˜„ì„± í™•ë³´:** ì •ì œëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•´ë‘ë©´, ì–¸ì œë“  ë™ì¼í•œ ì¡°ê±´ìœ¼ë¡œ í‰ê°€ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆìŒ.\n",
    "- **ë‹¨ì :** \n",
    "  - **ìˆ˜ë™ ê³µìˆ˜ ë°œìƒ:** ì´ˆê¸° ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê³  ì •ì œ ê·œì¹™ì„ ë§Œë“œëŠ” ë° ë§ì€ ë…¸ë ¥ì´ ë“¬.\n",
    "  - **ì „ì²˜ë¦¬ ë³µì¡ë„ ì¦ê°€:** ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¡œë”©ë³´ë‹¤ íŒŒì´í”„ë¼ì¸ì´ ì•½ê°„ ë” ë³µì¡í•´ì§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc5a92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eee247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from glob import glob\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b1d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_files(txt_files):\n",
    "    data = []\n",
    "    for text_file in txt_files:\n",
    "        print(f\"ë¡œë”© ì¤‘: {text_file}\")\n",
    "        loader = TextLoader(text_file, encoding='utf-8') \n",
    "        data.extend(loader.load())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ad8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë”© ì¤‘: ../data\\Rivian_KR.txt\n",
      "ë¡œë”© ì¤‘: ../data\\Tesla_KR.txt\n",
      "\n",
      "ë¡œë“œëœ ì „ì²´ ë¬¸ì„œ ìˆ˜: 2\n",
      "ì²« ë²ˆì§¸ ë¬¸ì„œ ë‚´ìš© ì¼ë¶€: ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ì œì¡°ì—…ì²´ì´ì ìë™ì°¨ ê¸°ìˆ  íšŒì‚¬ì„.\n",
      "2009ë…„ì— ë¡œë²„íŠ¸ \"RJ\" ìŠ¤ìºë¦°ì§€(Robert \"RJ\" S...\n"
     ]
    }
   ],
   "source": [
    "korean_txt_files = glob(os.path.join('../data', '*_KR.txt'))\n",
    "if not korean_txt_files:\n",
    "    print(\"'data' í´ë”ì— '*_KR.txt' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì˜ˆì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    korean_data = [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”\n",
    "else:\n",
    "    korean_data = load_text_files(korean_txt_files)\n",
    "\n",
    "print(f\"\\në¡œë“œëœ ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(korean_data)}\")\n",
    "if korean_data:\n",
    "    print(f\"ì²« ë²ˆì§¸ ë¬¸ì„œ ë‚´ìš© ì¼ë¶€: {korean_data[0].page_content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165bc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "# ë¬¸ì¥ êµ¬ë¶„ì(ì •ê·œì‹)ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,          # í† í° ìˆ˜ ê³„ì‚° ê¸°ì¤€\n",
    "    separator=r\"[.!?]\\s+\",     # ë¬¸ì¥ êµ¬ë¶„ì: ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ ê³µë°±\n",
    "    chunk_size=100,             # ì²­í¬ ìµœëŒ€ í† í° ìˆ˜ (í† í¬ë‚˜ì´ì € ê¸°ì¤€)\n",
    "    chunk_overlap=20,            # ì²­í¬ ê°„ ì¤‘ë³µ í† í° ìˆ˜\n",
    "    is_separator_regex=True,    # separatorê°€ ì •ê·œì‹ì„ì„ ëª…ì‹œ\n",
    "    keep_separator=True,        # êµ¬ë¶„ì ìœ ì§€ ì—¬ë¶€\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092f4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë¶„í• ëœ í•œêµ­ì–´ ë¬¸ì„œ ìˆ˜: 5\n",
      "ì²« ë²ˆì§¸ ë¶„í•  ë¬¸ì„œ ë‚´ìš©: ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ì œì¡°ì—…ì²´ì´ì ìë™ì°¨ ê¸°ìˆ  íšŒì‚¬ì„.\n",
      "2009ë…„ì— ë¡œë²„íŠ¸ \"RJ\" ìŠ¤ìºë¦°ì§€(Robert \"RJ\" Scaringe)ì— ì˜í•´ ì„¤ë¦½ë˜ì—ˆìŒ. ë³¸ì‚¬ëŠ” ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ì–´ë°”ì¸ì— ìœ„ì¹˜í•´ ìˆìŒ.\n",
      "ë¦¬ë¹„ì•ˆì˜ ì£¼ë ¥ ì œí’ˆì€ R1T ì „ê¸° í”½ì—…íŠ¸ëŸ­ê³¼ R1S ì „ê¸° SUVì„\n",
      "ì²« ë²ˆì§¸ ë¶„í•  ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {'source': '../data\\\\Rivian_KR.txt'}\n"
     ]
    }
   ],
   "source": [
    "korean_docs = []\n",
    "if korean_data: # ë¡œë“œëœ ë°ì´í„°ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ë¶„í•  ìˆ˜í–‰\n",
    "    korean_docs = text_splitter.split_documents(korean_data)\n",
    "\n",
    "print(f\"\\në¶„í• ëœ í•œêµ­ì–´ ë¬¸ì„œ ìˆ˜: {len(korean_docs)}\")\n",
    "if korean_docs:\n",
    "    print(f\"ì²« ë²ˆì§¸ ë¶„í•  ë¬¸ì„œ ë‚´ìš©: {korean_docs[0].page_content}\")\n",
    "    print(f\"ì²« ë²ˆì§¸ ë¶„í•  ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {korean_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577a0156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í‰ê°€ìš©ìœ¼ë¡œ ì •ì œëœ ë¬¸ì„œ ìˆ˜: 5\n",
      "\n",
      "ì²« ë²ˆì§¸ ì •ì œ ë¬¸ì„œ ì˜ˆì‹œ:\n",
      "ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ì œì¡°ì—…ì²´ì´ì ìë™ì°¨ ê¸°ìˆ  íšŒì‚¬ì„.\n",
      "2009ë…„ì— ë¡œë²„íŠ¸ \"RJ\" ìŠ¤ìºë¦°ì§€(Robert \"RJ\" Scaringe)ì— ì˜í•´ ì„¤ë¦½ë˜ì—ˆìŒ. ë³¸ì‚¬ëŠ” ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ì–´ë°”ì¸ì— ìœ„ì¹˜í•´ ìˆìŒ.\n",
      "ë¦¬ë¹„ì•ˆì˜ ì£¼ë ¥ ì œí’ˆì€ R1T ì „ê¸° í”½ì—…íŠ¸ëŸ­ê³¼ R1S ì „ê¸° SUVì„\n",
      "\n",
      "(ì°¸ê³ : ì´ ë¬¸ì„œëŠ” 'Rivian'ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.)\n",
      "------------------------------\n",
      "{'source': '../data\\\\Rivian_KR.txt', 'doc_id': '0'}\n",
      "==============================\n",
      "\n",
      "í‰ê°€ìš© ë¬¸ì„œê°€ './data/final_docs_for_eval.jsonl' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_28828\\42143547.py:5: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  new_doc = doc.copy()\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "final_docs_for_eval = []\n",
    "if korean_docs: \n",
    "    for i, doc in enumerate(korean_docs):\n",
    "        new_doc = doc.copy() \n",
    "        new_doc.metadata['doc_id'] = f'{i}'\n",
    "        source_filename = os.path.basename(new_doc.metadata.get('source', ''))\n",
    "        corp_name = source_filename.split('_')[0] if '_' in source_filename else source_filename.replace('.txt', '')\n",
    "        new_doc.page_content = f\"{new_doc.page_content}\\n\\n(ì°¸ê³ : ì´ ë¬¸ì„œëŠ” '{corp_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.)\"\n",
    "        final_docs_for_eval.append(new_doc)\n",
    "\n",
    "    print(f\"í‰ê°€ìš©ìœ¼ë¡œ ì •ì œëœ ë¬¸ì„œ ìˆ˜: {len(final_docs_for_eval)}\")\n",
    "    if final_docs_for_eval:\n",
    "        print(\"\\nì²« ë²ˆì§¸ ì •ì œ ë¬¸ì„œ ì˜ˆì‹œ:\")\n",
    "        print(final_docs_for_eval[0].page_content)\n",
    "        print(\"-\" * 30)\n",
    "        print(final_docs_for_eval[0].metadata)\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "    with open('../data/final_docs_for_eval.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for doc_item in final_docs_for_eval:\n",
    "            f.write(json.dumps({'page_content': doc_item.page_content, 'metadata': doc_item.metadata}) + '\\n')\n",
    "    print(\"\\ní‰ê°€ìš© ë¬¸ì„œê°€ './data/final_docs_for_eval.jsonl' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "else:\n",
    "    print(\"korean_docsê°€ ì—†ì–´ í‰ê°€ìš© ë¬¸ì„œë¥¼ ì •ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78e254",
   "metadata": {},
   "source": [
    "`(2) Question-Answer (QA) í•©ì„±`\n",
    "\n",
    "- ì •ì œëœ ë¬¸ì„œ(`final_docs_for_eval`)ì˜ ê° ì²­í¬(ë¬¸ì„œ)ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í•˜ì—¬, LLM(ì—¬ê¸°ì„œëŠ” OpenAI GPT ëª¨ë¸)ì„ ì‚¬ìš©í•´ ì§ˆë¬¸ê³¼ ë‹µë³€ ìŒì„ ìƒì„±í•¨.\n",
    "- **`PydanticOutputParser` í™œìš©:** LLMì˜ ì¶œë ¥ì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” `QAPair`, `QASet` ê°™ì€ êµ¬ì¡°í™”ëœ ê°ì²´ í˜•íƒœë¡œ ë‚˜ì˜¤ë„ë¡ ê°•ì œí•¨. ì´ë ‡ê²Œ í•˜ë©´ í›„ì† ì²˜ë¦¬ê°€ ë§¤ìš° ê¹”ë”í•´ì§.\n",
    "- **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§:** 'ê²€ìƒ‰ ì—”ì§„ ì‚¬ìš©ìì²˜ëŸ¼ ì§ˆë¬¸í•´ì¤˜', 'ë‹µë³€ì— ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í¬í•¨í•´ì¤˜' ì™€ ê°™ì´ êµ¬ì²´ì ì¸ ì§€ì‹œë¥¼ í†µí•´ ì›í•˜ëŠ” ìŠ¤íƒ€ì¼ê³¼ í’ˆì§ˆì˜ QAë¥¼ ì–»ì–´ë‚´ëŠ” ê²ƒì´ í•µì‹¬ì„.\n",
    "\n",
    "#### ì¥ì ê³¼ ë‹¨ì \n",
    "- **ì¥ì :**\n",
    "  - **ì••ë„ì ì¸ ì†ë„ì™€ íš¨ìœ¨ì„±:** ì‚¬ëŒì´ ì§ì ‘ ìˆ˜ë°±, ìˆ˜ì²œ ê°œì˜ QAë¥¼ ë§Œë“œëŠ” ê²ƒì— ë¹„í•´ í›¨ì”¬ ë¹ ë¥´ê³  ì €ë ´í•˜ê²Œ ëŒ€ê·œëª¨ í‰ê°€ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŒ.\n",
    "  - **ë‹¤ì–‘ì„± í™•ë³´:** LLMì€ ì‚¬ëŒì´ ìƒê°í•˜ì§€ ëª»í•œ ë‹¤ì–‘í•œ ê´€ì ì˜ ì§ˆë¬¸ì„ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆìŒ.\n",
    "- **ë‹¨ì :**\n",
    "  - **API ë¹„ìš© ë°œìƒ:** LLM APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ë¹„ìš©ì´ ë°œìƒí•¨. (íŠ¹íˆ ë¬¸ì„œê°€ ë§ì„ ê²½ìš°)\n",
    "  - **í’ˆì§ˆ ë¶ˆí™•ì‹¤ì„± (í™˜ê° í˜„ìƒ):** LLMì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ê±°ë‚˜(Hallucination), ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì˜ëª» íŒŒì•…í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ. **ë”°ë¼ì„œ í›„ì† ê²€ìˆ˜ ê³¼ì •ì´ í•„ìˆ˜ì ì„.**\n",
    "  - **í”„ë¡¬í”„íŠ¸ ë¯¼ê°ë„:** í”„ë¡¬í”„íŠ¸ì˜ ë¯¸ì„¸í•œ ì°¨ì´ì—ë„ ê²°ê³¼ë¬¼ì˜ í’ˆì§ˆì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´, ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” ë° ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "#### ğŸ”‘ **ê°œì¸ì  ìƒê°**\n",
    "> ì²˜ìŒë¶€í„° ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ QA ìƒì„±ì„ ëŒë¦¬ì§€ ë§ ê²ƒ. 5~10ê°œ ì •ë„ì˜ ì†Œìˆ˜ ë¬¸ì„œë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸í•˜ë©° í”„ë¡¬í”„íŠ¸ì™€ LLM ëª¨ë¸(e.g., gpt-4o-mini vs gpt-4o)ì„ íŠœë‹í•˜ëŠ” ê²ƒì´ ë¹„ìš©ê³¼ ì‹œê°„ì„ ì•„ë¼ëŠ” ê¸¸ì´ë¼ê³  ìƒê°. `gpt-4o-mini`ëŠ” ê°€ì„±ë¹„ê°€ ì¢‹ì€ ì„ íƒì§€ê°€ ë  ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9ec3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA ìƒì„± í…ŒìŠ¤íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸:\n",
      "ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ì œì¡°ì—…ì²´ì´ì ìë™ì°¨ ê¸°ìˆ  íšŒì‚¬ì„.\n",
      "2009ë…„ì— ë¡œë²„íŠ¸ \"RJ\" ìŠ¤ìºë¦°ì§€(Robert \"RJ\" Scaringe)ì— ì˜í•´ ì„¤ë¦½ë˜ì—ˆìŒ. ë³¸ì‚¬ëŠ” ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ì–´ë°”ì¸ì— ìœ„ì¹˜í•´ ìˆìŒ.\n",
      "ë¦¬ë¹„ì•ˆì˜ ì£¼ë ¥ ì œí’ˆì€ R1T ì „ê¸° í”½ì—…íŠ¸ëŸ­ê³¼ R1S ì „ê¸° SUVì„\n",
      "\n",
      "(ì°¸ê³ : ì´ ë¬¸ì„œëŠ” 'Rivian'ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.)\n",
      "\n",
      "ìƒì„±ëœ QA ìŒ ì˜ˆì‹œ:\n",
      "  ì§ˆë¬¸: ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸ŒëŠ” ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?\n",
      "  ë‹µë³€: ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸ŒëŠ” 2009ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "# Pydantic ëª¨ë¸: LLMì˜ ì¶œë ¥ êµ¬ì¡°ë¥¼ ì •ì˜í•¨\n",
    "# QAPair: ê°œë³„ ì§ˆë¬¸-ë‹µë³€ ìŒ\n",
    "class QAPair(BaseModel):\n",
    "    question: str = Field(description=\"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ, ì‚¬ì‹¤ì— ì…ê°í•œ ì§ˆë¬¸ (í•œêµ­ì–´)\")\n",
    "    answer: str = Field(description=\"ìƒì„±ëœ ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ (í•œêµ­ì–´, ì»¨í…ìŠ¤íŠ¸ ë‚´ì˜ ì •ë³´ë§Œ ì‚¬ìš©)\")\n",
    "\n",
    "# QASet: QAPairì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹´ëŠ” ì»¨í…Œì´ë„ˆ. LLMì´ ì—¬ëŸ¬ ê°œì˜ QAë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©\n",
    "class QASet(BaseModel):\n",
    "    qa_pairs: List[QAPair] = Field(description=\"ì§ˆë¬¸-ë‹µë³€(QAPair) ìŒì˜ ë¦¬ìŠ¤íŠ¸\")\n",
    "\n",
    "# QA ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "QA_GENERATION_TEMPLATE_KOREAN = \"\"\"\n",
    "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì‹¤ì— ì…ê°í•œ ì§ˆë¬¸-ë‹µë³€ ìŒì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤.\n",
    "ì˜¤ì§ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ë‚´ì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬, íŠ¹ì •í•˜ê³  ê°„ê²°í•œ ì‚¬ì‹¤ë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ {num_questions_per_chunk}ê°œ ë§Œë“œì„¸ìš”.\n",
    "\n",
    "## ì§€ì¹¨:\n",
    "1. ì§ˆë¬¸ì€ ì‹¤ì œ ì‚¬ìš©ìê°€ ê²€ìƒ‰ ì—”ì§„ì— ì…ë ¥í•  ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "2. \"ì œì‹œëœ êµ¬ì ˆì— ë”°ë¥´ë©´\", \"ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥´ë©´\" ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "3. ë‹µë³€ì€ ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬, ì™„ì „í•œ ë¬¸ì¥ í˜•íƒœë¡œ ëª…í™•í•˜ê²Œ ì œê³µí•´ì£¼ì„¸ìš”.\n",
    "4. ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì§ˆë¬¸ì„ ë§Œë“¤ê¸° ì–´ë µë‹¤ë©´, ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "---------------------------------------------------------\n",
    "## ì¶œë ¥ í˜•ì‹:\n",
    "{format_instructions}\n",
    "---------------------------------------------------------\n",
    "## ì»¨í…ìŠ¤íŠ¸:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# QA ìƒì„±ì— ì‚¬ìš©í•  LLM ì •ì˜ (gpt-4o-miniëŠ” ë¹„ìš© íš¨ìœ¨ì ì„)\n",
    "qa_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.1, # ì‚¬ì‹¤ ê¸°ë°˜ ìƒì„±ì´ë¯€ë¡œ ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •\n",
    ")\n",
    "\n",
    "# PydanticOutputParser: LLM ì¶œë ¥ì„ QASet ê°ì²´ë¡œ íŒŒì‹±\n",
    "pydantic_parser_qa = PydanticOutputParser(pydantic_object=QASet)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "qa_generation_prompt = ChatPromptTemplate.from_template(\n",
    "    template=QA_GENERATION_TEMPLATE_KOREAN,\n",
    "    partial_variables={\"format_instructions\": pydantic_parser_qa.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# ì²´ì¸(Chain) êµ¬ì„±: Prompt -> LLM -> Parser\n",
    "qa_generation_chain = qa_generation_prompt | qa_llm | pydantic_parser_qa\n",
    "\n",
    "# QA ìƒì„± í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ì •ì œ ë¬¸ì„œ ì‚¬ìš©)\n",
    "if final_docs_for_eval:\n",
    "    test_context_for_qa = final_docs_for_eval[0].page_content\n",
    "    print(f\"QA ìƒì„± í…ŒìŠ¤íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸:\\n{test_context_for_qa}\\n\")\n",
    "    \n",
    "    try:\n",
    "        qa_set_example = qa_generation_chain.invoke({\n",
    "            \"context\": test_context_for_qa,\n",
    "            \"num_questions_per_chunk\": 1 # í…ŒìŠ¤íŠ¸ë¡œ 1ê°œë§Œ ìƒì„±\n",
    "        })\n",
    "        print(\"ìƒì„±ëœ QA ìŒ ì˜ˆì‹œ:\")\n",
    "        for qa_pair in qa_set_example.qa_pairs:\n",
    "            print(f\"  ì§ˆë¬¸: {qa_pair.question}\")\n",
    "            print(f\"  ë‹µë³€: {qa_pair.answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"ì •ì œëœ í‰ê°€ìš© ë¬¸ì„œê°€ ì—†ì–´ QA ìƒì„± í…ŒìŠ¤íŠ¸ë¥¼ ìŠ¤í‚µí•¨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08953e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ê°œì˜ ë¬¸ì„œì— ëŒ€í•´ QA ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "1/5ë²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì„±ê³µ\n",
      "2/5ë²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì„±ê³µ\n",
      "3/5ë²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì„±ê³µ\n",
      "4/5ë²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì„±ê³µ\n",
      "5/5ë²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ì„±ê³µ\n",
      "\n",
      "ì´ 10ê°œì˜ QA ìŒì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ í‰ê°€ìš© ë¬¸ì„œì— ëŒ€í•´ QA ìƒì„±\n",
    "NUM_QUESTIONS_PER_CHUNK = 2 # ê° ë¬¸ì„œ ì²­í¬ë‹¹ ìƒì„±í•  QA ê°œìˆ˜\n",
    "generated_qa_outputs = []\n",
    "\n",
    "if final_docs_for_eval :\n",
    "    print(f\"{len(final_docs_for_eval)}ê°œì˜ ë¬¸ì„œì— ëŒ€í•´ QA ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    for i, doc_item in enumerate(final_docs_for_eval):\n",
    "        print(f\"{i+1}/{len(final_docs_for_eval)}ë²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...\", end=' ')\n",
    "        try:\n",
    "            qa_set_generated = qa_generation_chain.invoke({\n",
    "                \"context\": doc_item.page_content,\n",
    "                \"num_questions_per_chunk\": NUM_QUESTIONS_PER_CHUNK\n",
    "            })\n",
    "            for qa_pair in qa_set_generated.qa_pairs:\n",
    "                generated_qa_outputs.append({\n",
    "                    'context': [doc_item.page_content], # ì •ë‹µ ì»¨í…ìŠ¤íŠ¸ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)\n",
    "                    'source': [doc_item.metadata.get('source', '')], # ì¶œì²˜ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)\n",
    "                    'doc_id': [doc_item.metadata.get('doc_id', '')], # ë¬¸ì„œ ID (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)\n",
    "                    'question': qa_pair.question,\n",
    "                    'answer': qa_pair.answer\n",
    "                })\n",
    "            print(\"ì„±ê³µ\")\n",
    "        except Exception as e:\n",
    "            print(f\"ì‹¤íŒ¨: {e}\")\n",
    "            continue # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë¬¸ì„œë¡œ\n",
    "    \n",
    "    df_generated_qa_test = pd.DataFrame(generated_qa_outputs)\n",
    "    print(f\"\\nì´ {df_generated_qa_test.shape[0]}ê°œì˜ QA ìŒì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    df_generated_qa_test.to_excel(\"../data/generated_qa_test.xlsx\", index=False)\n",
    "else:\n",
    "    print(\"QA í•©ì„±ì€ ì‹¤ì œ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (final_docs_for_evalì´ ì—†ê±°ë‚˜, ì‹¤í–‰ í”Œë˜ê·¸ê°€ Falseì„)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8bcaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™...</td>\n",
       "      <td>[../data\\Rivian_KR.txt]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸ŒëŠ” ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸ŒëŠ” 2009ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™...</td>\n",
       "      <td>[../data\\Rivian_KR.txt]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆì˜ ë³¸ì‚¬ëŠ” ì–´ë””ì— ìœ„ì¹˜í•˜ê³  ìˆë‚˜ìš”?</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆì˜ ë³¸ì‚¬ëŠ” ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ì–´ë°”ì¸ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[. ì´ë“¤ ì°¨ëŸ‰ì€ \"ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ\" í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì˜¤í”„ë¡œë“œ ì„±ëŠ¥ê³¼ ì¥ê±°ë¦¬ ...</td>\n",
       "      <td>[../data\\Rivian_KR.txt]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆì€ ì–´ë–¤ í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì°¨ëŸ‰ì„ ì œì‘í•˜ë‚˜ìš”?</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆì€ 'ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ' í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì°¨ëŸ‰ì„ ì œì‘í•©ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[. ì´ë“¤ ì°¨ëŸ‰ì€ \"ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ\" í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì˜¤í”„ë¡œë“œ ì„±ëŠ¥ê³¼ ì¥ê±°ë¦¬ ...</td>\n",
       "      <td>[../data\\Rivian_KR.txt]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆì€ ì–´ë–¤ ê¸°ì—…ìœ¼ë¡œë¶€í„° íˆ¬ìë¥¼ ë°›ì•˜ë‚˜ìš”?</td>\n",
       "      <td>ë¦¬ë¹„ì•ˆì€ ì•„ë§ˆì¡´ê³¼ í¬ë“œ ë“± ì£¼ìš” ê¸°ì—…ìœ¼ë¡œë¶€í„° íˆ¬ìë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[í…ŒìŠ¬ë¼(Tesla, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì„.\\n2...</td>\n",
       "      <td>[../data\\Tesla_KR.txt]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>í…ŒìŠ¬ë¼ëŠ” ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?</td>\n",
       "      <td>í…ŒìŠ¬ë¼ëŠ” 2003ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[í…ŒìŠ¬ë¼(Tesla, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì„.\\n2...</td>\n",
       "      <td>[../data\\Tesla_KR.txt]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?</td>\n",
       "      <td>í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[. ë³¸ì‚¬ëŠ” í…ì‚¬ìŠ¤ì£¼ ì˜¤ìŠ¤í‹´ì— ìˆìŒ.\\ní…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ë¡œëŠ” ëª¨ë¸ S ...</td>\n",
       "      <td>[../data\\Tesla_KR.txt]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>í…ŒìŠ¬ë¼ì˜ ë³¸ì‚¬ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?</td>\n",
       "      <td>í…ŒìŠ¬ë¼ì˜ ë³¸ì‚¬ëŠ” í…ì‚¬ìŠ¤ì£¼ ì˜¤ìŠ¤í‹´ì— ìˆìŠµë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[. ë³¸ì‚¬ëŠ” í…ì‚¬ìŠ¤ì£¼ ì˜¤ìŠ¤í‹´ì— ìˆìŒ.\\ní…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ë¡œëŠ” ëª¨ë¸ S ...</td>\n",
       "      <td>[../data\\Tesla_KR.txt]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>í…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?</td>\n",
       "      <td>í…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ë¡œëŠ” ëª¨ë¸ S, ëª¨ë¸ 3, ëª¨ë¸ X, ëª¨ë¸ Y, ê·¸ë¦¬ê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[.\\në˜í•œ, í…ŒìŠ¬ë¼ëŠ” íƒœì–‘ê´‘ íŒ¨ë„, ê°€ì •ìš© ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œì¸ íŒŒì›Œì›”(Power...</td>\n",
       "      <td>[../data\\Tesla_KR.txt]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>í…ŒìŠ¬ë¼ëŠ” ì–´ë–¤ ì—ë„ˆì§€ ê´€ë ¨ ì œí’ˆì„ ìƒì‚°í•˜ë‚˜ìš”?</td>\n",
       "      <td>í…ŒìŠ¬ë¼ëŠ” íƒœì–‘ê´‘ íŒ¨ë„, ê°€ì •ìš© ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œì¸ íŒŒì›Œì›”(Powerwall), ëŒ€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[.\\në˜í•œ, í…ŒìŠ¬ë¼ëŠ” íƒœì–‘ê´‘ íŒ¨ë„, ê°€ì •ìš© ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œì¸ íŒŒì›Œì›”(Power...</td>\n",
       "      <td>[../data\\Tesla_KR.txt]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>í…ŒìŠ¬ë¼ëŠ” ì–´ë–¤ ê¸°ìˆ  ê°œë°œì— ë§ì€ íˆ¬ìë¥¼ í•˜ê³  ìˆë‚˜ìš”?</td>\n",
       "      <td>í…ŒìŠ¬ë¼ëŠ” ììœ¨ ì£¼í–‰ ê¸°ìˆ  ê°œë°œì— ë§ì€ íˆ¬ìë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context                   source  \\\n",
       "0  [ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™...  [../data\\Rivian_KR.txt]   \n",
       "1  [ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸Œ(Rivian Automotive, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™...  [../data\\Rivian_KR.txt]   \n",
       "2  [. ì´ë“¤ ì°¨ëŸ‰ì€ \"ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ\" í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì˜¤í”„ë¡œë“œ ì„±ëŠ¥ê³¼ ì¥ê±°ë¦¬ ...  [../data\\Rivian_KR.txt]   \n",
       "3  [. ì´ë“¤ ì°¨ëŸ‰ì€ \"ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ\" í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì˜¤í”„ë¡œë“œ ì„±ëŠ¥ê³¼ ì¥ê±°ë¦¬ ...  [../data\\Rivian_KR.txt]   \n",
       "4  [í…ŒìŠ¬ë¼(Tesla, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì„.\\n2...   [../data\\Tesla_KR.txt]   \n",
       "5  [í…ŒìŠ¬ë¼(Tesla, Inc.)ëŠ” ë¯¸êµ­ì˜ ì „ê¸° ìë™ì°¨ ë° ì²­ì • ì—ë„ˆì§€ íšŒì‚¬ì„.\\n2...   [../data\\Tesla_KR.txt]   \n",
       "6  [. ë³¸ì‚¬ëŠ” í…ì‚¬ìŠ¤ì£¼ ì˜¤ìŠ¤í‹´ì— ìˆìŒ.\\ní…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ë¡œëŠ” ëª¨ë¸ S ...   [../data\\Tesla_KR.txt]   \n",
       "7  [. ë³¸ì‚¬ëŠ” í…ì‚¬ìŠ¤ì£¼ ì˜¤ìŠ¤í‹´ì— ìˆìŒ.\\ní…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ë¡œëŠ” ëª¨ë¸ S ...   [../data\\Tesla_KR.txt]   \n",
       "8  [.\\në˜í•œ, í…ŒìŠ¬ë¼ëŠ” íƒœì–‘ê´‘ íŒ¨ë„, ê°€ì •ìš© ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œì¸ íŒŒì›Œì›”(Power...   [../data\\Tesla_KR.txt]   \n",
       "9  [.\\në˜í•œ, í…ŒìŠ¬ë¼ëŠ” íƒœì–‘ê´‘ íŒ¨ë„, ê°€ì •ìš© ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œì¸ íŒŒì›Œì›”(Power...   [../data\\Tesla_KR.txt]   \n",
       "\n",
       "  doc_id                       question  \\\n",
       "0    [0]          ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸ŒëŠ” ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?   \n",
       "1    [0]         ë¦¬ë¹„ì•ˆì˜ ë³¸ì‚¬ëŠ” ì–´ë””ì— ìœ„ì¹˜í•˜ê³  ìˆë‚˜ìš”?   \n",
       "2    [1]   ë¦¬ë¹„ì•ˆì€ ì–´ë–¤ í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì°¨ëŸ‰ì„ ì œì‘í•˜ë‚˜ìš”?   \n",
       "3    [1]       ë¦¬ë¹„ì•ˆì€ ì–´ë–¤ ê¸°ì—…ìœ¼ë¡œë¶€í„° íˆ¬ìë¥¼ ë°›ì•˜ë‚˜ìš”?   \n",
       "4    [2]                í…ŒìŠ¬ë¼ëŠ” ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?   \n",
       "5    [2]               í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬ì¸ê°€ìš”?   \n",
       "6    [3]              í…ŒìŠ¬ë¼ì˜ ë³¸ì‚¬ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?   \n",
       "7    [3]     í…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?   \n",
       "8    [4]      í…ŒìŠ¬ë¼ëŠ” ì–´ë–¤ ì—ë„ˆì§€ ê´€ë ¨ ì œí’ˆì„ ìƒì‚°í•˜ë‚˜ìš”?   \n",
       "9    [4]  í…ŒìŠ¬ë¼ëŠ” ì–´ë–¤ ê¸°ìˆ  ê°œë°œì— ë§ì€ íˆ¬ìë¥¼ í•˜ê³  ìˆë‚˜ìš”?   \n",
       "\n",
       "                                              answer  \n",
       "0                         ë¦¬ë¹„ì•ˆ ì˜¤í† ëª¨í‹°ë¸ŒëŠ” 2009ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
       "1                     ë¦¬ë¹„ì•ˆì˜ ë³¸ì‚¬ëŠ” ìº˜ë¦¬í¬ë‹ˆì•„ì£¼ ì–´ë°”ì¸ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.  \n",
       "2                 ë¦¬ë¹„ì•ˆì€ 'ìŠ¤ì¼€ì´íŠ¸ë³´ë“œ' í”Œë«í¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì°¨ëŸ‰ì„ ì œì‘í•©ë‹ˆë‹¤.  \n",
       "3                ë¦¬ë¹„ì•ˆì€ ì•„ë§ˆì¡´ê³¼ í¬ë“œ ë“± ì£¼ìš” ê¸°ì—…ìœ¼ë¡œë¶€í„° íˆ¬ìë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.  \n",
       "4                               í…ŒìŠ¬ë¼ëŠ” 2003ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
       "5                               í…ŒìŠ¬ë¼ì˜ CEOëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì…ë‹ˆë‹¤.  \n",
       "6                           í…ŒìŠ¬ë¼ì˜ ë³¸ì‚¬ëŠ” í…ì‚¬ìŠ¤ì£¼ ì˜¤ìŠ¤í‹´ì— ìˆìŠµë‹ˆë‹¤.  \n",
       "7  í…ŒìŠ¬ë¼ì˜ ëŒ€í‘œì ì¸ ì „ê¸°ì°¨ ëª¨ë¸ë¡œëŠ” ëª¨ë¸ S, ëª¨ë¸ 3, ëª¨ë¸ X, ëª¨ë¸ Y, ê·¸ë¦¬ê³ ...  \n",
       "8  í…ŒìŠ¬ë¼ëŠ” íƒœì–‘ê´‘ íŒ¨ë„, ê°€ì •ìš© ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œì¸ íŒŒì›Œì›”(Powerwall), ëŒ€...  \n",
       "9                  í…ŒìŠ¬ë¼ëŠ” ììœ¨ ì£¼í–‰ ê¸°ìˆ  ê°œë°œì— ë§ì€ íˆ¬ìë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generated_qa_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e79bc",
   "metadata": {},
   "source": [
    "### 2.3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²€í†  ë° ìˆ˜ì •\n",
    "\n",
    "- **ê°€ì¥ ì¤‘ìš”í•˜ì§€ë§Œ ê°€ì¥ ê°„ê³¼í•˜ê¸° ì‰¬ìš´ ë‹¨ê³„.** LLMì´ ìƒì„±í•œ QA ë°ì´í„°ëŠ” ì™„ë²½í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì‚¬ëŒì´ ì§ì ‘ ê²€í† í•˜ê³  ìˆ˜ì •í•˜ëŠ” ê³¼ì •ì´ ë°˜ë“œì‹œ í•„ìš”í•¨.\n",
    "- Excel ë“±ìœ¼ë¡œ ì €ì¥ëœ `generated_qa_test.xlsx` íŒŒì¼ì„ ì—´ì–´ ì•„ë˜ ì‚¬í•­ë“¤ì„ ì ê²€í•˜ê³  ìˆ˜ì •í•¨:\n",
    "  - ì§ˆë¬¸ì´ ì–´ìƒ‰í•˜ê±°ë‚˜ ëª¨í˜¸í•˜ì§€ ì•Šì€ê°€?\n",
    "  - ë‹µë³€ì´ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê²Œ ëŒ€ë‹µí•˜ê³  ìˆëŠ”ê°€?\n",
    "  - ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸(ground_truth_context)ì— ì‹¤ì œë¡œ ìˆëŠ” ë‚´ìš©ì¸ê°€? (í™˜ê°ì€ ì•„ë‹Œê°€?)\n",
    "  - ë” ì¢‹ì€ ì§ˆë¬¸ì´ë‚˜ ë‹µë³€ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "\n",
    "\n",
    "#### ì¥ì ê³¼ ë‹¨ì \n",
    "- **ì¥ì :**\n",
    "  - **í‰ê°€ ì‹ ë¢°ë„ ê·¹ëŒ€í™”:** ì˜ëª»ëœ ë°ì´í„°ë¡œ í‰ê°€í•˜ì—¬ ì˜ëª»ëœ ê²°ë¡ ì— ë„ë‹¬í•˜ëŠ” ê²ƒì„ ë°©ì§€í•¨. ì´ ê³¼ì •ì„ ê±°ì¹œ ë°ì´í„°ì…‹ì€ ì‹œìŠ¤í…œì˜ ì‹¤ì œ ì„±ëŠ¥ì„ ê°€ì¥ ì˜ ë°˜ì˜í•¨.\n",
    "  - **'ì–´ë ¤ìš´' ì§ˆë¬¸ ë°œêµ´:** ì‹œìŠ¤í…œì´ ì˜ ë‹µë³€í•˜ì§€ ëª»í•˜ëŠ” ê¹Œë‹¤ë¡œìš´ ì§ˆë¬¸(Hard negatives)ë“¤ì„ ë°œê²¬í•˜ê³  í‰ê°€ì…‹ì— ì¶”ê°€í•˜ì—¬ ê²¬ê³ ì„±ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŒ.\n",
    "- **ë‹¨ì :**\n",
    "  - **ëª…ë°±í•œ ë‹¨ì : ì‹œê°„ê³¼ ì¸ë ¥:** ë°ì´í„°ì˜ ì–‘ì´ ë§ì„ìˆ˜ë¡ ë§ì€ ì‹œê°„ê³¼ ë…¸ë ¥ì´ í•„ìš”í•œ ìˆ˜ë™ ì‘ì—…ì„. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815d28a",
   "metadata": {},
   "source": [
    "## 3. ì •ë³´ ê²€ìƒ‰(IR) í‰ê°€ì§€í‘œ\n",
    "\n",
    "- ì´ì œ ì˜ ì¤€ë¹„ëœ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ Retrieverì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•  ì°¨ë¡€ì„.\n",
    "- ì •ë³´ ê²€ìƒ‰(IR) ë¶„ì•¼ì—ì„œ ì˜¤ë«ë™ì•ˆ ì‚¬ìš©ë˜ì–´ ì˜¨ í‘œì¤€ ì§€í‘œë“¤ì„ ì‚¬ìš©í•˜ì—¬, \"Retrieverê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸(query)ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë¥¼, ì–¼ë§ˆë‚˜ ë†’ì€ ìˆœìœ„ë¡œ ê°€ì ¸ì˜¤ëŠ”ì§€\"ë¥¼ í‰ê°€í•¨.\n",
    "- ì£¼ìš” ì§€í‘œ: **Hit Rate, MRR, mAP, NDCG** ë“±. ê° ì§€í‘œëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ë¯€ë¡œ, ì—¬ëŸ¬ ì§€í‘œë¥¼ í•¨ê»˜ ë³´ëŠ” ê²ƒì´ ë°”ëŒì§í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07689e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒ˜í”Œ ì •ë‹µ ë¬¸ì„œ ë° ì˜ˆì¸¡ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "\n",
    "# ê° ì¿¼ë¦¬ì— ëŒ€í•œ ì •ë‹µ ë¬¸ì„œ\n",
    "actual_docs_sample = [\n",
    "    [Document(metadata={'doc_id': 'doc1'}, page_content='ë‚´ìš©1')],\n",
    "    [Document(metadata={'doc_id': 'doc2'}, page_content='ë‚´ìš©2'),\n",
    "     Document(metadata={'doc_id': 'doc5'}, page_content='ë‚´ìš©5')]\n",
    "]\n",
    "\n",
    "# ê° ì¿¼ë¦¬ì— ëŒ€í•œ ê²€ìƒ‰ê¸°ê°€ ì˜ˆì¸¡í•œ ë¬¸ì„œ\n",
    "predicted_docs_sample = [\n",
    "    [Document(metadata={'doc_id': 'doc1'}, page_content='ë‚´ìš©1'),\n",
    "     Document(metadata={'doc_id': 'doc3'}, page_content='ë‚´ìš©3')],\n",
    "    [Document(metadata={'doc_id': 'doc4'}, page_content='ë‚´ìš©4'),\n",
    "     Document(metadata={'doc_id': 'doc1'}, page_content='ë‚´ìš©1'),\n",
    "     Document(metadata={'doc_id': 'doc5'}, page_content='ë‚´ìš©5'),\n",
    "     Document(metadata={'doc_id': 'doc2'}, page_content='ë‚´ìš©2')]\n",
    "]\n",
    "\n",
    "print(\"ìƒ˜í”Œ ì •ë‹µ ë¬¸ì„œ ë° ì˜ˆì¸¡ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b07cd",
   "metadata": {},
   "source": [
    "### 3.1. Hit Rate @k (ì ì¤‘ë¥ )\n",
    "\n",
    "- **ê°œë…:** \"ìƒìœ„ kê°œ ê²°ê³¼ ì•ˆì— ì •ë‹µì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ê°€?\" (Yes or No)\n",
    "  - ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ëœ ìƒìœ„ `k`ê°œì˜ ë¬¸ì„œ ì¤‘ì— ì‹¤ì œ ì •ë‹µ ë¬¸ì„œê°€ **í•˜ë‚˜ë¼ë„ í¬í•¨**ë˜ì–´ ìˆìœ¼ë©´ 1ì  (Hit), ì•„ë‹ˆë©´ 0ì .\n",
    "  - ì „ì²´ ì¿¼ë¦¬ì— ëŒ€í•œ ì ìˆ˜ì˜ í‰ê· ì„ ëƒ„.\n",
    "- **ì¥ì :** ê°€ì¥ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì„. Retrieverê°€ ìµœì†Œí•œì˜ ì—­í• ì„ í•˜ëŠ”ì§€ ë¹ ë¥´ê²Œ íŒŒì•… ê°€ëŠ¥.\n",
    "- **ë‹¨ì :** ì •ë‹µ ë¬¸ì„œì˜ ìˆœìœ„(1ë“±ì¸ì§€ kë“±ì¸ì§€)ë‚˜ ì—¬ëŸ¬ ì •ë‹µ ì¤‘ ëª‡ ê°œë¥¼ ë§ì·„ëŠ”ì§€ëŠ” ì „í˜€ ê³ ë ¤í•˜ì§€ ì•ŠìŒ.\n",
    "- **ì–¸ì œ ì‚¬ìš©í• ê¹Œ?** \"ì¼ë‹¨ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê¸°ë§Œ í•˜ë©´ ëœë‹¤\"ëŠ” ìµœì†Œí•œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c771fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate @4: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def hit_rate_at_k(actual_doc_ids_list, predicted_docs_list, k=4):\n",
    "    hits = 0\n",
    "    for actual_docs, predicted in zip(actual_doc_ids_list, predicted_docs_list):\n",
    "        predicted_ids_at_k = {p.metadata['doc_id'] for p in predicted[:k]}\n",
    "        actual_ids = {d.metadata['doc_id'] for d in actual_docs}\n",
    "        if any(doc_id in predicted_ids_at_k for doc_id in actual_ids):\n",
    "            hits += 1\n",
    "    return hits / len(actual_doc_ids_list)\n",
    "\n",
    "\n",
    "hr_score = hit_rate_at_k(actual_docs_sample, predicted_docs_sample, k=4)\n",
    "print(f\"Hit Rate @4: {hr_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5da269",
   "metadata": {},
   "source": [
    "### 3.2. MRR @k (Mean Reciprocal Rank)\n",
    "\n",
    "- **ê°œë…:** \"ì²« ì •ë‹µì„ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬(ë†’ì€ ìˆœìœ„ë¡œ) ì°¾ì•˜ëŠ”ê°€?\"\n",
    "  - ê° ì¿¼ë¦¬ì— ëŒ€í•´ **ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ ì •ë‹µ ë¬¸ì„œ**ì˜ ìˆœìœ„(rank)ì˜ ì—­ìˆ˜(`1/rank`)ë¥¼ ê³„ì‚°.\n",
    "  - ì´ ê°’ë“¤ì˜ í‰ê· ì„ ëƒ„. ë§Œì•½ ìƒìœ„ `k`ê°œ ì•ˆì— ì •ë‹µì´ ì—†ìœ¼ë©´ 0ì .\n",
    "- **ì¥ì :** ì •ë‹µì˜ ìˆœìœ„ë¥¼ ê³ ë ¤í•¨. ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ì°¾ëŠ”ì§€ í‰ê°€í•˜ê¸° ì¢‹ìŒ.\n",
    "- **ë‹¨ì :** ì²« ë²ˆì§¸ ì •ë‹µë§Œ ì‹ ê²½ ì”€. ê·¸ ë’¤ì— ë‹¤ë¥¸ ì •ë‹µë“¤ì´ ë” ìˆì–´ë„ ë¬´ì‹œí•¨.\n",
    "- **ì–¸ì œ ì‚¬ìš©í• ê¹Œ?** ì‚¬ìš©ìê°€ ë‹¨ í•˜ë‚˜ì˜ ì •ë‹µì„ ì°¾ëŠ” 'ì‚¬ì‹¤ í™•ì¸í˜•' ì§ˆë¬¸(e.g., \"í…ŒìŠ¬ë¼ì˜ CEOëŠ” ëˆ„êµ¬?\")ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ì— ì í•©í•¨.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994be691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR @4: 0.6667\n"
     ]
    }
   ],
   "source": [
    "def mrr_at_k(actual_doc_ids_list, predicted_docs_list, k=4):\n",
    "    reciprocal_ranks = []\n",
    "    for actual_docs, predicted in zip(actual_doc_ids_list, predicted_docs_list):\n",
    "        actual_ids = {d.metadata['doc_id'] for d in actual_docs}\n",
    "        rank = 0\n",
    "        for i, p in enumerate(predicted[:k]):\n",
    "            if p.metadata['doc_id'] in actual_ids:\n",
    "                rank = i + 1\n",
    "                break\n",
    "        reciprocal_ranks.append(1 / rank if rank > 0 else 0)\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "mrr_score = mrr_at_k(actual_docs_sample, predicted_docs_sample, k=4)\n",
    "print(f\"MRR @4: {mrr_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408fd4ef",
   "metadata": {},
   "source": [
    "### 3.3. mAP @k (Mean Average Precision)\n",
    "\n",
    "- **ê°œë…:** \"ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ë§ì´, ê·¸ë¦¬ê³  ì–¼ë§ˆë‚˜ ë†’ì€ ìˆœìœ„ë¡œ ê°€ì ¸ì™”ëŠ”ê°€?\"\n",
    "  - ì •ë‹µì´ ì—¬ëŸ¬ ê°œì¼ ë•Œ, ìˆœì„œì™€ ì¬í˜„ìœ¨(Recall)ì„ í•¨ê»˜ ê³ ë ¤í•˜ëŠ” ì§€í‘œ. \n",
    "  - ê° ì¿¼ë¦¬ì— ëŒ€í•œ AP(Average Precision)ë¥¼ êµ¬í•˜ê³ , ì´ë“¤ì˜ í‰ê· ì„ ëƒ„.\n",
    "  - APëŠ” ìƒìœ„ `k`ê°œ ê²°ê³¼ ë‚´ì—ì„œ ê° **ì •ë‹µ ë¬¸ì„œê°€ ë‚˜ì˜¬ ë•Œë§ˆë‹¤ì˜ ì •ë°€ë„(Precision) ê°’**ë“¤ì˜ í‰ê· ì„.\n",
    "- **ì¥ì :** ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœì„œì™€ ì •í™•ë„ë¥¼ ëª¨ë‘ ê³ ë ¤í•¨. ì—¬ëŸ¬ ì •ë‹µì´ ìˆëŠ” ê²½ìš°ì— íŠ¹íˆ ìœ ìš©í•¨.\n",
    "- **ë‹¨ì :** ê°œë…ì´ ë‹¤ë¥¸ ì§€í‘œë³´ë‹¤ ì¡°ê¸ˆ ë³µì¡í•¨.\n",
    "- **ì–¸ì œ ì‚¬ìš©í• ê¹Œ?** ì‚¬ìš©ìê°€ í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ì—¬ëŸ¬ ê´€ë ¨ ì •ë³´ë¥¼ ì–»ê³  ì‹¶ì–´í•˜ëŠ” 'íƒìƒ‰í˜•' ì§ˆë¬¸(e.g., \"í…ŒìŠ¬ë¼ ëª¨ë¸ Yì˜ ì¥ì ì€?\")ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ì— ì í•©í•¨.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc2a225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP @4: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def precision_at_i(i, actual_ids, predicted_docs):\n",
    "    predicted_ids_at_i = {p.metadata['doc_id'] for p in predicted_docs[:i]}\n",
    "    num_correct = len(predicted_ids_at_i.intersection(actual_ids))\n",
    "    return num_correct / i\n",
    "\n",
    "def ap_at_k(actual_ids, predicted_docs, k=4):\n",
    "    if not actual_ids:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    for i, p in enumerate(predicted_docs[:k]):\n",
    "        if p.metadata['doc_id'] in actual_ids:\n",
    "            precisions.append(precision_at_i(i + 1, actual_ids, predicted_docs))\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    return sum(precisions) / len(actual_ids)\n",
    "\n",
    "def map_at_k(actual_doc_ids_list, predicted_docs_list, k=4):\n",
    "    aps = [ap_at_k(actual, pred, k) for actual, pred in zip(actual_doc_ids_list, predicted_docs_list)]\n",
    "    return sum(aps) / len(aps)\n",
    "\n",
    "map_score = map_at_k(actual_docs_sample, predicted_docs_sample, k=4)\n",
    "print(f\"mAP @4: {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7e101",
   "metadata": {},
   "source": [
    "### 3.4. NDCG @k (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "- **ê°œë…:** \"ê°€ì¥ ì´ìƒì ì¸ ê²€ìƒ‰ ê²°ê³¼ ëŒ€ë¹„ í˜„ì¬ ê²°ê³¼ëŠ” ì–¼ë§ˆë‚˜ ì¢‹ì€ê°€?\"\n",
    "  - ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœì„œì™€ ë¬¸ì„œì˜ **ê´€ë ¨ì„± ë“±ê¸‰**ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ í‰ê°€í•˜ëŠ” ê°€ì¥ ì •êµí•œ ì§€í‘œ.\n",
    "  - **Discounted Cumulative Gain (DCG):** ìˆœìœ„ê°€ ë‚®ì„ìˆ˜ë¡(log(rank+1)) íŒ¨ë„í‹°ë¥¼ ì£¼ì–´ ì ìˆ˜ë¥¼ í•©ì‚°. ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œê°€ ìƒìœ„ì— ìˆì„ìˆ˜ë¡ DCGê°€ ë†’ìŒ.\n",
    "  - **Normalized (N):** DCG ì ìˆ˜ë¥¼ ì´ìƒì ì¸ ìˆœì„œì¼ ë•Œì˜ DCG(IDCG, Ideal DCG)ë¡œ ë‚˜ëˆ„ì–´ 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ì •ê·œí™”í•¨. ì´ë¥¼ í†µí•´ ì¿¼ë¦¬ë§ˆë‹¤ ë‹¤ë¥¸ ì •ë‹µ ê°œìˆ˜ë‚˜ ê´€ë ¨ë„ ë¶„í¬ì— ìƒê´€ì—†ì´ ê³µì •í•œ ë¹„êµê°€ ê°€ëŠ¥í•´ì§.\n",
    "- **ì¥ì :** ê´€ë ¨ì„±ì„ ì´ì§„(0/1)ì´ ì•„ë‹Œ ë‹¤ë‹¨ê³„(e.g., ë§¤ìš° ê´€ë ¨ ë†’ìŒ=3, ê´€ë ¨ ìˆìŒ=2, ì•½ê°„ ê´€ë ¨=1)ë¡œ ì„¤ì •í•  ìˆ˜ ìˆì–´, ë¬¸ì„œì˜ ì¤‘ìš”ë„ë¥¼ ì„¸ë°€í•˜ê²Œ ë°˜ì˜ ê°€ëŠ¥. ê°€ì¥ ì •êµí•œ ìˆœìœ„ í‰ê°€ ì§€í‘œë¡œ ì•Œë ¤ì ¸ ìˆìŒ.\n",
    "- **ë‹¨ì :** ê°œë…ì´ ë³µì¡í•˜ê³ , ë¬¸ì„œë³„ ê´€ë ¨ì„± ë“±ê¸‰ì„ ë§¤ê¸°ëŠ” ì¶”ê°€ì ì¸ ì‘ì—…ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ. (ì—¬ê¸°ì„œëŠ” ì •ë‹µì´ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ë‹¨ìˆœí™”)\n",
    "- **ì–¸ì œ ì‚¬ìš©í• ê¹Œ?** ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ê°€ ë§¤ìš° ì¤‘ìš”í•˜ê³ , ë¬¸ì„œë§ˆë‹¤ ê´€ë ¨ì„±ì˜ ì •ë„ê°€ ë‹¤ë¥¼ ë•Œ ê°€ì¥ ê°•ë ¥í•œ í‰ê°€ ë„êµ¬ê°€ ë¨.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "684c786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NDCG @4: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def dcg_at_k(relevance_scores, k=4):\n",
    "    scores = np.asfarray(relevance_scores)[:k]\n",
    "    if scores.size:\n",
    "        return np.sum(scores / np.log2(np.arange(2, scores.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(actual_ids, predicted_docs, k=4):\n",
    "    relevance_scores = [1 if p.metadata['doc_id'] in actual_ids else 0 for p in predicted_docs]\n",
    "    ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "    actual_dcg = dcg_at_k(relevance_scores, k)\n",
    "    ideal_dcg = dcg_at_k(ideal_scores, k)\n",
    "    if not ideal_dcg:\n",
    "        return 0.0\n",
    "    return actual_dcg / ideal_dcg\n",
    "\n",
    "def mean_ndcg_at_k(actual_doc_ids_list, predicted_docs_list, k=4):\n",
    "    ndcgs = [ndcg_at_k(actual, pred, k) for actual, pred in zip(actual_doc_ids_list, predicted_docs_list)]\n",
    "    return np.mean(ndcgs)\n",
    "\n",
    "ndcg_score = mean_ndcg_at_k(actual_docs_sample, predicted_docs_sample, k=4)\n",
    "print(f\"Mean NDCG @4: {ndcg_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
