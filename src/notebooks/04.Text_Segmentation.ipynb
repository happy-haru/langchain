{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e4e156",
   "metadata": {},
   "source": [
    "## 1. 효과적인 텍스트 분할 전략\n",
    "- 개인적으로 모델의 성능 뿐만 아니라 가장 중요 요소라가 생각.\n",
    "- LLM은 한 번에 처리할 수 있는 토큰(텍스트의 단위) 수에 제한이 있음(컨텍스트 윈도우).\n",
    "- 긴 문서는 검색 및 LLM 처리에 적합하도록 여러 개의 작은 청크(chunk)로 분할해야 함\n",
    "- 효과적인 텍스트 분할은 RAG 시스템의 성능에 매우 중요\n",
    "\n",
    "\n",
    "**텍스트 분할의 중요성:**\n",
    "- **컨텍스트 윈도우 관리**: LLM의 입력 제한을 초과하지 않도록 해야함.\n",
    "- **검색 정확도 향상**: 관련성 높은 정보만 담긴 작은 청크를 검색하여 LLM에 제공함으로써 답변 품질을 높일 수 있음.\n",
    "- **비용 효율성**: 필요한 부분만 LLM에 전달하여 API 비용을 절감 가능 \n",
    "\n",
    "**고려사항:**\n",
    "- **청크 크기 (`chunk_size`)**: 너무 작으면 문맥이 손실되고, 너무 크면 관련 없는 정보가 포함될 수 있음.\n",
    "- **청크 중복 (`chunk_overlap`)**: 청크 간의 연속성을 유지하고, 중요한 정보가 청크 경계에서 잘리는 것을 방지 할 수 있지만, 너무 크면 중복 정보가 많아짐\n",
    "- **분할 기준**: 문장, 단락 등 의미론적 단위를 기준으로 분할하는 것이 좋은거 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3273251",
   "metadata": {},
   "source": [
    "### 1-1 RecursiveCharacterTextSplitter\n",
    "\n",
    "-`RecursiveCharacterTextSplitter`는 가장 일반적으로 사용되는 텍스트 분할기 중 하나\n",
    "- 지정된 구분자(separator) 리스트를 순서대로 적용하여 재귀적으로 텍스트를 분할\n",
    "- 예를 들어, 먼저 문단(`\\n\\n`)으로 나누고, 각 문단이 너무 길면 문장(`. `)으로, 그래도 길면 단어(` `)로 나누는 식으로 진행\n",
    "\n",
    "**주요 파라미터:**\n",
    "- `chunk_size`: 목표 청크 크기 (글자 수 또는 토큰 수).\n",
    "- `chunk_overlap`: 청크 간 중복되는 글자 수 또는 토큰 수. 문맥 유지를 위해 사용\n",
    "- `length_function`: 청크 크기를 계산하는 함수 (기본값은 `len`, 즉 글자 수).\n",
    "- `separators`: 분할에 사용할 구분자 리스트. 우선순위 순서대로 적용. (예: `[\"\\n\\n\", \"\\n\", \" \", \"\"]`)\n",
    "\n",
    "**장점:**\n",
    "- 설정이 비교적 간단하고, 일반적인 텍스트에 잘 작동.\n",
    "- 재귀적 접근 방식으로 의미론적 경계를 어느 정도 존중.\n",
    "\n",
    "**단점:**\n",
    "- `separators` 설정에 따라 성능이 달라질 수 있음.\n",
    "- 고정된 크기를 엄격하게 지키기보다, 구분자를 우선적으로 고려하여 분할하여서 청크 크기가 다소 불균일할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a924fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader('./data/transformer.pdf')\n",
    "pdf_docs = pdf_loader.load() # Document 객체의 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666aa18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 52\n",
      "각 청크의 길이 (처음 5개): [981, 910, 975, 451, 932]\n",
      "각 청크의 길이 (마지막 5개): [929, 849, 812, 814, 817]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # 각 청크의 최대 글자 수\n",
    "    chunk_overlap=200,      # 청크 간 중복되는 글자 수 (연속성 유지)\n",
    "    length_function=len,    # 글자 수를 기준으로 분할 (기본값)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # 분할 시도 순서: 문단 -> 줄바꿈 -> 마침표 -> 공백 -> 글자\n",
    "    is_separator_regex=False, # 구분자를 정규표현식으로 해석할지 여부\n",
    ")\n",
    "\n",
    "# PDF 문서(pdf_docs)를 분할합니다.\n",
    "recursive_texts = text_splitter_recursive.split_documents(pdf_docs)\n",
    "print(f\"생성된 텍스트 청크 수: {len(recursive_texts)}\")\n",
    "chunk_lengths = [len(text.page_content) for text in recursive_texts]\n",
    "print(f\"각 청크의 길이 (처음 5개): {chunk_lengths[:5]}\")\n",
    "print(f\"각 청크의 길이 (마지막 5개): {chunk_lengths[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89097e",
   "metadata": {},
   "source": [
    "- RecursiveCharacterTextSplitter는 이름에서 알 수 있듯이 재귀적으로 텍스트를 분할\n",
    "- 구분자를 순차적으로 적용하여 큰 청크에서 시작하여 점진적으로 더 작은 단위로 나뉨 \n",
    "- CharacterTextSplitter보다 더 엄격하게 크기를 준수하려고 하지만, 구분자를 우선시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ef6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 청크 1 (길이: 981) ---\n",
      "[시작 부분]\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "[...중략...]\n",
      "\n",
      "[끝 부분]\n",
      "the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "====================================================================================================\n",
      "\n",
      "--- 청크 2 (길이: 910) ---\n",
      "[시작 부분]\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine transla\n",
      "\n",
      "[...중략...]\n",
      "\n",
      "[끝 부분]\n",
      "the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "====================================================================================================\n",
      "\n",
      "--- 청크 3 (길이: 975) ---\n",
      "[시작 부분]\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Eq\n",
      "\n",
      "[...중략...]\n",
      "\n",
      "[끝 부분]\n",
      " experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 청크의 시작 부분과 끝 부분 확인 (중복(overlap)이 어떻게 적용되는지 관찰)\n",
    "for i, text_chunk in enumerate(recursive_texts[:3]): # 처음 3개 청크만 확인\n",
    "    print(f\"--- 청크 {i+1} (길이: {len(text_chunk.page_content)}) ---\")\n",
    "    print(\"[시작 부분]\")\n",
    "    print(text_chunk.page_content[:200])\n",
    "    print(\"\\n[...중략...]\\n\")\n",
    "    print(\"[끝 부분]\")\n",
    "    print(text_chunk.page_content[-200:])\n",
    "    print(\"=\" * 100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857805cc",
   "metadata": {},
   "source": [
    "### 1-2 정규표현식 사용 (CharacterTextSplitter)\n",
    "\n",
    "- `CharacterTextSplitter`는 `RecursiveCharacterTextSplitter`의 기반이 되는 분할기\n",
    "- `separator`를 단일 문자열로 받으며, `is_separator_regex=True`로 설정하면 이 `separator`를 정규표현식으로 해석하여 분할\n",
    "- 특정 패턴을 기준으로 텍스트를 정교하게 나눌 때 유용\n",
    "\n",
    "**주요 파라미터:**\n",
    "- `separator`: 분할 기준이 되는 문자열 또는 정규표현식.\n",
    "- `is_separator_regex`: `separator`를 정규표현식으로 다룰지 여부 (기본값 `False`).\n",
    "- `keep_separator`: 분할 후 각 청크에 구분자를 유지할지 여부 (기본값 `False`).\n",
    "\n",
    "**정규표현식 예시: `(?<=[.!?])\\s+`**\n",
    "- `(?<=[.!?])`: 마침표(.), 느낌표(!), 물음표(?) 중 하나가 앞에 오는 위치를 찾으며, 이 문자 자체는 분할 기준에 포함되지 않음\n",
    "- `\\s+`: 하나 이상의 공백 문자와 일치\n",
    "- 즉, 문장의 끝을 나타내는 구두점(`.`, `!`, `?`) 뒤에 오는 공백을 기준으로 분할\n",
    "\n",
    "**장점:**\n",
    "- 정규표현식을 통해 매우 유연하고 정교한 분할 규칙을 정의할 수 있습니다.\n",
    "\n",
    "**단점:**\n",
    "- 정규표현식 작성 및 디버깅이 어려울 수 있음.(아직 어려움)\n",
    "- 복잡한 정규표현식은 성능에 영향을 줄 수 있음.\n",
    "- `chunk_size`를 엄격히 지키기보다는 정규표현식에 의한 분할을 우선하여, 청크 크기가 매우 불균일하게 나올 수 있으며, `chunk_size`는 사실상 분할된 부분들을 합칠 때의 최대 크기 제한처럼 동작함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9117e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "\n",
    "    metadata[\"sender\"] = record.get(\"sender\")\n",
    "    metadata[\"timestamp\"] = record.get(\"timestamp\")\n",
    "    return metadata\n",
    "jsonl_loader_with_meta = JSONLoader(\n",
    "    file_path=\"./data/kakao_chat.jsonl\",\n",
    "    jq_schema=\".\",                 \n",
    "    content_key=\"content\",         \n",
    "    metadata_func=metadata_func,   \n",
    "    json_lines=True,            \n",
    ")\n",
    "\n",
    "jsonl_docs_with_meta = jsonl_loader_with_meta.load()\n",
    "json_docs = jsonl_docs_with_meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819a2855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 8\n",
      "각 청크의 길이 (처음 10개): [31, 9, 15, 19, 11, 27, 13, 13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 문장을 구분하여 분할 (마침표, 느낌표, 물음표 다음에 공백이 오는 경우 문장의 끝으로 판단)\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter_regex = CharacterTextSplitter(\n",
    "    chunk_size=20, # 정규식으로 분리된 조각들을 합칠 때의 최대 크기.\n",
    "                   # 실제 청크는 이보다 훨씬 작을 수 있음.\n",
    "    chunk_overlap=0, # 문장 단위 분할이므로 중복을 0으로 설정\n",
    "    separator=r'(?<=[.!?])\\s+', # 문장 끝 구두점 뒤 공백을 기준으로 분할하는 정규표현식\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=True, # 구분자(공백)를 청크에 포함하지 않음\n",
    "    # keep_separator False일때, 사용할수있는 경우는 언제일까????\n",
    "    \n",
    ")\n",
    "\n",
    "regex_texts = text_splitter_regex.split_documents(json_docs) \n",
    "print(f\"생성된 텍스트 청크 수: {len(regex_texts)}\")\n",
    "chunk_lengths_regex = [len(text.page_content) for text in regex_texts]\n",
    "print(f\"각 청크의 길이 (처음 10개): {chunk_lengths_regex[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ea7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 정규식 청크 1 (길이: 31) ---\n",
      "안녕하세요 여러분, 오늘 회의 시간 확인차 연락드립니다.\n",
      "======================================================================\n",
      "\n",
      "--- 정규식 청크 2 (길이: 9) ---\n",
      "네, 안녕하세요.\n",
      "======================================================================\n",
      "\n",
      "--- 정규식 청크 3 (길이: 15) ---\n",
      "오후 2시에 하기로 했어요.\n",
      "======================================================================\n",
      "\n",
      "--- 정규식 청크 4 (길이: 19) ---\n",
      "확인했습니다. 회의실은 어디인가요?\n",
      "======================================================================\n",
      "\n",
      "--- 정규식 청크 5 (길이: 11) ---\n",
      "3층 대회의실입니다.\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 청크의 내용 확인 (문장 단위로 잘 분할되었는지)\n",
    "for i, text_chunk in enumerate(regex_texts[:5]): # 처음 5개 청크만 확인\n",
    "    print(f\"--- 정규식 청크 {i+1} (길이: {len(text_chunk.page_content)}) ---\")\n",
    "    print(text_chunk.page_content)\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effad4d",
   "metadata": {},
   "source": [
    "### 1-3 토큰 수를 기반으로 분할\n",
    "\n",
    "- LLM은 내부적으로 텍스트를 토큰(token) 단위로 처리\n",
    "- 글자 수보다는 토큰 수를 기준으로 텍스트를 분할하는 것이 LLM의 컨텍스트 윈도우를 보다 정확하게 관리하는 방법\n",
    "- LangChain은 다양한 토크나이저와 연동하여 토큰 기반 분할을 지원\n",
    "\n",
    "**장점:**\n",
    "- LLM의 실제 처리 단위인 토큰을 기준으로 분할하므로 컨텍스트 윈도우 관리가 용이\n",
    "- 모델별 토크나이저를 사용하여 해당 모델에 최적화된 분할이 가능합니다.\n",
    "\n",
    "**단점:**\n",
    "- 어떤 토크나이저를 사용하느냐에 따라 분할 결과와 토큰 수가 달라짐\n",
    "- 토큰화 과정 자체에 약간의 연산 비용이 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65a54d",
   "metadata": {},
   "source": [
    "`(1) tiktoken`  \n",
    "- OpenAI에서 만든 BPE (Byte Pair Encoding) 기반 토크나이저 라이브러리\n",
    "- GPT 시리즈 (gpt-3.5-turbo, gpt-4, text-embedding-ada-002 등) 모델들이 사용하는 토큰화 방식을 따름.\n",
    "- `RecursiveCharacterTextSplitter.from_tiktoken_encoder()` 메서드를 사용하면 특정 OpenAI 모델의 토크나이저를 기준으로 분할할 수 있음\n",
    "  - `encoding_name`: `cl100k_base` (대부분의 최신 OpenAI 모델), `p50k_base` 등 Tiktoken 인코딩 이름을 직접 지정\n",
    "  - `model_name`: `gpt-4o-mini`, `text-embedding-3-small` 등 OpenAI 모델 이름을 지정하면 해당 모델의 기본 인코딩을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758cbb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수: 3\n",
      "각 청크의 글자 수: [1140, 1389, 783]\n",
      "\n",
      "--- 각 청크 미리보기 (Tiktoken) ---\n",
      "\n",
      "--- 청크 1 (글자 수: 1140) ---\n",
      "[시작 부분]\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and\n",
      "\n",
      "[...중략...]\n",
      "\n",
      "[끝 부분]\n",
      "w these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "======================================================================\n",
      "\n",
      "--- 청크 2 (글자 수: 1389) ---\n",
      "[시작 부분]\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experime\n",
      "\n",
      "[...중략...]\n",
      "\n",
      "[끝 부분]\n",
      "iki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "======================================================================\n",
      "\n",
      "--- 청크 3 (글자 수: 783) ---\n",
      "[시작 부분]\n",
      "attention and the parameter-free position representation and became the other person involved in nea\n",
      "\n",
      "[...중략...]\n",
      "\n",
      "[끝 부분]\n",
      "ormation Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tiktoken을 사용하여 토큰 수 기준으로 분할하는 TextSplitter 생성\n",
    "text_splitter_tiktoken = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  # text-embedding-ada-002, gpt-3.5-turbo, gpt-4 등 최신 모델용 인코딩\n",
    "    # model_name=\"gpt-4o-mini\", # 모델 이름을 지정하여 해당 모델의 토크나이저 사용\n",
    "    chunk_size=300,  # 각 청크의 최대 토큰 수\n",
    "    chunk_overlap=50, # 청크 간 중복되는 토큰 수\n",
    "    # separators 등 RecursiveCharacterTextSplitter의 다른 파라미터도 사용 가능\n",
    ")\n",
    "\n",
    "# PDF 문서의 첫 페이지만 분할 (pdf_docs[:1])\n",
    "chunks_tiktoken = text_splitter_tiktoken.split_documents(pdf_docs[:1])\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks_tiktoken)}\")\n",
    "chunk_char_lengths_tiktoken = [len(chunk.page_content) for chunk in chunks_tiktoken]\n",
    "print(f\"각 청크의 글자 수: {chunk_char_lengths_tiktoken}\")\n",
    "print(\"\\n--- 각 청크 미리보기 (Tiktoken) ---\")\n",
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for i, chunk in enumerate(chunks_tiktoken[:3]): # 처음 3개 청크만 확인\n",
    "    print(f\"\\n--- 청크 {i+1} (글자 수: {len(chunk.page_content)}) ---\")\n",
    "    print(\"[시작 부분]\")\n",
    "    print(chunk.page_content[:100])\n",
    "    print(\"\\n[...중략...]\\n\")\n",
    "    print(\"[끝 부분]\")\n",
    "    print(chunk.page_content[-100:])\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bab01",
   "metadata": {},
   "source": [
    "**Tiktoken으로 실제 토큰 수 확인**\n",
    "분할된 각 청크가 실제로 목표한 토큰 수 (`chunk_size=300`)에 근접하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1251af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 각 청크의 실제 토큰 수 (Tiktoken gpt-4o-mini) ---\n",
      "청크 1: 275 토큰\n",
      "청크 2: 287 토큰\n",
      "청크 3: 164 토큰\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# text_splitter_tiktoken에서 사용한 것과 동일한 인코딩/모델을 지정해야 정확합니다.\n",
    "# tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer_gpt4omini = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "print(\"--- 각 청크의 실제 토큰 수 (Tiktoken gpt-4o-mini) ---\")\n",
    "for i, chunk in enumerate(chunks_tiktoken[:5]): # 처음 5개 청크 확인\n",
    "    tokens = tokenizer_gpt4omini.encode(chunk.page_content)\n",
    "    print(f\"청크 {i+1}: {len(tokens)} 토큰\")\n",
    "    # print(f\"  첫 10개 토큰 ID: {tokens[:10]}\")\n",
    "    # token_strings = [tokenizer_gpt4omini.decode([token]) for token in tokens[:10]]\n",
    "    # print(f\"  첫 10개 토큰 문자열: {token_strings}\")\n",
    "    # print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b0c68",
   "metadata": {},
   "source": [
    "`(2) Hugging Face 토크나이저`  \n",
    "- Hugging Face `transformers` 라이브러리에서 제공하는 다양한 오픈소스 모델의 토크나이저를 사용할 수 있음.\n",
    "- `RecursiveCharacterTextSplitter.from_huggingface_tokenizer()` 메서드를 사용.\n",
    "  - `tokenizer`: Hugging Face `transformers.PreTrainedTokenizerBase`의 인스턴스를 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0f4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaTokenizerFast(name_or_path='BAAI/bge-m3', vocab_size=250002, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BAAI/bge-m3 모델의 토크나이저 로드\n",
    "hf_tokenizer_bge_m3 = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "print(hf_tokenizer_bge_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cbfade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'안녕하세요. 반갑습니다.' -> 토큰 ID: [0, 107687, 5, 20451, 54272, 16367, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face 토크나이저 인코딩 테스트\n",
    "sample_text_ko = \"안녕하세요. 반갑습니다.\"\n",
    "tokens_hf_bge_m3 = hf_tokenizer_bge_m3.encode(sample_text_ko)\n",
    "print(f\"'{sample_text_ko}' -> 토큰 ID: {tokens_hf_bge_m3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a2a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 문자열: ['<s>', '▁안녕하세요', '.', '▁반', '갑', '습니다', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# 토큰 ID를 실제 토큰 문자열로 변환\n",
    "print(f\"토큰 문자열: {hf_tokenizer_bge_m3.convert_ids_to_tokens(tokens_hf_bge_m3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4992d5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코딩된 텍스트: <s> 안녕하세요. 반갑습니다.</s>\n"
     ]
    }
   ],
   "source": [
    "# 토큰 ID를 다시 원본 텍스트로 디코딩\n",
    "print(f\"디코딩된 텍스트: {hf_tokenizer_bge_m3.decode(tokens_hf_bge_m3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680c8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Hugging Face 토크나이저를 사용하여 토큰 수 기준으로 분할하는 TextSplitter 생성\n",
    "text_splitter_hf = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=hf_tokenizer_bge_m3,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# PDF 문서의 첫 페이지만 분할 (pdf_docs[:1])\n",
    "chunks_hf = text_splitter_hf.split_documents(pdf_docs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b81e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수: 3\n",
      "각 청크의 글자 수: [1214, 1307, 783]\n",
      "\n",
      "--- 각 청크의 실제 토큰 수 (HuggingFace BAAI/bge-m3) ---\n",
      "청크 1: 301 토큰\n",
      "청크 2: 293 토큰\n",
      "청크 3: 181 토큰\n"
     ]
    }
   ],
   "source": [
    "print(f\"생성된 청크 수: {len(chunks_hf)}\")\n",
    "chunk_char_lengths_hf = [len(chunk.page_content) for chunk in chunks_hf]\n",
    "print(f\"각 청크의 글자 수: {chunk_char_lengths_hf}\")\n",
    "print()\n",
    "\n",
    "print(\"--- 각 청크의 실제 토큰 수 (HuggingFace BAAI/bge-m3) ---\")\n",
    "for i, chunk in enumerate(chunks_hf[:5]): # 처음 5개 청크 확인\n",
    "    tokens = hf_tokenizer_bge_m3.encode(chunk.page_content)\n",
    "    print(f\"청크 {i+1}: {len(tokens)} 토큰\")\n",
    "    # print(f\"  첫 10개 토큰 ID: {tokens[:10]}\")\n",
    "    # token_strings = hf_tokenizer_bge_m3.convert_ids_to_tokens(tokens[:10]) \n",
    "    # print(f\"  첫 10개 토큰 문자열: {token_strings}\")\n",
    "    # print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74b5cb",
   "metadata": {},
   "source": [
    "### 1-4 Semantic Chunking (시맨틱 청킹)\n",
    "\n",
    "- `SemanticChunker`은 고정된 크기나 규칙 기반이 아닌, 문장 간의 의미론적 유사성을 기반으로 텍스트를 분할\n",
    "- 임베딩 모델을 사용하여 각 문장의 임베딩 벡터를 계산하고, 인접한 문장들 간의 코사인 유사도를 측정합니다. 이 유사도가 특정 임계값(breakpoint)을 기준으로 크게 변하는 지점에서 청크를 나눕니다. 즉, 의미적으로 관련성이 높은 문장들을 하나의 청크로 묶으려는 시도입니다.\n",
    "\n",
    "**주요 파라미터:**\n",
    "- `embeddings`: 문장의 의미론적 유사도를 계산하는 데 사용할 임베딩 모델 (예: `OpenAIEmbeddings`).\n",
    "- `breakpoint_threshold_type`: 유사도 변화의 기준점을 정하는 방식.\n",
    "  - `\"percentile\"` (기본값): 유사도 분포의 특정 백분위수를 기준점\n",
    "  - `\"standard_deviation\"`: 평균에서 표준편차의 특정 배수만큼 떨어진 지점을 기준점\n",
    "  - `\"gradient\"`: 유사도 값의 변화율(기울기)이 급격히 변하는 지점을 찾으며, 문맥 전환을 더 잘 감지할 수 있음\n",
    "  - `\"interquartile\"`: 사분위수 범위를 사용하여 기준점을 설정\n",
    "- `breakpoint_threshold_amount` (또는 `percentile_threshold`, `threshold` 등 타입에 따라 다름): 기준점 타입에 따른 구체적인 값.\n",
    "\n",
    "**장점:**\n",
    "- 의미론적으로 응집력 있는 청크를 생성하여 RAG의 검색 품질 및 답변 생성 품질을 향상시킬 잠재력이 있음\n",
    "- 고정 크기 분할보다 문맥 유지가 잘 됨\n",
    "\n",
    "**단점:**\n",
    "- **실험적 기능**: 아직 개발 중이며, 성능이 불안정하거나 API가 변경될 수 있어, 고정된 값이 안 나올 수 있음.\n",
    "- **계산 비용**: 모든 문장에 대해 임베딩을 계산해야 하므로 다른 분할 방식보다 연산량이 많고 느림.\n",
    "- 임베딩 모델의 품질과 데이터 특성에 따라 분할 결과가 크게 달라질 수 있음.\n",
    "- 최적의 `breakpoint_threshold_type`과 값을 찾기 위한 실험이 필요함.\n",
    "\n",
    "\n",
    "**사내 연구용으로 적용하기에는 적절한 구조일것으로 추정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d6e996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # .env 파일에 정의된 환경변수들을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "523f8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# SemanticChunker는 임베딩 모델을 사용하여 문장 간 유사도를 계산\n",
    "text_splitter_semantic = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), # OpenAI 임베딩 사용\n",
    "    breakpoint_threshold_type=\"gradient\",  # 기준점 타입: 기울기 변화 감지\n",
    "    # breakpoint_threshold_type=\"percentile\", percentile_threshold=95, # 예: 상위 5% 변화 지점\n",
    "    # add_start_index=True # 메타데이터에 청크 시작 인덱스 추가 여부 (토큰 기반으로 문서를 재조합할 때 유용)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d2e9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 청크 수 (SemanticChunker): 2\n",
      "각 청크의 글자 수: [1736, 1116]\n",
      "\n",
      "--- 각 시맨틱 청크 미리보기 및 토큰 수 (참고용) ---\n",
      "\n",
      "--- 시맨틱 청크 1 (글자 수: 1736, 토큰 수: 415) ---\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works. Attention Is All You Need\n",
      "\n",
      "[...]\n",
      "======================================================================\n",
      "\n",
      "--- 시맨틱 청크 2 (글자 수: 1116, 토큰 수: 235) ---\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra\n",
      "[...]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# PDF 문서의 첫 페이지만 분할 (pdf_docs[:1])\n",
    "# 주의: SemanticChunker는 내부적으로 문장 단위로 나누고 임베딩을 계산하므로, \n",
    "# 입력 문서가 클 경우 상당한 시간이 소요될 수 있습니다.\n",
    "chunks_semantic = text_splitter_semantic.split_documents(pdf_docs[:1])\n",
    "\n",
    "print(f\"생성된 청크 수 (SemanticChunker): {len(chunks_semantic)}\")\n",
    "chunk_char_lengths_semantic = [len(chunk.page_content) for chunk in chunks_semantic]\n",
    "print(f\"각 청크의 글자 수: {chunk_char_lengths_semantic}\")\n",
    "print()\n",
    "\n",
    "# Tiktoken 토크나이저로 각 시맨틱 청크의 토큰 수 확인 (참고용)\n",
    "tokenizer_for_semantic_check = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "print(\"--- 각 시맨틱 청크 미리보기 및 토큰 수 (참고용) ---\")\n",
    "for i, chunk in enumerate(chunks_semantic[:5]): # 처음 5개 청크 확인\n",
    "    tokens = tokenizer_for_semantic_check.encode(chunk.page_content)\n",
    "    print(f\"\\n--- 시맨틱 청크 {i+1} (글자 수: {len(chunk.page_content)}, 토큰 수: {len(tokens)}) ---\")\n",
    "    print(chunk.page_content[:200]) # 내용 일부 출력\n",
    "    if len(chunk.page_content) > 200:\n",
    "        print(\"[...]\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb71caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8729c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
