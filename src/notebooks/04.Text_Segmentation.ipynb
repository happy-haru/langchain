{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e4e156",
   "metadata": {},
   "source": [
    "## 1. íš¨ê³¼ì ì¸ í…ìŠ¤íŠ¸ ë¶„í•  ì „ëµ\n",
    "- ëª¨ë¸ ì„±ëŠ¥ë§Œí¼ ì¤‘ìš”í•œ ìš”ì†Œë¼ê³  ìƒê°í•¨.\n",
    "- LLMì€ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í† í°(í…ìŠ¤íŠ¸ì˜ ë‹¨ìœ„) ìˆ˜ì— ì œí•œ(ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°)ì´ ìˆìŒ.\n",
    "- ê¸´ ë¬¸ì„œëŠ” ê²€ìƒ‰ ë° LLM ì²˜ë¦¬ì— ì í•©í•˜ë„ë¡ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ì²­í¬(chunk)ë¡œ ë¶„í• í•´ì•¼ í•¨.\n",
    "- íš¨ê³¼ì ì¸ í…ìŠ¤íŠ¸ ë¶„í• ì€ RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì— ë§¤ìš° ì¤‘ìš”í•¨.\n",
    "\n",
    "**í…ìŠ¤íŠ¸ ë¶„í• ì˜ ì¤‘ìš”ì„±:**\n",
    "- **ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬**: LLMì˜ ì…ë ¥ ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬í•´ì•¼ í•¨.\n",
    "- **ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ**: ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë§Œ ë‹´ê¸´ ì‘ì€ ì²­í¬ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì— ì œê³µí•¨ìœ¼ë¡œì¨ ë‹µë³€ í’ˆì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŒ.\n",
    "- **ë¹„ìš© íš¨ìœ¨ì„±**: í•„ìš”í•œ ë¶€ë¶„ë§Œ LLMì— ì „ë‹¬í•˜ì—¬ API í˜¸ì¶œ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "**ê³ ë ¤ì‚¬í•­:**\n",
    "- **ì²­í¬ í¬ê¸° (`chunk_size`)**: ë„ˆë¬´ ì‘ìœ¼ë©´ ë¬¸ë§¥ì´ ì†ì‹¤ë˜ê³ , ë„ˆë¬´ í¬ë©´ ê´€ë ¨ ì—†ëŠ” ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŒ. ì ì ˆí•œ í¬ê¸° ì„¤ì •ì´ ì¤‘ìš”í•¨.\n",
    "- **ì²­í¬ ì¤‘ë³µ (`chunk_overlap`)**: ì²­í¬ ê°„ì˜ ì—°ì†ì„±ì„ ìœ ì§€í•˜ê³ , ì¤‘ìš”í•œ ì •ë³´ê°€ ì²­í¬ ê²½ê³„ì—ì„œ ì˜ë¦¬ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŒ. ë‹¤ë§Œ, ë„ˆë¬´ í¬ë©´ ì¤‘ë³µ ì •ë³´ê°€ ë§ì•„ì ¸ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ.\n",
    "- **ë¶„í•  ê¸°ì¤€**: ë¬¸ì¥, ë‹¨ë½ ë“± ì˜ë¯¸ë¡ ì  ë‹¨ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ìœ¼ë¡œ íš¨ê³¼ì ì„.\n",
    "\n",
    "**ğŸ’¡ íŒ ë° ë…¸í•˜ìš°:**\n",
    "- **ë¬¸ì„œ ìœ í˜•ë³„ ì „ëµ**: ë¬¸ì„œì˜ ì¢…ë¥˜(ì˜ˆ: ì½”ë“œ, ë…¼ë¬¸, ëŒ€í™”, ë³´ê³ ì„œ)ì— ë”°ë¼ ìµœì ì˜ ë¶„í•  ì „ëµì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì˜ˆë¥¼ ë“¤ì–´, ì½”ë“œ ë¶„í•  ì‹œì—ëŠ” í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ ë‹¨ìœ„ ë¶„í• ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ.\n",
    "- **ë©”íƒ€ë°ì´í„° í™œìš©**: ë¶„í• ëœ ì²­í¬ì— ì›ë³¸ ë¬¸ì„œëª…, í˜ì´ì§€ ë²ˆí˜¸, ì²­í¬ ìˆœì„œ ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨ì‹œí‚¤ë©´, RAG ì‹œìŠ¤í…œì—ì„œ ì¶œì²˜ ì¶”ì ì´ë‚˜ ë””ë²„ê¹… ì‹œ ë§¤ìš° ìœ ìš©í•¨.\n",
    "- **ì‹¤í—˜ê³¼ ê²€ì¦**: `chunk_size`ì™€ `chunk_overlap` ê°™ì€ íŒŒë¼ë¯¸í„°ëŠ” ë°ì´í„°ì™€ ì‚¬ìš© ëª©ì ì— ë”°ë¼ ì‹¤í—˜ì„ í†µí•´ ìµœì ê°’ì„ ì°¾ëŠ” ê³¼ì •ì´ í•„ìš”í•¨. ë¶„í• ëœ ì²­í¬ì˜ ë‚´ìš©ì„ ì§ì ‘ í™•ì¸í•˜ì—¬ ì˜ë„ëŒ€ë¡œ ë¶„í• ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3273251",
   "metadata": {},
   "source": [
    "### 1-1. RecursiveCharacterTextSplitter\n",
    "\n",
    "- `RecursiveCharacterTextSplitter`ëŠ” ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì¤‘ í•˜ë‚˜ì„.\n",
    "- ì§€ì •ëœ êµ¬ë¶„ì(separator) ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì ìš©í•˜ì—¬ ì¬ê·€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•¨.\n",
    "- ì˜ˆë¥¼ ë“¤ì–´, ë¨¼ì € ë¬¸ë‹¨(`\\n\\n`)ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° ë¬¸ë‹¨ì´ ë„ˆë¬´ ê¸¸ë©´ ë¬¸ì¥(`. `)ìœ¼ë¡œ, ê·¸ë˜ë„ ê¸¸ë©´ ë‹¨ì–´(` `)ë¡œ ë‚˜ëˆ„ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰ë¨.\n",
    "\n",
    "**ì£¼ìš” íŒŒë¼ë¯¸í„°:**\n",
    "- `chunk_size`: ëª©í‘œ ì²­í¬ í¬ê¸° (ê¸€ì ìˆ˜ ë˜ëŠ” í† í° ìˆ˜).\n",
    "- `chunk_overlap`: ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” ê¸€ì ìˆ˜ ë˜ëŠ” í† í° ìˆ˜. ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ì‚¬ìš©ë¨.\n",
    "- `length_function`: ì²­í¬ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ (ê¸°ë³¸ê°’ì€ `len`, ì¦‰ ê¸€ì ìˆ˜).\n",
    "- `separators`: ë¶„í• ì— ì‚¬ìš©í•  êµ¬ë¶„ì ë¦¬ìŠ¤íŠ¸. ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ ì ìš©ë¨. (ì˜ˆ: `[\"\\n\\n\", \"\\n\", \" \", \"\"]`)\n",
    "\n",
    "**ì¥ì :**\n",
    "- ì„¤ì •ì´ ë¹„êµì  ê°„ë‹¨í•˜ê³ , ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ì— ì˜ ì‘ë™í•¨.\n",
    "- ì¬ê·€ì  ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì˜ë¯¸ë¡ ì  ê²½ê³„ë¥¼ ì–´ëŠ ì •ë„ ì¡´ì¤‘í•˜ë ¤ê³  ì‹œë„í•¨.\n",
    "- ë‹¤ì–‘í•œ êµ¬ë¶„ìë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ì—¬ ìœ ì—°í•˜ê²Œ ë¶„í•  ê°€ëŠ¥í•¨.\n",
    "\n",
    "**ë‹¨ì :**\n",
    "- `separators` ì„¤ì •ì— ë”°ë¼ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ. ì ì ˆí•œ êµ¬ë¶„ì ì„¤ì •ì´ ì¤‘ìš”í•¨.\n",
    "- ê³ ì •ëœ í¬ê¸°ë¥¼ ì—„ê²©í•˜ê²Œ ì§€í‚¤ê¸°ë³´ë‹¤, êµ¬ë¶„ìë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ë¶„í• í•˜ë¯€ë¡œ ì²­í¬ í¬ê¸°ê°€ ë‹¤ì†Œ ë¶ˆê· ì¼í•  ìˆ˜ ìˆìŒ.\n",
    "- ë³µì¡í•œ êµ¬ì¡°ì˜ ë¬¸ì„œ(ì˜ˆ: í‘œ, ë‹¤ë‹¨ ë¬¸ì„œ)ì—ì„œëŠ” ì™„ë²½í•œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "**ğŸ’¡ íŒ ë° ë…¸í•˜ìš°:**\n",
    "- `separators` ë¦¬ìŠ¤íŠ¸ì˜ ìˆœì„œê°€ ì¤‘ìš”í•¨. ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ì˜ ê²½ìš° `[\"\\n\\n\" (ë¬¸ë‹¨), \"\\n\" (ì¤„ë°”ê¿ˆ), \". \" (ë¬¸ì¥), \" \" (ë‹¨ì–´), \"\" (ê¸€ì)]` ìˆœì„œê°€ íš¨ê³¼ì ì´ì§€ë§Œ, ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì½”ë“œ ê°™ì€ íŠ¹ìˆ˜ í˜•ì‹ì€ ë‹¤ë¥¸ êµ¬ë¶„ì ìš°ì„ ìˆœìœ„ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ.\n",
    "- `chunk_overlap`ì€ ë³´í†µ `chunk_size`ì˜ 10~20% ì •ë„ë¡œ ì„¤ì •í•˜ì§€ë§Œ, ë¬¸ì„œì˜ íŠ¹ì„±ê³¼ ê²€ìƒ‰ ì „ëµì— ë”°ë¼ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì¢‹ìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a924fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader('../data/transformer.pdf')\n",
    "pdf_docs = pdf_loader.load() # Document ê°ì²´ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666aa18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: 52\n",
      "ê° ì²­í¬ì˜ ê¸¸ì´ (ì²˜ìŒ 5ê°œ): [981, 910, 975, 451, 932]\n",
      "ê° ì²­í¬ì˜ ê¸¸ì´ (ë§ˆì§€ë§‰ 5ê°œ): [929, 849, 812, 814, 817]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # ê° ì²­í¬ì˜ ìµœëŒ€ ê¸€ì ìˆ˜\n",
    "    chunk_overlap=200,      # ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” ê¸€ì ìˆ˜ (ì—°ì†ì„± ìœ ì§€)\n",
    "    length_function=len,    # ê¸€ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ê¸°ë³¸ê°’)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # ë¶„í•  ì‹œë„ ìˆœì„œ: ë¬¸ë‹¨ -> ì¤„ë°”ê¿ˆ -> ë§ˆì¹¨í‘œ -> ê³µë°± -> ê¸€ì\n",
    "    is_separator_regex=False, # êµ¬ë¶„ìë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í•´ì„í• ì§€ ì—¬ë¶€\n",
    ")\n",
    "\n",
    "# PDF ë¬¸ì„œ(pdf_docs)ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "recursive_texts = text_splitter_recursive.split_documents(pdf_docs)\n",
    "print(f\"ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: {len(recursive_texts)}\")\n",
    "chunk_lengths = [len(text.page_content) for text in recursive_texts]\n",
    "print(f\"ê° ì²­í¬ì˜ ê¸¸ì´ (ì²˜ìŒ 5ê°œ): {chunk_lengths[:5]}\")\n",
    "print(f\"ê° ì²­í¬ì˜ ê¸¸ì´ (ë§ˆì§€ë§‰ 5ê°œ): {chunk_lengths[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89097e",
   "metadata": {},
   "source": [
    "- `RecursiveCharacterTextSplitter`ëŠ” ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ì¬ê·€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•¨.\n",
    "- ì§€ì •ëœ êµ¬ë¶„ì ë¦¬ìŠ¤íŠ¸ë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ì—¬, í° ë©ì–´ë¦¬ì—ì„œ ì‹œì‘í•´ ì ì§„ì ìœ¼ë¡œ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë‚˜ê°.\n",
    "- `CharacterTextSplitter`ë³´ë‹¤ ì˜ë¯¸ë¡ ì  ê²½ê³„ë¥¼ ìœ ì§€í•˜ë ¤ ë…¸ë ¥í•˜ë©°, `chunk_size`ë¥¼ ë§ì¶”ë ¤ê³  í•˜ì§€ë§Œ êµ¬ë¶„ìë¥¼ ìš°ì„ ì‹œí•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ef6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ì²­í¬ 1 (ê¸¸ì´: 981) ---\n",
      "[ì‹œì‘ ë¶€ë¶„]\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "\n",
      "\n",
      "[...ì¤‘ëµ...]\n",
      "\n",
      "[ë ë¶€ë¶„]\n",
      "the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "====================================================================================================\n",
      "\n",
      "--- ì²­í¬ 2 (ê¸¸ì´: 910) ---\n",
      "[ì‹œì‘ ë¶€ë¶„]\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine transla\n",
      "\n",
      "[...ì¤‘ëµ...]\n",
      "\n",
      "[ë ë¶€ë¶„]\n",
      "the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "====================================================================================================\n",
      "\n",
      "--- ì²­í¬ 3 (ê¸¸ì´: 975) ---\n",
      "[ì‹œì‘ ë¶€ë¶„]\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "âˆ—Eq\n",
      "\n",
      "[...ì¤‘ëµ...]\n",
      "\n",
      "[ë ë¶€ë¶„]\n",
      " experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ê° ì²­í¬ì˜ ì‹œì‘ ë¶€ë¶„ê³¼ ë ë¶€ë¶„ í™•ì¸ (ì¤‘ë³µ(overlap)ì´ ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ ê´€ì°°)\n",
    "for i, text_chunk in enumerate(recursive_texts[:3]): # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ í™•ì¸\n",
    "    print(f\"--- ì²­í¬ {i+1} (ê¸¸ì´: {len(text_chunk.page_content)}) ---\")\n",
    "    print(\"[ì‹œì‘ ë¶€ë¶„]\")\n",
    "    print(text_chunk.page_content[:200])\n",
    "    print(\"\\n[...ì¤‘ëµ...]\\n\")\n",
    "    print(\"[ë ë¶€ë¶„]\")\n",
    "    print(text_chunk.page_content[-200:])\n",
    "    print(\"=\" * 100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857805cc",
   "metadata": {},
   "source": [
    "### 1-2. ì •ê·œí‘œí˜„ì‹ ì‚¬ìš© (CharacterTextSplitter)\n",
    "\n",
    "- `CharacterTextSplitter`ëŠ” `RecursiveCharacterTextSplitter`ì˜ ê¸°ë°˜ì´ ë˜ëŠ” ë¶„í• ê¸°ì„.\n",
    "- `separator`ë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë°›ìœ¼ë©°, `is_separator_regex=True`ë¡œ ì„¤ì •í•˜ë©´ ì´ `separator`ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í•´ì„í•˜ì—¬ ë¶„í• í•¨.\n",
    "- íŠ¹ì • íŒ¨í„´(ì˜ˆ: ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸, íŠ¹ì • í˜•ì‹ì˜ êµ¬ë¶„ì)ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì •êµí•˜ê²Œ ë‚˜ëˆŒ ë•Œ ìœ ìš©í•¨.\n",
    "\n",
    "**ì£¼ìš” íŒŒë¼ë¯¸í„°:**\n",
    "- `separator`: ë¶„í•  ê¸°ì¤€ì´ ë˜ëŠ” ë¬¸ìì—´ ë˜ëŠ” ì •ê·œí‘œí˜„ì‹.\n",
    "- `is_separator_regex`: `separator`ë¥¼ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë‹¤ë£°ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’ `False`).\n",
    "- `keep_separator`: ë¶„í•  í›„ ê° ì²­í¬ì— êµ¬ë¶„ìë¥¼ ìœ ì§€í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’ `False`).\n",
    "\n",
    "**ì •ê·œí‘œí˜„ì‹ ì˜ˆì‹œ: `(?<=[.!?])\\s+`**\n",
    "- `(?<=[.!?])`: ê¸ì •í˜• í›„ë°©íƒìƒ‰(positive lookbehind)ìœ¼ë¡œ, ë§ˆì¹¨í‘œ(.), ëŠë‚Œí‘œ(!), ë¬¼ìŒí‘œ(?) ì¤‘ í•˜ë‚˜ê°€ ì•ì— ì˜¤ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ìŒ. ì´ ë¬¸ì ìì²´ëŠ” ë¶„í•  ê¸°ì¤€ì— í¬í•¨ë˜ì§€ ì•ŠìŒ (ì¦‰, ì˜ë ¤ë‚˜ê°€ì§€ ì•ŠìŒ).\n",
    "- `\\s+`: í•˜ë‚˜ ì´ìƒì˜ ê³µë°± ë¬¸ìì™€ ì¼ì¹˜í•¨.\n",
    "- ì¦‰, ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” êµ¬ë‘ì (`.`, `!`, `?`) ë’¤ì— ì˜¤ëŠ” í•˜ë‚˜ ì´ìƒì˜ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•¨ (êµ¬ë‘ì ì€ ìœ ì§€ë¨).\n",
    "\n",
    "**ì¥ì :**\n",
    "- ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ ë§¤ìš° ìœ ì—°í•˜ê³  ì •êµí•œ ë¶„í•  ê·œì¹™ì„ ì •ì˜í•  ìˆ˜ ìˆìŒ.\n",
    "- íŠ¹ì • íŒ¨í„´ ê¸°ë°˜ ë¶„í• ì´ í•„ìš”í•  ë•Œ ê°•ë ¥í•œ ë„êµ¬ì„.\n",
    "\n",
    "**ë‹¨ì :**\n",
    "- ì •ê·œí‘œí˜„ì‹ ì‘ì„± ë° ë””ë²„ê¹…ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ.\n",
    "- ë³µì¡í•œ ì •ê·œí‘œí˜„ì‹ì€ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ.\n",
    "- `chunk_size`ë¥¼ ì—„ê²©íˆ ì§€í‚¤ê¸°ë³´ë‹¤ëŠ” ì •ê·œí‘œí˜„ì‹ì— ì˜í•œ ë¶„í• ì„ ìš°ì„ í•¨. ë”°ë¼ì„œ, ë¶„í• ëœ ì¡°ê°ë“¤ì„ `chunk_size`ì— ë§ì¶° í•©ì¹˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ì—¬, ì‹¤ì œ ì²­í¬ í¬ê¸°ê°€ ë§¤ìš° ë¶ˆê· ì¼í•˜ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë©°, `chunk_size`ëŠ” ì‚¬ì‹¤ìƒ ë¶„í• ëœ ë¶€ë¶„ë“¤ì„ í•©ì¹  ë•Œì˜ ìµœëŒ€ í¬ê¸° ì œí•œì²˜ëŸ¼ ë™ì‘í•¨.\n",
    "\n",
    "**ğŸ’¡ íŒ ë° ë…¸í•˜ìš°:**\n",
    "- **ì •ê·œí‘œí˜„ì‹ í…ŒìŠ¤íŠ¸**: ë³µì¡í•œ ì •ê·œí‘œí˜„ì‹ì€ `regex101.com` ê°™ì€ ì˜¨ë¼ì¸ ë„êµ¬ì—ì„œ ì¶©ë¶„íˆ í…ŒìŠ¤íŠ¸í•œ í›„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.\n",
    "- **`keep_separator` í™œìš©**: \n",
    "  - `True`ë¡œ ì„¤ì •í•˜ë©´ ë¶„í•  ê¸°ì¤€ì´ ëœ ë¬¸ìì—´(ì •ê·œí‘œí˜„ì‹ì— ë§¤ì¹­ëœ ë¶€ë¶„)ì„ ê° ì²­í¬ì˜ *ì‹œì‘ ë¶€ë¶„*ì— ìœ ì§€í•¨. ë¬¸ì¥ ë¶„ë¦¬ ì‹œ ë§ˆì¹¨í‘œ ë“±ì„ ìœ ì§€í•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜, ì˜ë„ì™€ ë‹¤ë¥´ê²Œ ë™ì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•´ì•¼ í•¨. (Langchainì˜ `CharacterTextSplitter`ëŠ” `keep_separator=True`ì¼ ë•Œ, êµ¬ë¶„ìë¥¼ ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ì— ë¶™ì´ëŠ” ê²½í–¥ì´ ìˆìŒ. í™•ì¸ í•„ìš”)\n",
    "  - `False`ë¡œ ì„¤ì •í•˜ë©´ êµ¬ë¶„ìëŠ” ì œê±°ë¨. êµ¬ë¶„ì ìì²´ê°€ ë¶ˆí•„ìš”í•˜ê±°ë‚˜, í›„ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë³„ë„ë¡œ ì²˜ë¦¬í•  ë•Œ ìœ ìš©í•¨. ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ ë¶„ë¦¬ í›„ ì•ë’¤ ê³µë°±ì„ ì™„ì „íˆ ì œê±°í•˜ê³  ì‹¶ì„ ë•Œ í™œìš© ê°€ëŠ¥.\n",
    "  - **(ì½”ë“œ ì˜ˆì‹œì˜ `keep_separator=True` ê´€ë ¨)**: ì˜ˆì‹œì˜ ì •ê·œì‹ `(?<=[.!?])\\s+`ì—ì„œ `\\s+`ê°€ êµ¬ë¶„ìì„. `keep_separator=True`ì´ë©´ ì´ ê³µë°±ì´ ë‹¤ìŒ ì²­í¬ ì‹œì‘ì— ë¶™ì„ ìˆ˜ ìˆìŒ. ë§Œì•½ ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸(`.!?`)ë§Œ ë‚¨ê¸°ê³  ê³µë°±ì„ ì œê±°í•˜ê³  ì‹¶ë‹¤ë©´, `keep_separator=False`ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì •ê·œì‹ ìì²´ì—ì„œ ê³µë°±ì„ ì†Œë¹„í•˜ë„ë¡ í•˜ê³  `keep_separator=False`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ëª…í™•í•  ìˆ˜ ìˆìŒ. ë˜ëŠ” ë¶„í•  í›„ ê° ì²­í¬ì— ëŒ€í•´ `.strip()`ì„ ì ìš©í•˜ëŠ” ê²ƒë„ ë°©ë²•ì„."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9117e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# JSONL íŒŒì¼ ë¡œë“œ ì‹œ ê° ê°ì²´ì˜ íŠ¹ì • í•„ë“œë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"sender\"] = record.get(\"sender\")\n",
    "    metadata[\"timestamp\"] = record.get(\"timestamp\")\n",
    "    return metadata\n",
    "\n",
    "jsonl_loader_with_meta = JSONLoader(\n",
    "    file_path=\"../data/kakao_chat.jsonl\", # ë°ì´í„° ê²½ë¡œ ìˆ˜ì •\n",
    "    jq_schema=\".\",                 # ê° JSON ê°ì²´ ì „ì²´ë¥¼ ê°€ì ¸ì˜´\n",
    "    content_key=\"content\",         # 'content' í•„ë“œë¥¼ page_contentë¡œ ì‚¬ìš©\n",
    "    metadata_func=metadata_func,   # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ ì ìš©\n",
    "    json_lines=True,               # JSONL í˜•ì‹ (í•œ ì¤„ì— í•˜ë‚˜ì˜ JSON ê°ì²´)\n",
    ")\n",
    "\n",
    "jsonl_docs_with_meta = jsonl_loader_with_meta.load()\n",
    "json_docs = jsonl_docs_with_meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74d5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: 9\n",
      "ê° ì²­í¬ì˜ ê¸¸ì´ (ì²˜ìŒ 10ê°œ): [31, 9, 15, 7, 11, 11, 27, 13, 13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì¥ì„ êµ¬ë¶„í•˜ì—¬ ë¶„í•  (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë‹¤ìŒì— ê³µë°±ì´ ì˜¤ëŠ” ê²½ìš° ë¬¸ì¥ì˜ ëìœ¼ë¡œ íŒë‹¨)\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter_regex = CharacterTextSplitter(\n",
    "    chunk_size=20, # ì •ê·œì‹ìœ¼ë¡œ ë¶„ë¦¬ëœ ì¡°ê°ë“¤ì„ í•©ì¹  ë•Œì˜ ìµœëŒ€ í¬ê¸°.\n",
    "                   # ì‹¤ì œ ì²­í¬ëŠ” ì´ë³´ë‹¤ í›¨ì”¬ ì‘ì„ ìˆ˜ ìˆìŒ (ì •ê·œì‹ ë¶„í•  ìš°ì„ ).\n",
    "    chunk_overlap=0, # ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• ì´ë¯€ë¡œ ì¤‘ë³µì„ 0ìœ¼ë¡œ ì„¤ì • (ì¼ë°˜ì ).\n",
    "    separator=r'(?<=[.!?])\\s+', # ë¬¸ì¥ ë êµ¬ë‘ì  ë’¤ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ì •ê·œí‘œí˜„ì‹\n",
    "                                # êµ¬ë‘ì ì€ ìœ ì§€ë˜ê³ , ë’¤ë”°ë¥´ëŠ” ê³µë°±ì´ separatorë¡œ ì‚¬ìš©ë¨.\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=False, # êµ¬ë¶„ì(ì—¬ê¸°ì„œëŠ” `\\s+`ì— í•´ë‹¹í•˜ëŠ” ê³µë°±)ë¥¼ ì²­í¬ì— í¬í•¨í•˜ì§€ ì•ŠìŒ.\n",
    "                         # Trueë¡œ í•˜ë©´ ë‹¤ìŒ ì²­í¬ ì‹œì‘ ë¶€ë¶„ì— ê³µë°±ì´ ì¶”ê°€ë  ìˆ˜ ìˆìŒ.\n",
    "                         # Falseë¡œ í•˜ì—¬ ë¬¸ì¥ ë’¤ ê³µë°±ì„ ì œê±°í•˜ëŠ” íš¨ê³¼.\n",
    ")\n",
    "\n",
    "regex_texts = text_splitter_regex.split_documents(json_docs) \n",
    "print(f\"ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜: {len(regex_texts)}\")\n",
    "chunk_lengths_regex = [len(text.page_content) for text in regex_texts]\n",
    "print(f\"ê° ì²­í¬ì˜ ê¸¸ì´ (ì²˜ìŒ 10ê°œ): {chunk_lengths_regex[:10]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ea7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ì •ê·œì‹ ì²­í¬ 1 (ê¸¸ì´: 31) ---\n",
      "ì•ˆë…•í•˜ì„¸ìš” ì—¬ëŸ¬ë¶„, ì˜¤ëŠ˜ íšŒì˜ ì‹œê°„ í™•ì¸ì°¨ ì—°ë½ë“œë¦½ë‹ˆë‹¤.\n",
      "======================================================================\n",
      "\n",
      "--- ì •ê·œì‹ ì²­í¬ 2 (ê¸¸ì´: 9) ---\n",
      "ë„¤, ì•ˆë…•í•˜ì„¸ìš”.\n",
      "======================================================================\n",
      "\n",
      "--- ì •ê·œì‹ ì²­í¬ 3 (ê¸¸ì´: 15) ---\n",
      "ì˜¤í›„ 2ì‹œì— í•˜ê¸°ë¡œ í–ˆì–´ìš”.\n",
      "======================================================================\n",
      "\n",
      "--- ì •ê·œì‹ ì²­í¬ 4 (ê¸¸ì´: 7) ---\n",
      "í™•ì¸í–ˆìŠµë‹ˆë‹¤.\n",
      "======================================================================\n",
      "\n",
      "--- ì •ê·œì‹ ì²­í¬ 5 (ê¸¸ì´: 11) ---\n",
      "íšŒì˜ì‹¤ì€ ì–´ë””ì¸ê°€ìš”?\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ê° ì²­í¬ì˜ ë‚´ìš© í™•ì¸ (ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜ ë¶„í• ë˜ì—ˆëŠ”ì§€)\n",
    "for i, text_chunk in enumerate(regex_texts[:5]): # ì²˜ìŒ 5ê°œ ì²­í¬ë§Œ í™•ì¸\n",
    "    print(f\"--- ì •ê·œì‹ ì²­í¬ {i+1} (ê¸¸ì´: {len(text_chunk.page_content)}) ---\")\n",
    "    print(text_chunk.page_content)\n",
    "    print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effad4d",
   "metadata": {},
   "source": [
    "### 1-3. í† í° ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• \n",
    "\n",
    "- LLMì€ ë‚´ë¶€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ í† í°(token) ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•¨.\n",
    "- ê¸€ì ìˆ˜ë³´ë‹¤ëŠ” í† í° ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ëŠ” ê²ƒì´ LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„.\n",
    "- LangChainì€ ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì €(tiktoken, Hugging Face tokenizers ë“±)ì™€ ì—°ë™í•˜ì—¬ í† í° ê¸°ë°˜ ë¶„í• ì„ ì§€ì›í•¨.\n",
    "\n",
    "**ì¥ì :**\n",
    "- LLMì˜ ì‹¤ì œ ì²˜ë¦¬ ë‹¨ìœ„ì¸ í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ë¯€ë¡œ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ê°€ ìš©ì´í•¨.\n",
    "- ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ëª¨ë¸ì— ìµœì í™”ëœ ë¶„í• ì´ ê°€ëŠ¥í•¨.\n",
    "- ê¸€ì ìˆ˜ ê¸°ë°˜ ë¶„í• ë³´ë‹¤ LLMì˜ ì‹¤ì œ ì…ë ¥ ì œí•œì— ë” ê°€ê¹ê²Œ ì²­í¬ í¬ê¸°ë¥¼ ì œì–´í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "**ë‹¨ì :**\n",
    "- ì–´ë–¤ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠëƒì— ë”°ë¼ ë¶„í•  ê²°ê³¼ì™€ í† í° ìˆ˜ê°€ ë‹¬ë¼ì§.\n",
    "- í† í°í™” ê³¼ì • ìì²´ì— ì•½ê°„ì˜ ì—°ì‚° ë¹„ìš©ì´ ì¶”ê°€ë¨ (ì¼ë°˜ì ìœ¼ë¡œëŠ” ë¯¸ë¯¸í•¨).\n",
    "- í† í° ê²½ê³„ì™€ ì˜ë¯¸ë¡ ì  ê²½ê³„ê°€ í•­ìƒ ì¼ì¹˜í•˜ì§€ëŠ” ì•Šì„ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "**ğŸ’¡ íŒ ë° ë…¸í•˜ìš°:**\n",
    "- **ëª¨ë¸ í˜¸í™˜ì„±**: ì‚¬ìš©í•˜ë ¤ëŠ” LLM ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ê³¼ ë™ì¼í•˜ê±°ë‚˜ í˜¸í™˜ë˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì •í™•í•˜ê³  íš¨ê³¼ì ì„.\n",
    "- **í† í° vs ê¸€ì ìˆ˜**: í† í° ìˆ˜ëŠ” ê¸€ì ìˆ˜ì™€ ì§ì ‘ì ì¸ ë¹„ë¡€ ê´€ê³„ê°€ ì•„ë‹˜. íŠ¹íˆ í•œêµ­ì–´ì™€ ê°™ì´ ë³µì¡í•œ ì–¸ì–´ë‚˜ íŠ¹ìˆ˜ ë¬¸ìê°€ ë§ì€ ê²½ìš°, ê¸€ì ìˆ˜ ê¸°ë°˜ ì˜ˆì¸¡ë³´ë‹¤ ì‹¤ì œ í† í° ìˆ˜ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨.\n",
    "- `RecursiveCharacterTextSplitter.from_tiktoken_encoder()` ë˜ëŠ” `from_huggingface_tokenizer()`ì™€ ê°™ì€ í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ í¸ë¦¬í•˜ê²Œ í† í° ê¸°ë°˜ ë¶„í• ê¸°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65a54d",
   "metadata": {},
   "source": [
    "#### (1) `tiktoken` ì‚¬ìš©\n",
    "- OpenAIì—ì„œ ë§Œë“  BPE (Byte Pair Encoding) ê¸°ë°˜ í† í¬ë‚˜ì´ì € ë¼ì´ë¸ŒëŸ¬ë¦¬ì„.\n",
    "- GPT ì‹œë¦¬ì¦ˆ (ì˜ˆ: gpt-3.5-turbo, gpt-4, text-embedding-ada-002 ë“±) ëª¨ë¸ë“¤ì´ ì‚¬ìš©í•˜ëŠ” í† í°í™” ë°©ì‹ì„ ë”°ë¦„.\n",
    "- `RecursiveCharacterTextSplitter.from_tiktoken_encoder()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ íŠ¹ì • OpenAI ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•  ìˆ˜ ìˆìŒ.\n",
    "  - `encoding_name`: `cl100k_base` (ëŒ€ë¶€ë¶„ì˜ ìµœì‹  OpenAI ëª¨ë¸), `p50k_base` ë“± Tiktoken ì¸ì½”ë”© ì´ë¦„ì„ ì§ì ‘ ì§€ì •í•  ìˆ˜ ìˆìŒ.\n",
    "  - `model_name`: `gpt-4o-mini`, `text-embedding-3-small` ë“± OpenAI ëª¨ë¸ ì´ë¦„ì„ ì§€ì •í•˜ë©´ í•´ë‹¹ ëª¨ë¸ì˜ ê¸°ë³¸ ì¸ì½”ë”©ì„ ì‚¬ìš©í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758cbb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ ì²­í¬ ìˆ˜: 3\n",
      "ê° ì²­í¬ì˜ ê¸€ì ìˆ˜: [1140, 1389, 783]\n",
      "\n",
      "--- ê° ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (Tiktoken) ---\n",
      "\n",
      "--- ì²­í¬ 1 (ê¸€ì ìˆ˜: 1140) ---\n",
      "[ì‹œì‘ ë¶€ë¶„]\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and\n",
      "\n",
      "[...ì¤‘ëµ...]\n",
      "\n",
      "[ë ë¶€ë¶„]\n",
      "w these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "======================================================================\n",
      "\n",
      "--- ì²­í¬ 2 (ê¸€ì ìˆ˜: 1389) ---\n",
      "[ì‹œì‘ ë¶€ë¶„]\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experime\n",
      "\n",
      "[...ì¤‘ëµ...]\n",
      "\n",
      "[ë ë¶€ë¶„]\n",
      "iki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "======================================================================\n",
      "\n",
      "--- ì²­í¬ 3 (ê¸€ì ìˆ˜: 783) ---\n",
      "[ì‹œì‘ ë¶€ë¶„]\n",
      "attention and the parameter-free position representation and became the other person involved in nea\n",
      "\n",
      "[...ì¤‘ëµ...]\n",
      "\n",
      "[ë ë¶€ë¶„]\n",
      "ormation Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tiktokenì„ ì‚¬ìš©í•˜ì—¬ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ëŠ” TextSplitter ìƒì„±\n",
    "text_splitter_tiktoken = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # encoding_name=\"cl100k_base\",  # text-embedding-ada-002, gpt-3.5-turbo, gpt-4 ë“± ìµœì‹  ëª¨ë¸ìš© ì¸ì½”ë”©\n",
    "    model_name=\"gpt-4o-mini\", # ëª¨ë¸ ì´ë¦„ì„ ì§€ì •í•˜ì—¬ í•´ë‹¹ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš© (ë” ê¶Œì¥)\n",
    "    chunk_size=300,  # ê° ì²­í¬ì˜ ìµœëŒ€ í† í° ìˆ˜\n",
    "    chunk_overlap=50, # ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” í† í° ìˆ˜\n",
    "    # separators ë“± RecursiveCharacterTextSplitterì˜ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë„ ì‚¬ìš© ê°€ëŠ¥ (ì˜ˆ: [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"])\n",
    "    # separatorsë¥¼ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ êµ¬ë¶„ì í˜¹ì€ ê¸€ì ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„\n",
    ")\n",
    "\n",
    "# PDF ë¬¸ì„œì˜ ì²« í˜ì´ì§€ë§Œ ë¶„í•  (pdf_docs[:1]) - ì˜ˆì‹œë¥¼ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©\n",
    "chunks_tiktoken = text_splitter_tiktoken.split_documents(pdf_docs[:1])\n",
    "\n",
    "print(f\"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks_tiktoken)}\")\n",
    "chunk_char_lengths_tiktoken = [len(chunk.page_content) for chunk in chunks_tiktoken]\n",
    "print(f\"ê° ì²­í¬ì˜ ê¸€ì ìˆ˜: {chunk_char_lengths_tiktoken}\")\n",
    "print(\"\\n--- ê° ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (Tiktoken) ---\")\n",
    "# ê° ì²­í¬ì˜ ì‹œì‘ ë¶€ë¶„ê³¼ ë ë¶€ë¶„ í™•ì¸\n",
    "for i, chunk in enumerate(chunks_tiktoken[:3]): # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ í™•ì¸\n",
    "    print(f\"\\n--- ì²­í¬ {i+1} (ê¸€ì ìˆ˜: {len(chunk.page_content)}) ---\")\n",
    "    print(\"[ì‹œì‘ ë¶€ë¶„]\")\n",
    "    print(chunk.page_content[:100])\n",
    "    print(\"\\n[...ì¤‘ëµ...]\\n\")\n",
    "    print(\"[ë ë¶€ë¶„]\")\n",
    "    print(chunk.page_content[-100:])\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bab01",
   "metadata": {},
   "source": [
    "**Tiktokenìœ¼ë¡œ ì‹¤ì œ í† í° ìˆ˜ í™•ì¸**\n",
    "\n",
    "ë¶„í• ëœ ê° ì²­í¬ê°€ ì‹¤ì œë¡œ ëª©í‘œí•œ í† í° ìˆ˜ (`chunk_size=300`)ì— ê·¼ì ‘í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  `chunk_overlap`ì´ ì–´ë–»ê²Œ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë´„."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1251af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ê° ì²­í¬ì˜ ì‹¤ì œ í† í° ìˆ˜ (Tiktoken gpt-4o-mini) ---\n",
      "ì²­í¬ 1: 275 í† í°\n",
      "ì²­í¬ 2: 287 í† í°\n",
      "ì²­í¬ 3: 164 í† í°\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# text_splitter_tiktokenì—ì„œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ ì¸ì½”ë”©/ëª¨ë¸ì„ ì§€ì •í•´ì•¼ ì •í™•í•¨.\n",
    "# tokenizer = tiktoken.get_encoding(\"cl100k_base\") # encoding_name ì‚¬ìš© ì‹œ\n",
    "tokenizer_gpt4omini = tiktoken.encoding_for_model(\"gpt-4o-mini\") # model_name ì‚¬ìš© ì‹œ\n",
    "\n",
    "print(\"--- ê° ì²­í¬ì˜ ì‹¤ì œ í† í° ìˆ˜ (Tiktoken gpt-4o-mini) ---\")\n",
    "for i, chunk in enumerate(chunks_tiktoken[:5]): # ì²˜ìŒ 5ê°œ ì²­í¬ í™•ì¸\n",
    "    tokens = tokenizer_gpt4omini.encode(chunk.page_content)\n",
    "    print(f\"ì²­í¬ {i+1}: {len(tokens)} í† í°\")\n",
    "    # print(f\"  ì²« 10ê°œ í† í° ID: {tokens[:10]}\")\n",
    "    # token_strings = [tokenizer_gpt4omini.decode([token]) for token in tokens[:10]]\n",
    "    # print(f\"  ì²« 10ê°œ í† í° ë¬¸ìì—´: {token_strings}\")\n",
    "    # print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b0c68",
   "metadata": {},
   "source": [
    "#### (2) Hugging Face í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
    "- Hugging Face `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.\n",
    "- `RecursiveCharacterTextSplitter.from_huggingface_tokenizer()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•¨.\n",
    "  - `tokenizer`: Hugging Face `transformers.PreTrainedTokenizerBase`ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§ì ‘ ì „ë‹¬í•¨.\n",
    "- **ì¥ì **: OpenAI ëª¨ë¸ ì™¸ì— ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM(ì˜ˆ: Llama, Mistral, í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ë“±)ì— ë§ëŠ” í† í°í™” ë°©ì‹ì„ ì ìš©í•  ìˆ˜ ìˆìŒ.\n",
    "- **ë‹¨ì **: í•´ë‹¹ í† í¬ë‚˜ì´ì €ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•´ì•¼ í•˜ë©°, ëª¨ë¸ì— ë”°ë¼ í† í°í™” ë°©ì‹ì´ ë§¤ìš° ë‹¤ì–‘í•  ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0f4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaTokenizerFast(name_or_path='BAAI/bge-m3', vocab_size=250002, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ì˜ˆì‹œ: BAAI/bge-m3 ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ë¡œë“œ (ì£¼ë¡œ ì„ë² ë”© ëª¨ë¸ë¡œ ì‚¬ìš©ë¨)\n",
    "# ì‹¤ì œ ì‚¬ìš©í•  LLMì´ë‚˜ ì„ë² ë”© ëª¨ë¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì„ íƒí•´ì•¼ í•¨.\n",
    "hf_tokenizer_bge_m3 = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "print(hf_tokenizer_bge_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cbfade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.' -> í† í° ID: [0, 107687, 5, 20451, 54272, 16367, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face í† í¬ë‚˜ì´ì € ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (í•œêµ­ì–´ ì˜ˆì‹œ)\n",
    "sample_text_ko = \"ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.\"\n",
    "tokens_hf_bge_m3 = hf_tokenizer_bge_m3.encode(sample_text_ko)\n",
    "print(f\"'{sample_text_ko}' -> í† í° ID: {tokens_hf_bge_m3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a2a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í° ë¬¸ìì—´: ['<s>', 'â–ì•ˆë…•í•˜ì„¸ìš”', '.', 'â–ë°˜', 'ê°‘', 'ìŠµë‹ˆë‹¤', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# í† í° IDë¥¼ ì‹¤ì œ í† í° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í™•ì¸\n",
    "print(f\"í† í° ë¬¸ìì—´: {hf_tokenizer_bge_m3.convert_ids_to_tokens(tokens_hf_bge_m3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4992d5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: <s> ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.</s>\n"
     ]
    }
   ],
   "source": [
    "# í† í° IDë¥¼ ë‹¤ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬í•œ í˜•íƒœë¡œ ë””ì½”ë”©\n",
    "print(f\"ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: {hf_tokenizer_bge_m3.decode(tokens_hf_bge_m3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680c8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Hugging Face í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ëŠ” TextSplitter ìƒì„±\n",
    "text_splitter_hf = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=hf_tokenizer_bge_m3, # ë¯¸ë¦¬ ë¡œë“œí•œ Hugging Face í† í¬ë‚˜ì´ì € ì „ë‹¬\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50,\n",
    "    # separators ë“± RecursiveCharacterTextSplitterì˜ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë„ ì‚¬ìš© ê°€ëŠ¥\n",
    ")\n",
    "\n",
    "# PDF ë¬¸ì„œì˜ ì²« í˜ì´ì§€ë§Œ ë¶„í•  (pdf_docs[:1]) - ì˜ˆì‹œë¥¼ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©\n",
    "chunks_hf = text_splitter_hf.split_documents(pdf_docs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b81e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ ì²­í¬ ìˆ˜: 3\n",
      "ê° ì²­í¬ì˜ ê¸€ì ìˆ˜: [1214, 1307, 783]\n",
      "\n",
      "--- ê° ì²­í¬ì˜ ì‹¤ì œ í† í° ìˆ˜ (HuggingFace BAAI/bge-m3) ---\n",
      "ì²­í¬ 1: 301 í† í°\n",
      "ì²­í¬ 2: 293 í† í°\n",
      "ì²­í¬ 3: 181 í† í°\n"
     ]
    }
   ],
   "source": [
    "print(f\"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks_hf)}\")\n",
    "chunk_char_lengths_hf = [len(chunk.page_content) for chunk in chunks_hf]\n",
    "print(f\"ê° ì²­í¬ì˜ ê¸€ì ìˆ˜: {chunk_char_lengths_hf}\")\n",
    "print()\n",
    "\n",
    "print(\"--- ê° ì²­í¬ì˜ ì‹¤ì œ í† í° ìˆ˜ (HuggingFace BAAI/bge-m3) ---\")\n",
    "for i, chunk in enumerate(chunks_hf[:5]): # ì²˜ìŒ 5ê°œ ì²­í¬ í™•ì¸\n",
    "    # .encode()ëŠ” í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨. ê¸¸ì´ë¥¼ í†µí•´ í† í° ìˆ˜ í™•ì¸.\n",
    "    tokens = hf_tokenizer_bge_m3.encode(chunk.page_content)\n",
    "    print(f\"ì²­í¬ {i+1}: {len(tokens)} í† í°\")\n",
    "    # print(f\"  ì²« 10ê°œ í† í° ID: {tokens[:10]}\")\n",
    "    # token_strings = hf_tokenizer_bge_m3.convert_ids_to_tokens(tokens[:10]) \n",
    "    # print(f\"  ì²« 10ê°œ í† í° ë¬¸ìì—´: {token_strings}\")\n",
    "    # print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74b5cb",
   "metadata": {},
   "source": [
    "### 1-4. Semantic Chunking (ì‹œë§¨í‹± ì²­í‚¹)\n",
    "\n",
    "- `SemanticChunker`ëŠ” ê³ ì •ëœ í¬ê¸°ë‚˜ ê·œì¹™ ê¸°ë°˜ì´ ì•„ë‹Œ, ë¬¸ì¥ ê°„ì˜ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•¨.\n",
    "- ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì¥ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ê³„ì‚°í•˜ê³ , ì¸ì ‘í•œ ë¬¸ì¥ë“¤ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•¨. ì´ ìœ ì‚¬ë„ê°€ íŠ¹ì • ì„ê³„ê°’(breakpoint)ì„ ê¸°ì¤€ìœ¼ë¡œ í¬ê²Œ ë³€í•˜ëŠ” ì§€ì ì—ì„œ ì²­í¬ë¥¼ ë‚˜ëˆ”. ì¦‰, ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë¬¶ìœ¼ë ¤ëŠ” ì‹œë„ì„.\n",
    "\n",
    "**ì£¼ìš” íŒŒë¼ë¯¸í„°:**\n",
    "- `embeddings`: ë¬¸ì¥ì˜ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (ì˜ˆ: `OpenAIEmbeddings`, `HuggingFaceEmbeddings`).\n",
    "- `breakpoint_threshold_type`: ìœ ì‚¬ë„ ë³€í™”ì˜ ê¸°ì¤€ì ì„ ì •í•˜ëŠ” ë°©ì‹.\n",
    "  - `\"percentile\"` (ê¸°ë³¸ê°’): ìœ ì‚¬ë„ ë¶„í¬ì˜ íŠ¹ì • ë°±ë¶„ìœ„ìˆ˜ë¥¼ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©í•¨.\n",
    "  - `\"standard_deviation\"`: í‰ê· ì—ì„œ í‘œì¤€í¸ì°¨ì˜ íŠ¹ì • ë°°ìˆ˜ë§Œí¼ ë–¨ì–´ì§„ ì§€ì ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©í•¨.\n",
    "  - `\"gradient\"`: ìœ ì‚¬ë„ ê°’ì˜ ë³€í™”ìœ¨(ê¸°ìš¸ê¸°)ì´ ê¸‰ê²©íˆ ë³€í•˜ëŠ” ì§€ì ì„ ì°¾ìœ¼ë©°, ë¬¸ë§¥ ì „í™˜ì„ ë” ì˜ ê°ì§€í•  ìˆ˜ ìˆìŒ.\n",
    "  - `\"interquartile\"`: ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¤€ì ì„ ì„¤ì •í•¨.\n",
    "- `breakpoint_threshold_amount` (ë˜ëŠ” `percentile_threshold`, `threshold` ë“± íƒ€ì…ì— ë”°ë¼ ë‹¤ë¦„): ê¸°ì¤€ì  íƒ€ì…ì— ë”°ë¥¸ êµ¬ì²´ì ì¸ ê°’.\n",
    "\n",
    "**ì¥ì :**\n",
    "- ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì‘ì§‘ë ¥ ìˆëŠ” ì²­í¬ë¥¼ ìƒì„±í•˜ì—¬ RAGì˜ ê²€ìƒ‰ í’ˆì§ˆ ë° ë‹µë³€ ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚¬ ì ì¬ë ¥ì´ ìˆìŒ.\n",
    "- ê³ ì • í¬ê¸° ë¶„í• ë³´ë‹¤ ë¬¸ë§¥ ìœ ì§€ê°€ ì˜ ë  ìˆ˜ ìˆìœ¼ë©°, ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë¬¶ì–´ì¤Œ.\n",
    "- ë¬¸ë§¥ì´ ê°‘ìê¸° ë°”ë€ŒëŠ” ë¶€ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ë¯€ë¡œ, ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆìŒ.\n",
    "\n",
    "**ë‹¨ì :**\n",
    "- **ì‹¤í—˜ì  ê¸°ëŠ¥**: LangChain ë‚´ì—ì„œ ì•„ì§ ì‹¤í—˜ì ì¸ ê¸°ëŠ¥ìœ¼ë¡œ ë¶„ë¥˜ë  ìˆ˜ ìˆìœ¼ë©°, APIë‚˜ ë™ì‘ ë°©ì‹ì´ ë³€ê²½ë  ìˆ˜ ìˆìŒ. (í˜„ì¬ëŠ” `langchain_experimental`ì— ìœ„ì¹˜)\n",
    "- **ê³„ì‚° ë¹„ìš©**: ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•´ ì„ë² ë”©ì„ ê³„ì‚°í•˜ê³  ìœ ì‚¬ë„ë¥¼ ë¹„êµí•´ì•¼ í•˜ë¯€ë¡œ, ë‹¤ë¥¸ ë¶„í•  ë°©ì‹ë³´ë‹¤ ì—°ì‚°ëŸ‰ì´ ë§ê³  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ.\n",
    "- ì„ë² ë”© ëª¨ë¸ì˜ í’ˆì§ˆê³¼ ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ë¶„í•  ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.\n",
    "- ìµœì ì˜ `breakpoint_threshold_type`ê³¼ ê´€ë ¨ ê°’ì„ ì°¾ê¸° ìœ„í•œ ì‹¤í—˜ì´ í•„ìš”í•˜ë©°, ë°ì´í„°ì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ.\n",
    "- ì²­í¬ì˜ í¬ê¸°ê°€ ë§¤ìš° ê°€ë³€ì ì¼ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "**ğŸ’¡ íŒ ë° ë…¸í•˜ìš°:**\n",
    "- **ì„ë² ë”© ëª¨ë¸ ì„ íƒ**: ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ì–¸ì–´ì— ì í•©í•œ ê³ í’ˆì§ˆ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨. í•œêµ­ì–´ ë¬¸ì„œë¼ë©´ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•´ì•¼ í•¨.\n",
    "- **ì„ê³„ê°’ íŠœë‹**: `breakpoint_threshold_type`ê³¼ ê´€ë ¨ ê°’ì€ ë°ì´í„°ì…‹ì— ë”°ë¼ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì—¬ëŸ¬ ì˜µì…˜ì„ í…ŒìŠ¤íŠ¸í•´ë³´ëŠ” ê²ƒì´ ì¢‹ìŒ. 'gradient' ë°©ì‹ì´ ì˜ë¯¸ ë³€í™”ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ëŠ” ê²½í–¥ì´ ìˆì–´ ì‹œì‘ì ìœ¼ë¡œ ê³ ë ¤í•´ë³¼ ë§Œí•¨.\n",
    "- **ì‚¬ì „ ë¬¸ì¥ ë¶„ë¦¬**: `SemanticChunker`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¨¼ì € ë‚˜ëˆ”. ë”°ë¼ì„œ ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì ì ˆí•œ ë¬¸ì¥ ë¶„ë¦¬(sentence splitting)ê°€ ì„ í–‰ë˜ê±°ë‚˜, `SemanticChunker`ê°€ ì‚¬ìš©í•˜ëŠ” ë¬¸ì¥ ë¶„ë¦¬ ë¡œì§ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨.\n",
    "- **í™œìš© ë¶„ì•¼**: ì˜ë¯¸ë¡ ì  ì¼ê´€ì„±ì´ ë§¤ìš° ì¤‘ìš”í•œ ê³ ê¸‰ RAG ì‹œìŠ¤í…œì´ë‚˜, íŠ¹ì • ë„ë©”ì¸ ë¬¸ì„œ(ì˜ˆ: ë²•ë¥  ë¬¸ì„œ, ì—°êµ¬ ë…¼ë¬¸)ì˜ ì‹¬ì¸µ ë¶„ì„ì— ìœ ìš©í•  ìˆ˜ ìˆìŒ. ë‹¤ë§Œ, ê³„ì‚° ë¹„ìš©ê³¼ ì‹¤í—˜ì  ì„±ê²©ì„ ê³ ë ¤í•´ì•¼ í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6e996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523f8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# SemanticChunkerëŠ” ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•¨\n",
    "# OpenAIEmbeddings ì™¸ì—ë„ HuggingFaceEmbeddings ë“± ì‚¬ìš© ê°€ëŠ¥\n",
    "text_splitter_semantic = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), # OpenAI ì„ë² ë”© ì‚¬ìš© (API í‚¤ í•„ìš”)\n",
    "    breakpoint_threshold_type=\"gradient\",  # ê¸°ì¤€ì  íƒ€ì…: ê¸°ìš¸ê¸° ë³€í™” ê°ì§€ (ë¬¸ë§¥ ë³€í™”ì— ë¯¼ê°)\n",
    "    # breakpoint_threshold_type=\"percentile\", percentile_threshold=95, # ì˜ˆ: ìƒìœ„ 5% ë³€í™” ì§€ì \n",
    "    # breakpoint_threshold_type=\"standard_deviation\", breakpoint_threshold_amount=2, # ì˜ˆ: í‘œì¤€í¸ì°¨ 2ë°° ì´ìƒ ë³€í™”\n",
    "    # add_start_index=True # ë©”íƒ€ë°ì´í„°ì— ì²­í¬ ì‹œì‘ ì¸ë±ìŠ¤ ì¶”ê°€ ì—¬ë¶€ (í† í° ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì¬ì¡°í•©í•  ë•Œ ìœ ìš©)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d2e9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒì„±ëœ ì²­í¬ ìˆ˜ (SemanticChunker): 2\n",
      "ê° ì²­í¬ì˜ ê¸€ì ìˆ˜: [1736, 1116]\n",
      "\n",
      "--- ê° ì‹œë§¨í‹± ì²­í¬ ë¯¸ë¦¬ë³´ê¸° ë° í† í° ìˆ˜ (ì°¸ê³ ìš©) ---\n",
      "\n",
      "--- ì‹œë§¨í‹± ì²­í¬ 1 (ê¸€ì ìˆ˜: 1736, í† í° ìˆ˜: 415) ---\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works. Attention Is All You Need\n",
      "\n",
      "[...]\n",
      "======================================================================\n",
      "\n",
      "--- ì‹œë§¨í‹± ì²­í¬ 2 (ê¸€ì ìˆ˜: 1116, í† í° ìˆ˜: 235) ---\n",
      "âˆ—Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra\n",
      "[...]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# PDF ë¬¸ì„œì˜ ì²« í˜ì´ì§€ë§Œ ë¶„í•  (pdf_docs[:1]) - ì˜ˆì‹œë¥¼ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©\n",
    "# ì£¼ì˜: SemanticChunkerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ì„ë² ë”©ì„ ê³„ì‚°í•˜ë¯€ë¡œ, \n",
    "# ì…ë ¥ ë¬¸ì„œê°€ í´ ê²½ìš° ìƒë‹¹í•œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "chunks_semantic = text_splitter_semantic.split_documents(pdf_docs[:1])\n",
    "\n",
    "print(f\"ìƒì„±ëœ ì²­í¬ ìˆ˜ (SemanticChunker): {len(chunks_semantic)}\")\n",
    "chunk_char_lengths_semantic = [len(chunk.page_content) for chunk in chunks_semantic]\n",
    "print(f\"ê° ì²­í¬ì˜ ê¸€ì ìˆ˜: {chunk_char_lengths_semantic}\")\n",
    "print()\n",
    "\n",
    "# Tiktoken í† í¬ë‚˜ì´ì €ë¡œ ê° ì‹œë§¨í‹± ì²­í¬ì˜ í† í° ìˆ˜ í™•ì¸ (ì°¸ê³ ìš©)\n",
    "# SemanticChunkerê°€ ì‚¬ìš©í•œ ì„ë² ë”© ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¨ìˆœ ì°¸ê³ ìš©ì„.\n",
    "tokenizer_for_semantic_check = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "print(\"--- ê° ì‹œë§¨í‹± ì²­í¬ ë¯¸ë¦¬ë³´ê¸° ë° í† í° ìˆ˜ (ì°¸ê³ ìš©) ---\")\n",
    "for i, chunk in enumerate(chunks_semantic[:5]): # ì²˜ìŒ 5ê°œ ì²­í¬ í™•ì¸\n",
    "    tokens = tokenizer_for_semantic_check.encode(chunk.page_content)\n",
    "    print(f\"\\n--- ì‹œë§¨í‹± ì²­í¬ {i+1} (ê¸€ì ìˆ˜: {len(chunk.page_content)}, í† í° ìˆ˜: {len(tokens)}) ---\")\n",
    "    print(chunk.page_content[:200]) # ë‚´ìš© ì¼ë¶€ ì¶œë ¥\n",
    "    if len(chunk.page_content) > 200:\n",
    "        print(\"[...]\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
