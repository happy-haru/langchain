{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) API 키 로딩 (dotenv)\n",
    "필수! API 키 같은 민감 정보는 `.env` 파일로 관리하고, `load_dotenv()`로 불러옴. 깔끔함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6d77f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pprint import pprint # 결과 보기 좋게 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 벡터 저장소 로드\n",
    "미리 만들어둔 ChromaDB 벡터 저장소에서 임베딩된 문서들 불러옴. RAG의 핵심 재료임.\n",
    "- 임베딩 모델: `text-embedding-3-small` (가성비 좋음)\n",
    "- 컬렉션 이름: `chroma_test`\n",
    "- 저장 경로: `./chroma_db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0562285f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_chroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-3-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[0;32m      9\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m     10\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma_test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chroma_db\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# 미리 저장해둔 DB 경로\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m벡터 저장소에 저장된 문서 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorstore\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\lang_chain_1\\Lib\\site-packages\\pydantic\\v1\\main.py:347\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    345\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", \n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"chroma_test\",\n",
    "    persist_directory=\"./chroma_db\", # 미리 저장해둔 DB 경로\n",
    "    )\n",
    "\n",
    "print(f\"벡터 저장소에 저장된 문서 수: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LCEL의 힘: 손쉽게 체인 만들기\n",
    "LangChain Expression Language (LCEL)은 파이프(`|`) 연산자를 사용해 다양한 컴포넌트(프롬프트, 모델, 파서 등)를 유연하게 연결할 수 있게 해줌. 코드 가독성 UP, 개발 속도 UP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 프롬프트 + LLM: 기본 중의 기본\n",
    "가장 기본적인 체인 구성. 프롬프트 템플릿을 만들고 LLM과 연결함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6650c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.3, # 답변의 창의성 조절 (낮을수록 결정적)\n",
    "    max_tokens=100, # 최대 답변 길이\n",
    "    )\n",
    "\n",
    "# 프롬프트 메시지 리스트 정의 (시스템 메시지, 사용자 메시지)\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"), # 시스템 역할 부여\n",
    "    (\"user\", \"{query}\"), # 사용자 질문 템플릿\n",
    "]\n",
    "\n",
    "# 메시지 리스트로부터 ChatPromptTemplate 생성\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# 생성된 프롬프트 템플릿 구조 확인\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff39eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query']\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 템플릿이 어떤 입력 변수를 사용하는지 확인\n",
    "print(prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771e1e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant.\n",
      "Human: 테슬라 창업자는 누구인가요?\n"
     ]
    }
   ],
   "source": [
    "# 템플릿에 실제 값(`query`)을 넣어 프롬프트 텍스트 완성 (렌더링)\n",
    "prompt_text = prompt.format(query=\"테슬라 창업자는 누구인가요?\")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0e9bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테슬라의 창립자는 엘론 머스크(Elon Musk), 마틴 에버하르드(Martin Eberhard), 마크 타페닝(Mark Tarpenning), 제프 스프레처(JB Straubel), 이안 라이트(Ian Wright) 등입니다. 테슬라는 2003년에 설립되었으며, 엘론 머스크는 2004년에 투자자로 참여한 후 CEO로서 회사를 이끌고\n"
     ]
    }
   ],
   "source": [
    "# 완성된 프롬프트 텍스트를 LLM에 직접 입력하여 응답 받기 (LCEL 체인 사용 전)\n",
    "response_from_llm_direct = llm.invoke(prompt_text)\n",
    "\n",
    "# LLM 응답(AIMessage 객체)에서 내용(content)만 추출하여 출력\n",
    "print(response_from_llm_direct.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27045656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))]) last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1338c8710>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x133873dd0>, root_client=<openai.OpenAI object at 0x13388b790>, root_async_client=<openai.AsyncOpenAI object at 0x1338c8850>, model_name='gpt-4o-mini', temperature=0.3, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=100)\n"
     ]
    }
   ],
   "source": [
    "# LCEL을 사용한 체인 구성: 프롬프트와 LLM을 `|` (파이프) 연산자로 연결. 이게 핵심!\n",
    "chain = prompt | llm\n",
    "\n",
    "# 구성된 체인 정보 출력 (어떤 컴포넌트들이 연결되었는지 보여줌)\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a65c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'query': {'title': 'Query', 'type': 'string'}},\n",
      " 'required': ['query'],\n",
      " 'title': 'PromptInput',\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# 체인의 입력 스키마 확인 (어떤 입력을 받는지 JSON 스키마 형태로 보여줌)\n",
    "pprint(chain.input_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b371cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테슬라의 창립자는 엘론 머스크(Elon Musk)입니다. 그러나 테슬라는 2003년에 마틴 에버하드(Martin Eberhard)와 마크 타페닝(Mark Tarpenning)에 의해 설립되었습니다. 엘론 머스크는 2004년에 투자자로 참여한 후, CEO로 취임하고 회사의 비전을 이끌어가면서 테슬라의 주요 인물로 자리잡게 되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 체인 실행 방법 1: 딕셔너리 형태로 입력 (입력 변수 이름을 키로 사용)\n",
    "response_from_chain_dict = chain.invoke({\"query\":\"테슬라 창업자는 누구인가요?\"})\n",
    "\n",
    "# 체인 응답(AIMessage 객체)의 내용 출력\n",
    "print(response_from_chain_dict.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68e91772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테슬라의 창립자는 엘론 머스크(Elon Musk), 마틴 에버하드(Martin Eberhard), 마크 타페닝(Mark Tarpenning), 제프 스프레처(Jeffrey B. Straubel), 이안 라이트(Ian Wright) 등 여러 명이 있습니다. 그러나 엘론 머스크가 2004년에 회사에 투자하고 CEO로 취임한 이후, 테슬라의 얼굴이자 가장 잘\n"
     ]
    }
   ],
   "source": [
    "# 체인 실행 방법 2: 입력 변수가 하나일 경우, 문자열로 직접 입력 가능 (간편함)\n",
    "response_from_chain_str = chain.invoke(\"테슬라 창업자는 누구인가요?\") # 이 response_from_chain_str은 아래 Output Parser에서 사용됨\n",
    "\n",
    "# 체인 응답(AIMessage 객체)의 내용 출력\n",
    "print(response_from_chain_str.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 출력 파서: 원하는 형태로 결과 받기\n",
    "LLM의 응답(주로 AIMessage 객체)을 우리가 원하는 포맷(문자열, JSON 등)으로 변환해주는 역할. 체인의 마지막에 연결함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) 문자열 파싱 (StrOutputParser)\n",
    "LLM 응답(AIMessage 객체)에서 실제 텍스트 내용만 깔끔하게 뽑아줌. 제일 흔하게 씀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8de35b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='테슬라의 창립자는 엘론 머스크(Elon Musk), 마틴 에버하드(Martin Eberhard), 마크 타페닝(Mark Tarpenning), 제프 스프레처(Jeffrey B. Straubel), 이안 라이트(Ian Wright) 등 여러 명이 있습니다. 그러나 엘론 머스크가 2004년에 회사에 투자하고 CEO로 취임한 이후, 테슬라의 얼굴이자 가장 잘', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 27, 'total_tokens': 127, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'length', 'logprobs': None}, id='run-71ae890c-1298-463f-9ea0-9200e151e8ac-0', usage_metadata={'input_tokens': 27, 'output_tokens': 100, 'total_tokens': 127})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 셀에서 실행한 체인의 결과 (AIMessage 객체)\n",
    "response_from_chain_str # 이 변수는 위에서 `chain.invoke(\"테슬라 창업자는 누구인가요?\")`로 얻은 AIMessage 객체임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a73a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'테슬라의 창립자는 엘론 머스크(Elon Musk), 마틴 에버하드(Martin Eberhard), 마크 타페닝(Mark Tarpenning), 제프 스프레처(Jeffrey B. Straubel), 이안 라이트(Ian Wright) 등 여러 명이 있습니다. 그러나 엘론 머스크가 2004년에 회사에 투자하고 CEO로 취임한 이후, 테슬라의 얼굴이자 가장 잘'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# AIMessage 객체를 StrOutputParser에 통과시키면 문자열 내용만 반환됨\n",
    "output_parser.invoke(response_from_chain_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2ab22fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리비안(Rivian)은 2009년에 설립되었습니다. 이 회사는 전기차를 개발하고 생산하는 미국의 자동차 제조업체로, 특히 전기 픽업트럭과 SUV에 주력하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 체인에 StrOutputParser 연결: prompt | llm | output_parser\n",
    "str_chain = prompt | llm  | output_parser\n",
    "\n",
    "query = \"리비안의 설립년도는 언제인가요?\"\n",
    "str_response = str_chain.invoke(query)\n",
    "\n",
    "print(str_response)\n",
    "print(type(str_response)) # 타입이 문자열(str)인지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) JSON 출력 (JsonOutputParser)\n",
    "LLM이 JSON 형식 문자열을 주면, 이걸 파이썬 딕셔너리로 변환해줌. LLM에게 JSON으로 달라고 요청해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea62a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n  \"회사\": \"테슬라\",\\n  \"창업자\": [\\n    {\\n      \"이름\": \"마틴 에버하드\",\\n      \"역할\": \"공동 창립자\"\\n    },\\n    {\\n      \"이름\": \"마크 타페닝\",\\n      \"역할\": \"공동 창립자\"\\n    },\\n    {\\n      \"이름\": \"엘론 머스크\",\\n      \"역할\": \"투자자 및' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 34, 'total_tokens': 134, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'length', 'logprobs': None} id='run-24a84e04-6a83-4f37-b726-6305e6014d8e-0' usage_metadata={'input_tokens': 34, 'output_tokens': 100, 'total_tokens': 134}\n",
      "{'회사': '테슬라', '창업자': [{'이름': '마틴 에버하드', '역할': '공동 창립자'}, {'이름': '마크 타페닝', '역할': '공동 창립자'}, {'이름': '엘론 머스크', '역할': '투자자 및'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "# 기본 체인 (prompt | llm)을 사용. LLM에게 JSON 형식으로 출력하라고 요청함.\n",
    "json_response_from_llm = chain.invoke(\"테슬라 창업자는 누구인가요? JSON 형식으로 출력해주세요.\") \n",
    "print(json_response_from_llm) # AIMessage 객체, content 안에 JSON 문자열이 들어있음\n",
    "\n",
    "# JsonOutputParser로 AIMessage의 content (JSON 문자열)를 파싱하여 파이썬 딕셔너리로 변환\n",
    "json_parser_output = json_parser.invoke(json_response_from_llm)\n",
    "print(json_parser_output)\n",
    "print(type(json_parser_output)) # 타입이 딕셔너리(dict)인지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) 스키마 기반 파싱 (PydanticOutputParser)\n",
    "Pydantic 모델로 원하는 출력 구조를 정의하고, LLM이 그 구조에 맞게 출력하도록 유도함. `get_format_instructions()`로 LLM에게 가이드라인 전달. 복잡한 데이터 받을 때 안정적임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3ec0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "PydanticOutputParser 프롬프트 가이드라인:\n",
      "----------------------------------------\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"title\": {\"title\": \"Title\", \"description\": \"The title or position of the person.\", \"type\": \"string\"}}, \"required\": [\"name\", \"title\"]}\n",
      "```\n",
      "========================================\n",
      "최종 프롬프트 템플릿 (가이드라인 포함):\n",
      "----------------------------------------\n",
      "System: Answer the user query. Wrap the output in `json` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"title\": {\"title\": \"Title\", \"description\": \"The title or position of the person.\", \"type\": \"string\"}}, \"required\": [\"name\", \"title\"]}\n",
      "```\n",
      "Human: 테슬라 창업자는 누구인가요?\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # LangChain은 pydantic_v1을 내부적으로 사용\n",
    "\n",
    "# Pydantic 모델 정의: 원하는 출력 스키마를 클래스로 명시\n",
    "class Person(BaseModel):\n",
    "    \"\"\"사람에 대한 정보.\"\"\"\n",
    "    name: str = Field(..., description=\"그 사람의 이름\")\n",
    "    title: str = Field(..., description=\"그 사람의 직함 또는 직책.\")\n",
    "\n",
    "# PydanticOutputParser 생성 (정의한 모델을 인자로 전달)\n",
    "person_parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"PydanticOutputParser 프롬프트 가이드라인:\")\n",
    "print(\"----------------------------------------\")\n",
    "# LLM에게 어떤 형식으로 출력해야 하는지 알려주는 가이드라인 생성\n",
    "format_instructions = person_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "print(\"========================================\")\n",
    "\n",
    "# 새로운 프롬프트 템플릿 생성 (시스템 메시지에 format_instructions 포함)\n",
    "pydantic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"사용자 질문에 답하세요. 출력은 `json` 태그로 감싸주세요.\\n{format_instructions}\", # 여기에 가이드라인 삽입\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=format_instructions) # .partial로 format_instructions 값을 미리 채워둠\n",
    "\n",
    "print(\"최종 프롬프트 템플릿 (가이드라인 포함):\")\n",
    "print(\"----------------------------------------\")\n",
    "print(pydantic_prompt.format(query=\"테슬라 창업자는 누구인가요?\"))\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b8a57e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='엘론 머스크', title='CEO 및 창립자')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PydanticOutputParser를 포함한 체인 구성\n",
    "person_chain = pydantic_prompt | llm | person_parser\n",
    "\n",
    "# 체인 실행\n",
    "pydantic_response = person_chain.invoke({\"query\":\"테슬라 창업자는 누구인가요?\"})\n",
    "\n",
    "# 체인 응답 출력 (Pydantic 모델 객체로 반환됨)\n",
    "pydantic_response\n",
    "print(type(pydantic_response)) # 타입이 Person 클래스 객체인지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM 호출, 다양하게 활용하기\n",
    "LLM 객체는 `invoke` 외에도 `stream`, `batch` 등 유용한 호출 방식을 제공함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) stream: 실시간 응답 스트리밍\n",
    "답변을 한 번에 다 받는 게 아니라, 생성되는 대로 토큰 단위로 바로바로 받아볼 수 있음. 사용자 경험(UX)에 좋음. `flush=True`로 즉시 출력!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b87c23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테슬라의 창립자는 엘론 머스크(Elon Musk)입니다. 그러나 테슬라는 2003년에 마틴 에버하드(Martin Eberhard)와 마크 타페닝(Mark Tarpenning)에 의해 설립되었습니다. 엘론 머스크는 2004년에 테슬라에 투자하고 이후 CEO로 취임하면서 회사의 성장에 큰 영향을 미쳤습니다."
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "print(\"스트리밍 응답 시작:\")\n",
    "for chunk in llm.stream(\"테슬라 창업자는 누구인가요?\"): # 체인이 아닌 llm 객체 자체의 stream 사용\n",
    "    # chunk는 AIMessageChunk 객체. content 속성에 토큰이 들어있음\n",
    "    print(chunk.content, end=\"\", flush=True)  \n",
    "    # time.sleep(0.05) # 너무 빠르면 눈으로 보기 힘드니 약간의 딜레이 (선택 사항)\n",
    "print(\"\\n스트리밍 응답 종료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) batch: 여러 질문 한 번에 처리\n",
    "질문 여러 개를 리스트로 묶어서 한 방에 처리함. API 호출을 효율적으로 관리할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1477bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "테슬라의 창립자는 마틴 에버하드(Martin Eberhard)와 마크 타페닝(Mark Tarpenning)입니다. 이 두 사람은 2003년에 테슬라 모터스를 설립했습니다. 이후 일론 머스크(Elon Musk)가 2004년에 투자자로 참여하면서 회사의 주요 인물 중 하나가 되었고, 이후 CEO로 취임하여 회사의 성장에 큰 영향을 미쳤습니다.\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "리비안(Rivian)의 창업자는 RJ 스칼링(RJ Scaringe)입니다. 그는 2009년에 리비안을 설립하였으며, 전기차 및 친환경 차량 개발에 주력하고 있습니다. 리비안은 전기 픽업트럭인 R1T와 전기 SUV인 R1S를 출시하여 주목받고 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"테슬라의 창업자는 누구인가요?\",\n",
    "    \"리비안의 창업자는 누구인가요?\",\n",
    "]\n",
    "\n",
    "# 여러 질문을 리스트로 전달하여 batch 처리\n",
    "batch_responses = llm.batch(questions) # 체인이 아닌 llm 객체 자체의 batch 사용\n",
    "\n",
    "for response in batch_responses:\n",
    "    # AIMessage 객체의 pretty_print() 메서드로 보기 좋게 출력\n",
    "    response.pretty_print()\n",
    "    print() # 줄바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Runnable: 더 유연한 체인 구성\n",
    "LCEL의 핵심 `Runnable` 프로토콜을 따르는 다양한 클래스들. 복잡한 데이터 흐름이나 커스텀 로직을 체인에 통합할 때 유용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) RunnableParallel: 병렬 실행과 데이터 매핑\n",
    "여러 Runnable을 동시에 실행하거나, 입력 데이터를 딕셔너리 형태로 가공하여 다음 Runnable에 전달할 때 씀. `itemgetter`와 함께 자주 사용됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f21024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전기차 제조업체입니다. 2003년 마틴 에버하드(CEO)와 '\n",
      " '마크 타페닝(CFO)에 의해 설립된 테슬라는 2004년 페이팔과 Zip2의 공동 창업자인 일론 머스크의 참여로 큰 전환점을 맞았습니다. '\n",
      " '머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 '\n",
      " '이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다.')\n"
     ]
    }
   ],
   "source": [
    "# RAG를 위해 벡터 저장소에서 문서를 검색하는 Retriever 준비\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={'k': 1}, # 가장 유사한 문서 1개 검색\n",
    ")\n",
    "\n",
    "query = \"테슬라 창업자는 누구인가요?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# 검색된 문서(Document 객체 리스트)들의 page_content를 합쳐서 하나의 문자열로 만듦\n",
    "retrieved_docs_text = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "pprint(retrieved_docs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcbc4d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전기차 제조업체입니다. 2003년 마틴 에버하드(CEO)와 마크 타페닝(CFO)에 의해 설립된 테슬라는 2004년 페이팔과 Zip2의 공동 창업자인 일론 머스크의 참여로 큰 전환점을 맞았습니다. 머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다.',\n",
       " 'question': '테슬라 창업자는 누구인가요?'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from operator import itemgetter # 딕셔너리에서 특정 키의 값을 가져올 때 사용\n",
    "\n",
    "# RunnableParallel 구성: 입력 딕셔셔너리에서 'context'와 'question' 키의 값을 그대로 가져와 새로운 딕셔너리 생성\n",
    "setup = RunnableParallel(\n",
    "    context=itemgetter(\"context\") , \n",
    "    question=itemgetter(\"question\")\n",
    ")\n",
    "\n",
    "# 실행: 입력으로 딕셔너리를 전달\n",
    "runnable_parallel_output = setup.invoke({\"context\": retrieved_docs_text, \"question\": query})\n",
    "runnable_parallel_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) RunnablePassthrough: 입력 그대로 전달\n",
    "입력값을 다음 단계로 그대로 넘기거나, `RunnableParallel`과 함께 사용하여 특정 키에 원본 입력을 할당할 때 유용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b6495f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_input': {'query': '테슬라 창업자는 누구인가요?'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RunnableParallel 내에서 RunnablePassthrough 사용 예시\n",
    "pass_through_setup = RunnableParallel(\n",
    "    original_input=RunnablePassthrough(), # 입력을 그대로 'original_input' 키에 할당\n",
    ") # 여기에 다른 Runnable들을 추가하여 병렬 처리 가능\n",
    "\n",
    "pass_through_setup.invoke({\"query\":\"테슬라 창업자는 누구인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) RunnableLambda: 파이썬 함수도 체인에 착!\n",
    "간단한 파이썬 함수를 LCEL 체인 안에 컴포넌트처럼 넣을 수 있음. 커스텀 로직 추가에 매우 유용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "509b3f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '테슬라 창업자는 누구인가요?', 'word_count': 3}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 간단한 단어 수 세는 함수 정의\n",
    "def count_num_words(text_input):\n",
    "    if isinstance(text_input, dict) and 'query' in text_input: # 입력이 딕셔너리인 경우 'query' 키 사용\n",
    "        return len(text_input['query'].split())\n",
    "    elif isinstance(text_input, str): # 입력이 문자열인 경우\n",
    "        return len(text_input.split())\n",
    "    return 0\n",
    "\n",
    "# RunnableParallel과 RunnableLambda 조합\n",
    "lambda_setup = RunnableParallel(\n",
    "    question=RunnablePassthrough(), # 입력을 'question' 키에 그대로 전달\n",
    "    word_count=RunnableLambda(count_num_words), # count_num_words 함수를 Runnable로 만들어 'word_count' 키에 할당\n",
    ")\n",
    "\n",
    "lambda_setup.invoke(\"테슬라 창업자는 누구인가요?\") # 문자열 입력도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 실전! RAG 파이프라인 구축\n",
    "지금까지 배운 LCEL 컴포넌트들을 조합하여 질문에 대해 관련 문서를 찾아 답변하는 RAG 파이프라인을 만듦."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) RAG용 프롬프트 템플릿\n",
    "RAG의 핵심 프롬프트. LLM에게 '주어진 Context 안에서만 답변하고, 모르면 모른다고 해!'라고 지시하는 게 중요함. 외부 지식 사용 방지."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c9b52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Answer the question based only on the following context.\n",
      "Do not use any external information or knowledge. \n",
      "If the answer is not in the context, answer \"잘 모르겠습니다.\".\n",
      "\n",
      "[Context]\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "[Question] \n",
      "\u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "\n",
      "[Answer]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate # 이미 위에서 임포트 했지만, 명시적으로 다시 보여줌\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context.\n",
    "Do not use any external information or knowledge. \n",
    "If the answer is not in the context, answer \"잘 모르겠습니다.\".\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question] \n",
    "{question}\n",
    "\n",
    "[Answer]\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_prompt.pretty_print() # 생성된 RAG 프롬프트 구조 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 리트리버 체인: 문서 가져오고 포맷팅\n",
    "질문과 가장 유사한 문서를 벡터 저장소에서 찾아옴 (`retriever`). 찾은 문서들(Document 객체 리스트)을 LLM이 이해하기 쉬운 하나의 문자열로 합침 (`format_docs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09b7abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전기차 제조업체입니다. 2003년 마틴 에버하드(CEO)와 '\n",
      " '마크 타페닝(CFO)에 의해 설립된 테슬라는 2004년 페이팔과 Zip2의 공동 창업자인 일론 머스크의 참여로 큰 전환점을 맞았습니다. '\n",
      " '머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 '\n",
      " '이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다.\\n\\n2023년 테슬라는 1,808,581대의 차량을 '\n",
      " '판매하여 2022년에 비해 37.65% 증가했습니다. 2012년부터 2023년 3분기까지 테슬라의 전 세계 누적 판매량은 '\n",
      " '4,962,975대를 초과했습니다. SMT Packaging에 따르면, 2023년 테슬라의 판매량은 전 세계 전기차 시장의 약 12.9%를 '\n",
      " '차지했습니다.')\n"
     ]
    }
   ],
   "source": [
    "# 벡터 저장소 기반 리트리버 (k=2: 가장 유사한 문서 2개 검색)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 2})\n",
    "\n",
    "# 검색된 Document 객체 리스트를 하나의 문자열로 포맷팅하는 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# 리트리버 체인: retriever | format_docs (RunnableLambda로 함수를 감싸도 동일)\n",
    "retriever_chain = retriever | RunnableLambda(format_docs)\n",
    "# 또는 retriever_chain = RunnableLambda(lambda x: format_docs(retriever.invoke(x))) 이런식으로 한번에 구성도 가능\n",
    "\n",
    "# 리트리버 체인 테스트\n",
    "test_retrieved_text = retriever_chain.invoke(\"테슬라 창업자는 누구인가요?\")\n",
    "pprint(test_retrieved_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) RAG 체인 완성: 모든 조각 맞추기\n",
    "이제 모든 걸 연결함:\n",
    "1.  사용자 질문(`question`)을 받음.\n",
    "2.  `retriever_chain`을 사용해 질문과 관련된 문서(`context`)를 가져옴.\n",
    "3.  `RunnablePassthrough`를 사용해 원본 질문을 그대로 전달함.\n",
    "4.  이 `context`와 `question`을 `rag_prompt`에 넣어 완성된 프롬프트를 만듦.\n",
    "5.  이 프롬프트를 `llm`에 전달하여 답변 생성.\n",
    "6.  `StrOutputParser`로 LLM의 답변에서 텍스트만 추출함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d12f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser # 이미 임포트됨\n",
    "from langchain_openai import ChatOpenAI # 이미 임포트됨\n",
    "\n",
    "# RAG용 LLM 모델 (temperature=0으로 좀 더 사실 기반 답변 유도)\n",
    "rag_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=150)\n",
    "\n",
    "# 전체 RAG 체인 구성\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        context=retriever_chain,  # retriever_chain의 출력이 'context' 키로 들어감\n",
    "        question=RunnablePassthrough() # 원본 질문이 'question' 키로 들어감\n",
    "    )\n",
    "    | rag_prompt # context와 question을 받아 프롬프트 완성\n",
    "    | rag_llm    # 완성된 프롬프트를 LLM에 전달\n",
    "    | StrOutputParser() # LLM 응답(AIMessage)에서 문자열만 추출\n",
    ")\n",
    "\n",
    "# RAG 체인 실행\n",
    "query = \"테슬라 창업자는 누구인가요?\"\n",
    "final_response = rag_chain.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2b7fc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'마틴 에버하드와 마크 타페닝입니다.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 결과 출력\n",
    "final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradio로 챗봇 UI 만들기\n",
    "지금까지 만든 RAG 체인을 Gradio 인터페이스에 연결하여 간단한 웹 기반 챗봇을 만듦. 사용자와 상호작용 가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) invoke 방식: 한 번에 답변\n",
    "사용자 질문을 받아서 `rag_chain.invoke()`로 답변하고 전체를 한 번에 보여줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b207180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steve2/Library/Caches/pypoetry/virtualenvs/langchain-env-9sqiXrer-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio 인터페이스에 연결할 함수 정의 (invoke 사용)\n",
    "def answer_invoke(message, history):\n",
    "    # message: 사용자가 입력한 메시지\n",
    "    # history: 이전 대화 기록 (여기서는 사용 안 함)\n",
    "    response = rag_chain.invoke(message)\n",
    "    return response\n",
    "\n",
    "# Gradio ChatInterface 생성\n",
    "demo_invoke = gr.ChatInterface(fn=answer_invoke, title=\"간단 RAG QA 봇 (Invoke 방식)\")\n",
    "\n",
    "# Gradio 앱 실행 (Jupyter Notebook에서는 인라인으로 표시됨)\n",
    "demo_invoke.launch() # share=True 옵션으로 외부 공유 링크 생성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82f7a6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# Gradio 앱 종료 (새로운 앱 실행 전 기존 것 종료)\n",
    "demo_invoke.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) stream 방식: 실시간 타이핑 효과\n",
    "`rag_chain.stream()` 써서 LLM이 생성하는 답변을 실시간으로 UI에 뿌려줌. 사용자가 덜 지루함. `yield` 키워드가 핵심."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40b11b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time # 스트리밍 효과를 위해 약간의 딜레이 추가 (선택 사항)\n",
    "\n",
    "# Gradio 인터페이스에 연결할 함수 정의 (stream 사용)\n",
    "def answer_stream(message, history):\n",
    "    partial_message = \"\" # 부분적으로 완성된 메시지를 저장할 변수\n",
    "    for chunk in rag_chain.stream(message):\n",
    "        if chunk is not None: # stream의 각 chunk는 문자열 조각임\n",
    "            partial_message = partial_message + chunk\n",
    "            # time.sleep(0.05) # 너무 빠르면 눈으로 보기 힘드니 약간의 딜레이 (선택 사항)\n",
    "            yield partial_message # 생성된 부분 메시지를 UI로 실시간 전송\n",
    "\n",
    "# Gradio ChatInterface 생성\n",
    "demo_stream = gr.ChatInterface(fn=answer_stream, title=\"간단 RAG QA 봇 (Stream 방식)\")\n",
    "\n",
    "# Gradio 앱 실행\n",
    "demo_stream.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be3f36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# Gradio 앱 종료\n",
    "demo_stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "이 노트북을 통해 LCEL을 사용하여 프롬프트, LLM, 출력 파서, 리트리버 등 다양한 컴포넌트를 유연하게 연결하고, 이를 바탕으로 RAG 파이프라인을 구축하는 방법을 익힘.\n",
    "또한, `stream`, `batch`와 같은 LLM 호출 방식과 `RunnableParallel`, `RunnablePassthrough`, `RunnableLambda` 등 고급 Runnable 사용법도 살펴봄.\n",
    "마지막으로 Gradio를 이용해 만든 RAG 챗봇으로 실제 활용 예시까지 확인 완료!\n",
    "LCEL은 LangChain 애플리케이션 개발을 훨씬 직관적이고 효율적으로 만들어주는 강력한 도구임. 앞으로 더 복잡한 애플리케이션도 자신있게 만들 수 있을 거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ddd92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
